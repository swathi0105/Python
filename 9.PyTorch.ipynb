{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-cuyP_z8g-d3ppvqjHiaFuJP-6np42aD","timestamp":1659363221312},{"file_id":"1oXeLn9nvkqpfGsk7L3QzE59LAfpCbhp1","timestamp":1658391457813}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <center> ACM India and RBCDSAI Joint Summer School on DS/AI/ML\n","## <center> Hands-on Tutorial Day-10\n","## <center> Introduction to neural networks"],"metadata":{"id":"yl6sfeJc7AJa"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","np.random.seed(0)"],"metadata":{"id":"01kNmltn7FNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv('Train_Dataset1.csv',header=None)\n","X_train, y_train = train_df[0].to_numpy().reshape(1,-1),train_df[1].to_numpy().reshape(1,-1)\n","test_df = pd.read_csv('Test_Dataset1.csv',header=None)\n","X_test, y_test = test_df[0].to_numpy().reshape(1,-1),test_df[1].to_numpy().reshape(1,-1)"],"metadata":{"id":"GWS4lToG-yFv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.scatter(X_train,y_train,label='Training Data')\n","plt.legend()"],"metadata":{"id":"fQcmqNbO-yHk","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"ok","timestamp":1658396824019,"user_tz":-330,"elapsed":16,"user":{"displayName":"Indumathi palanikumar","userId":"12151466194348290233"}},"outputId":"6d9eadc8-fc35-4638-d7c1-01b0a5dfbb39"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7ff1663dbc10>"]},"metadata":{},"execution_count":9},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["class ANN:\n","    def __init__(self, X, y, hidden_units,learning_rate,epochs):#Give training data, number of neurons,learning rate and epochs\n","        self.X = X\n","        self.y = y\n","        self.hidden_units = hidden_units\n","        self.input_features = X.shape[0] #Get shape from X\n","        self.output_units = y.shape[0] #Get shape from y\n","        self.lr = learning_rate #Get learning rate from init\n","        self.epochs = epochs #Get epochs from init\n","        \n","    def init_weights(self):#Initialize weights from a random normal distribution (Bias only for hidden layer)\n","        self.W1 = np.random.normal(size = (self.hidden_units,self.input_features)) #Shapes of weights and biases\n","        self.b1 = np.ones((self.hidden_units,1))\n","        self.W2 = np.random.normal(size = (self.output_units,self.hidden_units))\n","    \n","    def sigmoid(self,X):#sigmoid activation \n","        return 1/(1+np.exp(-X))\n","\n","    def mse_loss(self,y_hat,y):#Calculate mean squared error loss\n","        errors = (y_hat-y)**2\n","        return np.mean(errors)\n","\n","    def d_sigmoid(self,X):#Derivative for sigmoid activation\n","        return self.sigmoid(X)*(1-self.sigmoid(X))\n","           \n","    def forward(self,X):#Forward propagation for single layer ANN with sigmoid activation applied only for hidden layer\n","        self.Z1 = self.W1@X + self.b1\n","        self.A1 = self.sigmoid(self.Z1)\n","        self.Z2 = self.W2@self.A1\n","        return self.Z2\n","        \n","    def backward(self):#Backward propagation using Mean Squared Error loss as objective function as it is a regression problem\n","        d_E_y = 2*(self.Z2-self.y)/self.y.size\n","        d_E_W2 = d_E_y@self.A1.T\n","        d_E_A1 = self.W2.T@d_E_y\n","        d_E_Z1 = d_E_A1*self.d_sigmoid(self.Z1)\n","        d_E_W1 = d_E_Z1@self.X.T\n","        d_E_b1 = d_E_Z1@np.ones((d_E_A1.shape[1],1))\n","        self.W1 -=  self.lr*d_E_W1 #Update weights based on learning rate and gradient\n","        self.W2 -= self.lr*d_E_W2\n","        self.b1 -= self.lr*d_E_b1\n","    \n","    def train(self,X_test,y_test):#Train the network for n number of epochs and calculate train and test loss\n","        self.init_weights()\n","        train_losses = []\n","        test_losses = []\n","        for i in range(self.epochs):\n","            train_preds = self.forward(self.X) #Forward propagation for training data\n","            self.backward()#Backward propagation for training data\n","            test_preds = self.forward(X_test)#Only forward propagation for test to get predictions\n","            train_loss = self.mse_loss(train_preds,self.y)\n","            test_loss = self.mse_loss(test_preds,y_test)\n","            train_losses.append(train_loss)\n","            test_losses.append(test_loss)\n","            print('Epoch: %s, Train Loss: %s, Test Loss: %s' % (i,train_loss,test_loss))\n","        return train_losses,test_losses"],"metadata":{"id":"4BNIjqGi7FQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ann = ANN(X_train,y_train,20,0.1,10000)\n","train_losses,test_losses = ann.train(X_test,y_test)"],"metadata":{"id":"-VMCIHH27FYR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658399476310,"user_tz":-330,"elapsed":35247,"user":{"displayName":"Indumathi palanikumar","userId":"12151466194348290233"}},"outputId":"9fd704f6-76ce-47f2-c052-259f582238b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch: 5000, Train Loss: 0.04019922376163419, Test Loss: 0.034445297433949425\n","Epoch: 5001, Train Loss: 0.040193813327256155, Test Loss: 0.03444071424252119\n","Epoch: 5002, Train Loss: 0.040188402075320935, Test Loss: 0.03443613036120398\n","Epoch: 5003, Train Loss: 0.04018299000545936, Test Loss: 0.03443154578965498\n","Epoch: 5004, Train Loss: 0.04017757711730286, Test Loss: 0.03442696052753211\n","Epoch: 5005, Train Loss: 0.04017216341048342, Test Loss: 0.0344223745744936\n","Epoch: 5006, Train Loss: 0.04016674888463369, Test Loss: 0.03441778793019835\n","Epoch: 5007, Train Loss: 0.04016133353938688, Test Loss: 0.0344132005943058\n","Epoch: 5008, Train Loss: 0.04015591737437685, Test Loss: 0.034408612566475906\n","Epoch: 5009, Train Loss: 0.04015050038923806, Test Loss: 0.034404023846369175\n","Epoch: 5010, Train Loss: 0.04014508258360552, Test Loss: 0.034399434433646646\n","Epoch: 5011, Train Loss: 0.04013966395711494, Test Loss: 0.03439484432796994\n","Epoch: 5012, Train Loss: 0.04013424450940257, Test Loss: 0.03439025352900115\n","Epoch: 5013, Train Loss: 0.040128824240105304, Test Loss: 0.03438566203640302\n","Epoch: 5014, Train Loss: 0.04012340314886064, Test Loss: 0.03438106984983871\n","Epoch: 5015, Train Loss: 0.04011798123530666, Test Loss: 0.03437647696897202\n","Epoch: 5016, Train Loss: 0.04011255849908208, Test Loss: 0.03437188339346728\n","Epoch: 5017, Train Loss: 0.04010713493982624, Test Loss: 0.03436728912298933\n","Epoch: 5018, Train Loss: 0.04010171055717905, Test Loss: 0.03436269415720359\n","Epoch: 5019, Train Loss: 0.040096285350781066, Test Loss: 0.03435809849577601\n","Epoch: 5020, Train Loss: 0.04009085932027344, Test Loss: 0.03435350213837304\n","Epoch: 5021, Train Loss: 0.040085432465297936, Test Loss: 0.03434890508466179\n","Epoch: 5022, Train Loss: 0.04008000478549694, Test Loss: 0.03434430733430979\n","Epoch: 5023, Train Loss: 0.04007457628051344, Test Loss: 0.034339708886985267\n","Epoch: 5024, Train Loss: 0.04006914694999104, Test Loss: 0.03433510974235682\n","Epoch: 5025, Train Loss: 0.04006371679357393, Test Loss: 0.03433050990009369\n","Epoch: 5026, Train Loss: 0.040058285810907, Test Loss: 0.034325909359865674\n","Epoch: 5027, Train Loss: 0.04005285400163562, Test Loss: 0.0343213081213431\n","Epoch: 5028, Train Loss: 0.04004742136540593, Test Loss: 0.03431670618419684\n","Epoch: 5029, Train Loss: 0.040041987901864515, Test Loss: 0.0343121035480983\n","Epoch: 5030, Train Loss: 0.040036553610658714, Test Loss: 0.034307500212719444\n","Epoch: 5031, Train Loss: 0.0400311184914364, Test Loss: 0.03430289617773281\n","Epoch: 5032, Train Loss: 0.04002568254384612, Test Loss: 0.034298291442811456\n","Epoch: 5033, Train Loss: 0.04002024576753699, Test Loss: 0.03429368600762905\n","Epoch: 5034, Train Loss: 0.04001480816215875, Test Loss: 0.034289079871859654\n","Epoch: 5035, Train Loss: 0.04000936972736179, Test Loss: 0.034284473035178065\n","Epoch: 5036, Train Loss: 0.04000393046279705, Test Loss: 0.03427986549725952\n","Epoch: 5037, Train Loss: 0.03999849036811615, Test Loss: 0.03427525725777984\n","Epoch: 5038, Train Loss: 0.03999304944297131, Test Loss: 0.034270648316415374\n","Epoch: 5039, Train Loss: 0.039987607687015356, Test Loss: 0.03426603867284309\n","Epoch: 5040, Train Loss: 0.039982165099901726, Test Loss: 0.03426142832674042\n","Epoch: 5041, Train Loss: 0.039976721681284494, Test Loss: 0.034256817277785366\n","Epoch: 5042, Train Loss: 0.03997127743081835, Test Loss: 0.0342522055256565\n","Epoch: 5043, Train Loss: 0.0399658323481586, Test Loss: 0.034247593070033014\n","Epoch: 5044, Train Loss: 0.03996038643296116, Test Loss: 0.03424297991059448\n","Epoch: 5045, Train Loss: 0.039954939684882576, Test Loss: 0.03423836604702121\n","Epoch: 5046, Train Loss: 0.03994949210358003, Test Loss: 0.03423375147899395\n","Epoch: 5047, Train Loss: 0.03994404368871127, Test Loss: 0.03422913620619402\n","Epoch: 5048, Train Loss: 0.03993859443993472, Test Loss: 0.034224520228303315\n","Epoch: 5049, Train Loss: 0.03993314435690938, Test Loss: 0.034219903545004295\n","Epoch: 5050, Train Loss: 0.03992769343929494, Test Loss: 0.03421528615597993\n","Epoch: 5051, Train Loss: 0.03992224168675163, Test Loss: 0.03421066806091374\n","Epoch: 5052, Train Loss: 0.03991678909894037, Test Loss: 0.03420604925948987\n","Epoch: 5053, Train Loss: 0.03991133567552265, Test Loss: 0.03420142975139299\n","Epoch: 5054, Train Loss: 0.03990588141616058, Test Loss: 0.034196809536308236\n","Epoch: 5055, Train Loss: 0.03990042632051694, Test Loss: 0.03419218861392142\n","Epoch: 5056, Train Loss: 0.03989497038825512, Test Loss: 0.03418756698391886\n","Epoch: 5057, Train Loss: 0.039889513619039106, Test Loss: 0.03418294464598744\n","Epoch: 5058, Train Loss: 0.0398840560125335, Test Loss: 0.03417832159981452\n","Epoch: 5059, Train Loss: 0.03987859756840359, Test Loss: 0.034173697845088176\n","Epoch: 5060, Train Loss: 0.03987313828631524, Test Loss: 0.03416907338149693\n","Epoch: 5061, Train Loss: 0.03986767816593494, Test Loss: 0.03416444820872984\n","Epoch: 5062, Train Loss: 0.0398622172069298, Test Loss: 0.03415982232647657\n","Epoch: 5063, Train Loss: 0.039856755408967603, Test Loss: 0.03415519573442736\n","Epoch: 5064, Train Loss: 0.03985129277171669, Test Loss: 0.03415056843227295\n","Epoch: 5065, Train Loss: 0.039845829294846064, Test Loss: 0.03414594041970468\n","Epoch: 5066, Train Loss: 0.039840364978025365, Test Loss: 0.034141311696414434\n","Epoch: 5067, Train Loss: 0.03983489982092482, Test Loss: 0.03413668226209465\n","Epoch: 5068, Train Loss: 0.03982943382321533, Test Loss: 0.034132052116438334\n","Epoch: 5069, Train Loss: 0.03982396698456839, Test Loss: 0.034127421259139035\n","Epoch: 5070, Train Loss: 0.03981849930465614, Test Loss: 0.03412278968989085\n","Epoch: 5071, Train Loss: 0.03981303078315135, Test Loss: 0.03411815740838849\n","Epoch: 5072, Train Loss: 0.0398075614197274, Test Loss: 0.03411352441432717\n","Epoch: 5073, Train Loss: 0.03980209121405828, Test Loss: 0.03410889070740269\n","Epoch: 5074, Train Loss: 0.03979662016581866, Test Loss: 0.03410425628731139\n","Epoch: 5075, Train Loss: 0.03979114827468383, Test Loss: 0.03409962115375021\n","Epoch: 5076, Train Loss: 0.039785675540329676, Test Loss: 0.03409498530641661\n","Epoch: 5077, Train Loss: 0.03978020196243274, Test Loss: 0.03409034874500861\n","Epoch: 5078, Train Loss: 0.03977472754067016, Test Loss: 0.03408571146922483\n","Epoch: 5079, Train Loss: 0.03976925227471977, Test Loss: 0.03408107347876439\n","Epoch: 5080, Train Loss: 0.03976377616425998, Test Loss: 0.03407643477332702\n","Epoch: 5081, Train Loss: 0.03975829920896983, Test Loss: 0.03407179535261303\n","Epoch: 5082, Train Loss: 0.03975282140852903, Test Loss: 0.03406715521632323\n","Epoch: 5083, Train Loss: 0.0397473427626179, Test Loss: 0.034062514364159004\n","Epoch: 5084, Train Loss: 0.039741863270917346, Test Loss: 0.03405787279582237\n","Epoch: 5085, Train Loss: 0.039736382933109014, Test Loss: 0.034053230511015796\n","Epoch: 5086, Train Loss: 0.039730901748875086, Test Loss: 0.034048587509442424\n","Epoch: 5087, Train Loss: 0.0397254197178984, Test Loss: 0.034043943790805845\n","Epoch: 5088, Train Loss: 0.03971993683986247, Test Loss: 0.03403929935481031\n","Epoch: 5089, Train Loss: 0.03971445311445141, Test Loss: 0.034034654201160586\n","Epoch: 5090, Train Loss: 0.039708968541349934, Test Loss: 0.03403000832956205\n","Epoch: 5091, Train Loss: 0.03970348312024344, Test Loss: 0.034025361739720573\n","Epoch: 5092, Train Loss: 0.03969799685081796, Test Loss: 0.03402071443134258\n","Epoch: 5093, Train Loss: 0.039692509732760124, Test Loss: 0.03401606640413523\n","Epoch: 5094, Train Loss: 0.03968702176575725, Test Loss: 0.03401141765780599\n","Epoch: 5095, Train Loss: 0.03968153294949722, Test Loss: 0.034006768192063115\n","Epoch: 5096, Train Loss: 0.03967604328366863, Test Loss: 0.03400211800661527\n","Epoch: 5097, Train Loss: 0.03967055276796066, Test Loss: 0.033997467101171804\n","Epoch: 5098, Train Loss: 0.03966506140206315, Test Loss: 0.03399281547544252\n","Epoch: 5099, Train Loss: 0.03965956918566655, Test Loss: 0.0339881631291379\n","Epoch: 5100, Train Loss: 0.03965407611846197, Test Loss: 0.0339835100619689\n","Epoch: 5101, Train Loss: 0.03964858220014116, Test Loss: 0.0339788562736471\n","Epoch: 5102, Train Loss: 0.039643087430396504, Test Loss: 0.03397420176388461\n","Epoch: 5103, Train Loss: 0.039637591808921, Test Loss: 0.033969546532394146\n","Epoch: 5104, Train Loss: 0.03963209533540831, Test Loss: 0.03396489057888892\n","Epoch: 5105, Train Loss: 0.03962659800955275, Test Loss: 0.03396023390308279\n","Epoch: 5106, Train Loss: 0.039621099831049236, Test Loss: 0.03395557650469014\n","Epoch: 5107, Train Loss: 0.03961560079959333, Test Loss: 0.03395091838342598\n","Epoch: 5108, Train Loss: 0.03961010091488127, Test Loss: 0.033946259539005756\n","Epoch: 5109, Train Loss: 0.03960460017660989, Test Loss: 0.03394159997114563\n","Epoch: 5110, Train Loss: 0.03959909858447669, Test Loss: 0.03393693967956223\n","Epoch: 5111, Train Loss: 0.03959359613817978, Test Loss: 0.033932278663972824\n","Epoch: 5112, Train Loss: 0.039588092837417974, Test Loss: 0.03392761692409521\n","Epoch: 5113, Train Loss: 0.03958258868189067, Test Loss: 0.033922954459647756\n","Epoch: 5114, Train Loss: 0.0395770836712979, Test Loss: 0.033918291270349406\n","Epoch: 5115, Train Loss: 0.03957157780534039, Test Loss: 0.03391362735591968\n","Epoch: 5116, Train Loss: 0.03956607108371946, Test Loss: 0.03390896271607864\n","Epoch: 5117, Train Loss: 0.03956056350613711, Test Loss: 0.03390429735054698\n","Epoch: 5118, Train Loss: 0.03955505507229596, Test Loss: 0.03389963125904591\n","Epoch: 5119, Train Loss: 0.039549545781899266, Test Loss: 0.033894964441297186\n","Epoch: 5120, Train Loss: 0.03954403563465097, Test Loss: 0.03389029689702323\n","Epoch: 5121, Train Loss: 0.039538524630255575, Test Loss: 0.03388562862594696\n","Epoch: 5122, Train Loss: 0.03953301276841833, Test Loss: 0.0338809596277919\n","Epoch: 5123, Train Loss: 0.03952750004884504, Test Loss: 0.03387628990228213\n","Epoch: 5124, Train Loss: 0.039521986471242214, Test Loss: 0.033871619449142264\n","Epoch: 5125, Train Loss: 0.03951647203531699, Test Loss: 0.03386694826809756\n","Epoch: 5126, Train Loss: 0.03951095674077712, Test Loss: 0.03386227635887387\n","Epoch: 5127, Train Loss: 0.03950544058733105, Test Loss: 0.033857603721197456\n","Epoch: 5128, Train Loss: 0.039499923574687824, Test Loss: 0.03385293035479535\n","Epoch: 5129, Train Loss: 0.0394944057025572, Test Loss: 0.03384825625939502\n","Epoch: 5130, Train Loss: 0.03948888697064949, Test Loss: 0.033843581434724615\n","Epoch: 5131, Train Loss: 0.03948336737867574, Test Loss: 0.033838905880512735\n","Epoch: 5132, Train Loss: 0.039477846926347565, Test Loss: 0.03383422959648867\n","Epoch: 5133, Train Loss: 0.039472325613377324, Test Loss: 0.03382955258238222\n","Epoch: 5134, Train Loss: 0.03946680343947793, Test Loss: 0.03382487483792374\n","Epoch: 5135, Train Loss: 0.039461280404362954, Test Loss: 0.03382019636284424\n","Epoch: 5136, Train Loss: 0.0394557565077467, Test Loss: 0.033815517156875266\n","Epoch: 5137, Train Loss: 0.03945023174934404, Test Loss: 0.03381083721974889\n","Epoch: 5138, Train Loss: 0.039444706128870494, Test Loss: 0.03380615655119783\n","Epoch: 5139, Train Loss: 0.039439179646042294, Test Loss: 0.03380147515095535\n","Epoch: 5140, Train Loss: 0.039433652300576265, Test Loss: 0.03379679301875528\n","Epoch: 5141, Train Loss: 0.039428124092189876, Test Loss: 0.03379211015433206\n","Epoch: 5142, Train Loss: 0.039422595020601296, Test Loss: 0.03378742655742063\n","Epoch: 5143, Train Loss: 0.03941706508552934, Test Loss: 0.03378274222775663\n","Epoch: 5144, Train Loss: 0.039411534286693424, Test Loss: 0.03377805716507616\n","Epoch: 5145, Train Loss: 0.03940600262381362, Test Loss: 0.03377337136911599\n","Epoch: 5146, Train Loss: 0.03940047009661072, Test Loss: 0.03376868483961342\n","Epoch: 5147, Train Loss: 0.0393949367048061, Test Loss: 0.03376399757630627\n","Epoch: 5148, Train Loss: 0.039389402448121794, Test Loss: 0.03375930957893305\n","Epoch: 5149, Train Loss: 0.03938386732628053, Test Loss: 0.03375462084723279\n","Epoch: 5150, Train Loss: 0.03937833133900566, Test Loss: 0.0337499313809451\n","Epoch: 5151, Train Loss: 0.039372794486021205, Test Loss: 0.03374524117981017\n","Epoch: 5152, Train Loss: 0.03936725676705178, Test Loss: 0.033740550243568763\n","Epoch: 5153, Train Loss: 0.03936171818182273, Test Loss: 0.03373585857196229\n","Epoch: 5154, Train Loss: 0.03935617873006004, Test Loss: 0.03373116616473264\n","Epoch: 5155, Train Loss: 0.03935063841149031, Test Loss: 0.03372647302162231\n","Epoch: 5156, Train Loss: 0.039345097225840814, Test Loss: 0.033721779142374404\n","Epoch: 5157, Train Loss: 0.039339555172839484, Test Loss: 0.03371708452673262\n","Epoch: 5158, Train Loss: 0.03933401225221492, Test Loss: 0.03371238917444119\n","Epoch: 5159, Train Loss: 0.03932846846369638, Test Loss: 0.033707693085244926\n","Epoch: 5160, Train Loss: 0.03932292380701373, Test Loss: 0.03370299625888924\n","Epoch: 5161, Train Loss: 0.039317378281897526, Test Loss: 0.033698298695120185\n","Epoch: 5162, Train Loss: 0.039311831888079, Test Loss: 0.0336936003936843\n","Epoch: 5163, Train Loss: 0.039306284625290004, Test Loss: 0.033688901354328754\n","Epoch: 5164, Train Loss: 0.03930073649326307, Test Loss: 0.033684201576801256\n","Epoch: 5165, Train Loss: 0.039295187491731366, Test Loss: 0.03367950106085017\n","Epoch: 5166, Train Loss: 0.03928963762042875, Test Loss: 0.03367479980622436\n","Epoch: 5167, Train Loss: 0.039284086879089714, Test Loss: 0.03367009781267336\n","Epoch: 5168, Train Loss: 0.0392785352674494, Test Loss: 0.033665395079947216\n","Epoch: 5169, Train Loss: 0.03927298278524363, Test Loss: 0.03366069160779657\n","Epoch: 5170, Train Loss: 0.039267429432208886, Test Loss: 0.03365598739597265\n","Epoch: 5171, Train Loss: 0.03926187520808227, Test Loss: 0.03365128244422731\n","Epoch: 5172, Train Loss: 0.03925632011260161, Test Loss: 0.03364657675231294\n","Epoch: 5173, Train Loss: 0.03925076414550534, Test Loss: 0.03364187031998253\n","Epoch: 5174, Train Loss: 0.03924520730653255, Test Loss: 0.03363716314698962\n","Epoch: 5175, Train Loss: 0.039239649595423046, Test Loss: 0.03363245523308842\n","Epoch: 5176, Train Loss: 0.039234091011917244, Test Loss: 0.033627746578033656\n","Epoch: 5177, Train Loss: 0.03922853155575621, Test Loss: 0.03362303718158062\n","Epoch: 5178, Train Loss: 0.039222971226681766, Test Loss: 0.03361832704348527\n","Epoch: 5179, Train Loss: 0.03921741002443624, Test Loss: 0.03361361616350405\n","Epoch: 5180, Train Loss: 0.03921184794876277, Test Loss: 0.03360890454139409\n","Epoch: 5181, Train Loss: 0.03920628499940506, Test Loss: 0.03360419217691304\n","Epoch: 5182, Train Loss: 0.03920072117610756, Test Loss: 0.033599479069819145\n","Epoch: 5183, Train Loss: 0.039195156478615274, Test Loss: 0.03359476521987127\n","Epoch: 5184, Train Loss: 0.03918959090667395, Test Loss: 0.033590050626828814\n","Epoch: 5185, Train Loss: 0.03918402446003001, Test Loss: 0.03358533529045184\n","Epoch: 5186, Train Loss: 0.03917845713843045, Test Loss: 0.033580619210500894\n","Epoch: 5187, Train Loss: 0.039172888941623046, Test Loss: 0.03357590238673718\n","Epoch: 5188, Train Loss: 0.03916731986935614, Test Loss: 0.03357118481892252\n","Epoch: 5189, Train Loss: 0.039161749921378795, Test Loss: 0.03356646650681921\n","Epoch: 5190, Train Loss: 0.039156179097440715, Test Loss: 0.03356174745019022\n","Epoch: 5191, Train Loss: 0.03915060739729227, Test Loss: 0.03355702764879913\n","Epoch: 5192, Train Loss: 0.03914503482068453, Test Loss: 0.03355230710241006\n","Epoch: 5193, Train Loss: 0.03913946136736918, Test Loss: 0.03354758581078769\n","Epoch: 5194, Train Loss: 0.0391338870370986, Test Loss: 0.033542863773697355\n","Epoch: 5195, Train Loss: 0.03912831182962582, Test Loss: 0.03353814099090494\n","Epoch: 5196, Train Loss: 0.03912273574470454, Test Loss: 0.03353341746217695\n","Epoch: 5197, Train Loss: 0.03911715878208916, Test Loss: 0.03352869318728047\n","Epoch: 5198, Train Loss: 0.039111580941534725, Test Loss: 0.033523968165983124\n","Epoch: 5199, Train Loss: 0.0391060022227969, Test Loss: 0.03351924239805317\n","Epoch: 5200, Train Loss: 0.03910042262563208, Test Loss: 0.03351451588325953\n","Epoch: 5201, Train Loss: 0.03909484214979734, Test Loss: 0.03350978862137155\n","Epoch: 5202, Train Loss: 0.03908926079505035, Test Loss: 0.0335050606121593\n","Epoch: 5203, Train Loss: 0.03908367856114951, Test Loss: 0.03350033185539336\n","Epoch: 5204, Train Loss: 0.03907809544785388, Test Loss: 0.033495602350845005\n","Epoch: 5205, Train Loss: 0.03907251145492317, Test Loss: 0.03349087209828601\n","Epoch: 5206, Train Loss: 0.03906692658211775, Test Loss: 0.033486141097488736\n","Epoch: 5207, Train Loss: 0.039061340829198696, Test Loss: 0.033481409348226215\n","Epoch: 5208, Train Loss: 0.03905575419592776, Test Loss: 0.03347667685027199\n","Epoch: 5209, Train Loss: 0.0390501666820673, Test Loss: 0.03347194360340029\n","Epoch: 5210, Train Loss: 0.0390445782873804, Test Loss: 0.033467209607385806\n","Epoch: 5211, Train Loss: 0.03903898901163081, Test Loss: 0.03346247486200391\n","Epoch: 5212, Train Loss: 0.03903339885458296, Test Loss: 0.0334577393670306\n","Epoch: 5213, Train Loss: 0.03902780781600188, Test Loss: 0.03345300312224237\n","Epoch: 5214, Train Loss: 0.03902221589565338, Test Loss: 0.03344826612741636\n","Epoch: 5215, Train Loss: 0.039016623093303865, Test Loss: 0.033443528382330334\n","Epoch: 5216, Train Loss: 0.03901102940872044, Test Loss: 0.033438789886762584\n","Epoch: 5217, Train Loss: 0.03900543484167089, Test Loss: 0.03343405064049205\n","Epoch: 5218, Train Loss: 0.03899983939192363, Test Loss: 0.03342931064329823\n","Epoch: 5219, Train Loss: 0.038994243059247806, Test Loss: 0.033424569894961266\n","Epoch: 5220, Train Loss: 0.03898864584341321, Test Loss: 0.03341982839526179\n","Epoch: 5221, Train Loss: 0.0389830477441903, Test Loss: 0.03341508614398118\n","Epoch: 5222, Train Loss: 0.03897744876135024, Test Loss: 0.0334103431409013\n","Epoch: 5223, Train Loss: 0.038971848894664825, Test Loss: 0.0334055993858046\n","Epoch: 5224, Train Loss: 0.03896624814390657, Test Loss: 0.03340085487847424\n","Epoch: 5225, Train Loss: 0.03896064650884861, Test Loss: 0.03339610961869384\n","Epoch: 5226, Train Loss: 0.03895504398926482, Test Loss: 0.033391363606247754\n","Epoch: 5227, Train Loss: 0.0389494405849297, Test Loss: 0.033386616840920744\n","Epoch: 5228, Train Loss: 0.038943836295618456, Test Loss: 0.0333818693224984\n","Epoch: 5229, Train Loss: 0.03893823112110695, Test Loss: 0.03337712105076671\n","Epoch: 5230, Train Loss: 0.038932625061171726, Test Loss: 0.033372372025512395\n","Epoch: 5231, Train Loss: 0.03892701811559004, Test Loss: 0.03336762224652266\n","Epoch: 5232, Train Loss: 0.03892141028413973, Test Loss: 0.03336287171358544\n","Epoch: 5233, Train Loss: 0.03891580156659944, Test Loss: 0.03335812042648914\n","Epoch: 5234, Train Loss: 0.038910191962748406, Test Loss: 0.033353368385022855\n","Epoch: 5235, Train Loss: 0.03890458147236653, Test Loss: 0.03334861558897621\n","Epoch: 5236, Train Loss: 0.0388989700952345, Test Loss: 0.033343862038139466\n","Epoch: 5237, Train Loss: 0.03889335783113354, Test Loss: 0.03333910773230348\n","Epoch: 5238, Train Loss: 0.03888774467984564, Test Loss: 0.033334352671259726\n","Epoch: 5239, Train Loss: 0.03888213064115348, Test Loss: 0.033329596854800275\n","Epoch: 5240, Train Loss: 0.03887651571484034, Test Loss: 0.03332484028271772\n","Epoch: 5241, Train Loss: 0.038870899900690266, Test Loss: 0.03332008295480533\n","Epoch: 5242, Train Loss: 0.03886528319848794, Test Loss: 0.03331532487085698\n","Epoch: 5243, Train Loss: 0.03885966560801874, Test Loss: 0.03331056603066713\n","Epoch: 5244, Train Loss: 0.0388540471290687, Test Loss: 0.033305806434030825\n","Epoch: 5245, Train Loss: 0.03884842776142456, Test Loss: 0.0333010460807437\n","Epoch: 5246, Train Loss: 0.03884280750487374, Test Loss: 0.03329628497060204\n","Epoch: 5247, Train Loss: 0.03883718635920432, Test Loss: 0.03329152310340265\n","Epoch: 5248, Train Loss: 0.038831564324205106, Test Loss: 0.03328676047894308\n","Epoch: 5249, Train Loss: 0.038825941399665526, Test Loss: 0.03328199709702129\n","Epoch: 5250, Train Loss: 0.03882031758537573, Test Loss: 0.03327723295743599\n","Epoch: 5251, Train Loss: 0.038814692881126545, Test Loss: 0.03327246805998645\n","Epoch: 5252, Train Loss: 0.03880906728670948, Test Loss: 0.03326770240447255\n","Epoch: 5253, Train Loss: 0.038803440801916715, Test Loss: 0.03326293599069468\n","Epoch: 5254, Train Loss: 0.03879781342654117, Test Loss: 0.033258168818454004\n","Epoch: 5255, Train Loss: 0.038792185160376344, Test Loss: 0.033253400887552124\n","Epoch: 5256, Train Loss: 0.03878655600321648, Test Loss: 0.03324863219779141\n","Epoch: 5257, Train Loss: 0.038780925954856556, Test Loss: 0.03324386274897467\n","Epoch: 5258, Train Loss: 0.03877529501509213, Test Loss: 0.03323909254090539\n","Epoch: 5259, Train Loss: 0.038769663183719535, Test Loss: 0.03323432157338767\n","Epoch: 5260, Train Loss: 0.03876403046053574, Test Loss: 0.033229549846226204\n","Epoch: 5261, Train Loss: 0.038758396845338404, Test Loss: 0.033224777359226305\n","Epoch: 5262, Train Loss: 0.03875276233792588, Test Loss: 0.03322000411219386\n","Epoch: 5263, Train Loss: 0.03874712693809724, Test Loss: 0.033215230104935364\n","Epoch: 5264, Train Loss: 0.03874149064565216, Test Loss: 0.033210455337257944\n","Epoch: 5265, Train Loss: 0.03873585346039109, Test Loss: 0.03320567980896932\n","Epoch: 5266, Train Loss: 0.038730215382115095, Test Loss: 0.03320090351987783\n","Epoch: 5267, Train Loss: 0.038724576410625995, Test Loss: 0.033196126469792336\n","Epoch: 5268, Train Loss: 0.03871893654572623, Test Loss: 0.03319134865852243\n","Epoch: 5269, Train Loss: 0.03871329578721899, Test Loss: 0.03318657008587826\n","Epoch: 5270, Train Loss: 0.03870765413490813, Test Loss: 0.033181790751670506\n","Epoch: 5271, Train Loss: 0.03870201158859815, Test Loss: 0.0331770106557106\n","Epoch: 5272, Train Loss: 0.03869636814809432, Test Loss: 0.03317222979781046\n","Epoch: 5273, Train Loss: 0.03869072381320252, Test Loss: 0.03316744817778265\n","Epoch: 5274, Train Loss: 0.03868507858372939, Test Loss: 0.03316266579544033\n","Epoch: 5275, Train Loss: 0.03867943245948219, Test Loss: 0.03315788265059731\n","Epoch: 5276, Train Loss: 0.0386737854402689, Test Loss: 0.03315309874306796\n","Epoch: 5277, Train Loss: 0.038668137525898226, Test Loss: 0.03314831407266733\n","Epoch: 5278, Train Loss: 0.03866248871617952, Test Loss: 0.033143528639210953\n","Epoch: 5279, Train Loss: 0.03865683901092284, Test Loss: 0.03313874244251503\n","Epoch: 5280, Train Loss: 0.038651188409938905, Test Loss: 0.03313395548239646\n","Epoch: 5281, Train Loss: 0.038645536913039194, Test Loss: 0.03312916775867261\n","Epoch: 5282, Train Loss: 0.0386398845200358, Test Loss: 0.03312437927116154\n","Epoch: 5283, Train Loss: 0.03863423123074156, Test Loss: 0.033119590019681906\n","Epoch: 5284, Train Loss: 0.03862857704496998, Test Loss: 0.033114800004052945\n","Epoch: 5285, Train Loss: 0.03862292196253527, Test Loss: 0.033110009224094517\n","Epoch: 5286, Train Loss: 0.038617265983252325, Test Loss: 0.03310521767962717\n","Epoch: 5287, Train Loss: 0.03861160910693674, Test Loss: 0.03310042537047192\n","Epoch: 5288, Train Loss: 0.03860595133340479, Test Loss: 0.03309563229645044\n","Epoch: 5289, Train Loss: 0.03860029266247344, Test Loss: 0.033090838457385106\n","Epoch: 5290, Train Loss: 0.03859463309396037, Test Loss: 0.03308604385309878\n","Epoch: 5291, Train Loss: 0.03858897262768396, Test Loss: 0.03308124848341508\n","Epoch: 5292, Train Loss: 0.03858331126346323, Test Loss: 0.03307645234815803\n","Epoch: 5293, Train Loss: 0.03857764900111798, Test Loss: 0.033071655447152425\n","Epoch: 5294, Train Loss: 0.0385719858404686, Test Loss: 0.033066857780223634\n","Epoch: 5295, Train Loss: 0.038566321781336295, Test Loss: 0.03306205934719764\n","Epoch: 5296, Train Loss: 0.03856065682354285, Test Loss: 0.03305726014790103\n","Epoch: 5297, Train Loss: 0.0385549909669108, Test Loss: 0.033052460182160966\n","Epoch: 5298, Train Loss: 0.03854932421126342, Test Loss: 0.0330476594498053\n","Epoch: 5299, Train Loss: 0.038543656556424596, Test Loss: 0.03304285795066243\n","Epoch: 5300, Train Loss: 0.038537988002218956, Test Loss: 0.03303805568456139\n","Epoch: 5301, Train Loss: 0.03853231854847182, Test Loss: 0.03303325265133184\n","Epoch: 5302, Train Loss: 0.03852664819500919, Test Loss: 0.03302844885080406\n","Epoch: 5303, Train Loss: 0.038520976941657806, Test Loss: 0.03302364428280888\n","Epoch: 5304, Train Loss: 0.03851530478824506, Test Loss: 0.033018838947177824\n","Epoch: 5305, Train Loss: 0.03850963173459902, Test Loss: 0.03301403284374297\n","Epoch: 5306, Train Loss: 0.03850395778054856, Test Loss: 0.03300922597233704\n","Epoch: 5307, Train Loss: 0.03849828292592315, Test Loss: 0.03300441833279336\n","Epoch: 5308, Train Loss: 0.038492607170552987, Test Loss: 0.03299960992494592\n","Epoch: 5309, Train Loss: 0.038486930514268984, Test Loss: 0.03299480074862924\n","Epoch: 5310, Train Loss: 0.03848125295690271, Test Loss: 0.032989990803678475\n","Epoch: 5311, Train Loss: 0.03847557449828651, Test Loss: 0.03298518008992945\n","Epoch: 5312, Train Loss: 0.03846989513825332, Test Loss: 0.03298036860721857\n","Epoch: 5313, Train Loss: 0.0384642148766369, Test Loss: 0.0329755563553828\n","Epoch: 5314, Train Loss: 0.038458533713271625, Test Loss: 0.03297074333425983\n","Epoch: 5315, Train Loss: 0.038452851647992586, Test Loss: 0.032965929543687926\n","Epoch: 5316, Train Loss: 0.03844716868063558, Test Loss: 0.03296111498350592\n","Epoch: 5317, Train Loss: 0.038441484811037104, Test Loss: 0.0329562996535533\n","Epoch: 5318, Train Loss: 0.03843580003903438, Test Loss: 0.03295148355367016\n","Epoch: 5319, Train Loss: 0.038430114364465286, Test Loss: 0.03294666668369726\n","Epoch: 5320, Train Loss: 0.03842442778716845, Test Loss: 0.03294184904347587\n","Epoch: 5321, Train Loss: 0.03841874030698316, Test Loss: 0.032937030632847995\n","Epoch: 5322, Train Loss: 0.03841305192374942, Test Loss: 0.03293221145165619\n","Epoch: 5323, Train Loss: 0.03840736263730797, Test Loss: 0.0329273914997436\n","Epoch: 5324, Train Loss: 0.038401672447500194, Test Loss: 0.03292257077695407\n","Epoch: 5325, Train Loss: 0.038395981354168204, Test Loss: 0.03291774928313203\n","Epoch: 5326, Train Loss: 0.03839028935715485, Test Loss: 0.032912927018122484\n","Epoch: 5327, Train Loss: 0.03838459645630363, Test Loss: 0.03290810398177114\n","Epoch: 5328, Train Loss: 0.03837890265145878, Test Loss: 0.03290328017392423\n","Epoch: 5329, Train Loss: 0.03837320794246523, Test Loss: 0.03289845559442866\n","Epoch: 5330, Train Loss: 0.0383675123291686, Test Loss: 0.032893630243131945\n","Epoch: 5331, Train Loss: 0.03836181581141527, Test Loss: 0.032888804119882216\n","Epoch: 5332, Train Loss: 0.03835611838905226, Test Loss: 0.03288397722452823\n","Epoch: 5333, Train Loss: 0.03835042006192729, Test Loss: 0.03287914955691939\n","Epoch: 5334, Train Loss: 0.038344720829888886, Test Loss: 0.032874321116905636\n","Epoch: 5335, Train Loss: 0.038339020692786135, Test Loss: 0.03286949190433758\n","Epoch: 5336, Train Loss: 0.038333319650468954, Test Loss: 0.03286466191906652\n","Epoch: 5337, Train Loss: 0.0383276177027879, Test Loss: 0.03285983116094424\n","Epoch: 5338, Train Loss: 0.03832191484959423, Test Loss: 0.03285499962982326\n","Epoch: 5339, Train Loss: 0.038316211090739974, Test Loss: 0.03285016732555664\n","Epoch: 5340, Train Loss: 0.038310506426077785, Test Loss: 0.0328453342479981\n","Epoch: 5341, Train Loss: 0.03830480085546107, Test Loss: 0.03284050039700199\n","Epoch: 5342, Train Loss: 0.03829909437874396, Test Loss: 0.03283566577242328\n","Epoch: 5343, Train Loss: 0.038293386995781255, Test Loss: 0.03283083037411749\n","Epoch: 5344, Train Loss: 0.03828767870642848, Test Loss: 0.03282599420194088\n","Epoch: 5345, Train Loss: 0.03828196951054185, Test Loss: 0.032821157255750265\n","Epoch: 5346, Train Loss: 0.038276259407978346, Test Loss: 0.032816319535403066\n","Epoch: 5347, Train Loss: 0.03827054839859555, Test Loss: 0.032811481040757345\n","Epoch: 5348, Train Loss: 0.03826483648225188, Test Loss: 0.03280664177167184\n","Epoch: 5349, Train Loss: 0.03825912365880638, Test Loss: 0.03280180172800582\n","Epoch: 5350, Train Loss: 0.03825340992811882, Test Loss: 0.032796960909619206\n","Epoch: 5351, Train Loss: 0.0382476952900497, Test Loss: 0.03279211931637262\n","Epoch: 5352, Train Loss: 0.0382419797444602, Test Loss: 0.03278727694812719\n","Epoch: 5353, Train Loss: 0.03823626329121223, Test Loss: 0.032782433804744736\n","Epoch: 5354, Train Loss: 0.038230545930168405, Test Loss: 0.03277758988608769\n","Epoch: 5355, Train Loss: 0.038224827661192055, Test Loss: 0.03277274519201912\n","Epoch: 5356, Train Loss: 0.03821910848414725, Test Loss: 0.032767899722402676\n","Epoch: 5357, Train Loss: 0.038213388398898686, Test Loss: 0.032763053477102656\n","Epoch: 5358, Train Loss: 0.03820766740531184, Test Loss: 0.032758206455984026\n","Epoch: 5359, Train Loss: 0.03820194550325289, Test Loss: 0.03275335865891227\n","Epoch: 5360, Train Loss: 0.03819622269258873, Test Loss: 0.03274851008575363\n","Epoch: 5361, Train Loss: 0.03819049897318693, Test Loss: 0.03274366073637488\n","Epoch: 5362, Train Loss: 0.03818477434491584, Test Loss: 0.03273881061064344\n","Epoch: 5363, Train Loss: 0.038179048807644465, Test Loss: 0.03273395970842736\n","Epoch: 5364, Train Loss: 0.0381733223612425, Test Loss: 0.032729108029595326\n","Epoch: 5365, Train Loss: 0.03816759500558043, Test Loss: 0.03272425557401662\n","Epoch: 5366, Train Loss: 0.03816186674052942, Test Loss: 0.03271940234156124\n","Epoch: 5367, Train Loss: 0.03815613756596134, Test Loss: 0.032714548332099636\n","Epoch: 5368, Train Loss: 0.038150407481748756, Test Loss: 0.03270969354550308\n","Epoch: 5369, Train Loss: 0.03814467648776501, Test Loss: 0.0327048379816433\n","Epoch: 5370, Train Loss: 0.03813894458388409, Test Loss: 0.032699981640392825\n","Epoch: 5371, Train Loss: 0.038133211769980746, Test Loss: 0.03269512452162461\n","Epoch: 5372, Train Loss: 0.03812747804593041, Test Loss: 0.03269026662521246\n","Epoch: 5373, Train Loss: 0.03812174341160927, Test Loss: 0.032685407951030566\n","Epoch: 5374, Train Loss: 0.03811600786689417, Test Loss: 0.03268054849895397\n","Epoch: 5375, Train Loss: 0.038110271411662705, Test Loss: 0.03267568826885819\n","Epoch: 5376, Train Loss: 0.038104534045793226, Test Loss: 0.03267082726061946\n","Epoch: 5377, Train Loss: 0.03809879576916471, Test Loss: 0.03266596547411458\n","Epoch: 5378, Train Loss: 0.03809305658165694, Test Loss: 0.032661102909221036\n","Epoch: 5379, Train Loss: 0.03808731648315035, Test Loss: 0.03265623956581686\n","Epoch: 5380, Train Loss: 0.038081575473526114, Test Loss: 0.032651375443780835\n","Epoch: 5381, Train Loss: 0.03807583355266614, Test Loss: 0.03264651054299223\n","Epoch: 5382, Train Loss: 0.03807009072045301, Test Loss: 0.03264164486333109\n","Epoch: 5383, Train Loss: 0.03806434697677009, Test Loss: 0.032636778404677935\n","Epoch: 5384, Train Loss: 0.03805860232150141, Test Loss: 0.03263191116691407\n","Epoch: 5385, Train Loss: 0.03805285675453173, Test Loss: 0.03262704314992129\n","Epoch: 5386, Train Loss: 0.03804711027574652, Test Loss: 0.03262217435358213\n","Epoch: 5387, Train Loss: 0.038041362885032015, Test Loss: 0.0326173047777797\n","Epoch: 5388, Train Loss: 0.0380356145822751, Test Loss: 0.032612434422397765\n","Epoch: 5389, Train Loss: 0.03802986536736341, Test Loss: 0.03260756328732062\n","Epoch: 5390, Train Loss: 0.038024115240185355, Test Loss: 0.03260269137243338\n","Epoch: 5391, Train Loss: 0.03801836420062995, Test Loss: 0.032597818677621626\n","Epoch: 5392, Train Loss: 0.03801261224858702, Test Loss: 0.03259294520277168\n","Epoch: 5393, Train Loss: 0.038006859383947086, Test Loss: 0.03258807094777036\n","Epoch: 5394, Train Loss: 0.03800110560660139, Test Loss: 0.032583195912505275\n","Epoch: 5395, Train Loss: 0.037995350916441864, Test Loss: 0.03257832009686455\n","Epoch: 5396, Train Loss: 0.037989595313361206, Test Loss: 0.032573443500736994\n","Epoch: 5397, Train Loss: 0.03798383879725281, Test Loss: 0.03256856612401206\n","Epoch: 5398, Train Loss: 0.03797808136801082, Test Loss: 0.03256368796657978\n","Epoch: 5399, Train Loss: 0.03797232302553002, Test Loss: 0.03255880902833084\n","Epoch: 5400, Train Loss: 0.037966563769706016, Test Loss: 0.032553929309156605\n","Epoch: 5401, Train Loss: 0.03796080360043509, Test Loss: 0.03254904880894898\n","Epoch: 5402, Train Loss: 0.03795504251761426, Test Loss: 0.03254416752760059\n","Epoch: 5403, Train Loss: 0.037949280521141236, Test Loss: 0.03253928546500465\n","Epoch: 5404, Train Loss: 0.03794351761091447, Test Loss: 0.03253440262105501\n","Epoch: 5405, Train Loss: 0.037937753786833124, Test Loss: 0.032529518995646156\n","Epoch: 5406, Train Loss: 0.03793198904879716, Test Loss: 0.03252463458867327\n","Epoch: 5407, Train Loss: 0.03792622339670714, Test Loss: 0.032519749400032\n","Epoch: 5408, Train Loss: 0.03792045683046442, Test Loss: 0.032514863429618845\n","Epoch: 5409, Train Loss: 0.03791468934997106, Test Loss: 0.03250997667733075\n","Epoch: 5410, Train Loss: 0.03790892095512988, Test Loss: 0.03250508914306541\n","Epoch: 5411, Train Loss: 0.03790315164584438, Test Loss: 0.03250020082672109\n","Epoch: 5412, Train Loss: 0.03789738142201881, Test Loss: 0.03249531172819675\n","Epoch: 5413, Train Loss: 0.03789161028355814, Test Loss: 0.03249042184739195\n","Epoch: 5414, Train Loss: 0.03788583823036804, Test Loss: 0.032485531184206856\n","Epoch: 5415, Train Loss: 0.03788006526235495, Test Loss: 0.03248063973854231\n","Epoch: 5416, Train Loss: 0.03787429137942599, Test Loss: 0.03247574751029982\n","Epoch: 5417, Train Loss: 0.03786851658148902, Test Loss: 0.03247085449938144\n","Epoch: 5418, Train Loss: 0.03786274086845267, Test Loss: 0.032465960705689925\n","Epoch: 5419, Train Loss: 0.037856964240226235, Test Loss: 0.032461066129128635\n","Epoch: 5420, Train Loss: 0.037851186696719774, Test Loss: 0.032456170769601625\n","Epoch: 5421, Train Loss: 0.03784540823784402, Test Loss: 0.03245127462701345\n","Epoch: 5422, Train Loss: 0.03783962886351052, Test Loss: 0.0324463777012695\n","Epoch: 5423, Train Loss: 0.03783384857363146, Test Loss: 0.03244147999227558\n","Epoch: 5424, Train Loss: 0.03782806736811982, Test Loss: 0.032436581499938374\n","Epoch: 5425, Train Loss: 0.03782228524688927, Test Loss: 0.03243168222416493\n","Epoch: 5426, Train Loss: 0.03781650220985422, Test Loss: 0.032426782164863194\n","Epoch: 5427, Train Loss: 0.0378107182569298, Test Loss: 0.03242188132194156\n","Epoch: 5428, Train Loss: 0.03780493338803186, Test Loss: 0.03241697969530917\n","Epoch: 5429, Train Loss: 0.03779914760307702, Test Loss: 0.032412077284875725\n","Epoch: 5430, Train Loss: 0.03779336090198258, Test Loss: 0.03240717409055166\n","Epoch: 5431, Train Loss: 0.037787573284666594, Test Loss: 0.03240227011224791\n","Epoch: 5432, Train Loss: 0.037781784751047834, Test Loss: 0.03239736534987622\n","Epoch: 5433, Train Loss: 0.037775995301045806, Test Loss: 0.032392459803348796\n","Epoch: 5434, Train Loss: 0.03777020493458075, Test Loss: 0.032387553472578635\n","Epoch: 5435, Train Loss: 0.037764413651573625, Test Loss: 0.032382646357479214\n","Epoch: 5436, Train Loss: 0.03775862145194617, Test Loss: 0.03237773845796488\n","Epoch: 5437, Train Loss: 0.03775282833562073, Test Loss: 0.0323728297739503\n","Epoch: 5438, Train Loss: 0.03774703430252052, Test Loss: 0.03236792030535115\n","Epoch: 5439, Train Loss: 0.037741239352569414, Test Loss: 0.03236301005208337\n","Epoch: 5440, Train Loss: 0.037735443485692, Test Loss: 0.03235809901406389\n","Epoch: 5441, Train Loss: 0.03772964670181366, Test Loss: 0.03235318719120992\n","Epoch: 5442, Train Loss: 0.037723849000860454, Test Loss: 0.03234827458343974\n","Epoch: 5443, Train Loss: 0.037718050382759216, Test Loss: 0.0323433611906718\n","Epoch: 5444, Train Loss: 0.03771225084743744, Test Loss: 0.032338447012825594\n","Epoch: 5445, Train Loss: 0.03770645039482345, Test Loss: 0.03233353204982092\n","Epoch: 5446, Train Loss: 0.037700649024846225, Test Loss: 0.03232861630157853\n","Epoch: 5447, Train Loss: 0.0376948467374355, Test Loss: 0.03232369976801956\n","Epoch: 5448, Train Loss: 0.037689043532521734, Test Loss: 0.03231878244906601\n","Epoch: 5449, Train Loss: 0.03768323941003615, Test Loss: 0.03231386434464023\n","Epoch: 5450, Train Loss: 0.037677434369910674, Test Loss: 0.03230894545466557\n","Epoch: 5451, Train Loss: 0.03767162841207798, Test Loss: 0.03230402577906565\n","Epoch: 5452, Train Loss: 0.03766582153647147, Test Loss: 0.03229910531776512\n","Epoch: 5453, Train Loss: 0.03766001374302526, Test Loss: 0.03229418407068886\n","Epoch: 5454, Train Loss: 0.03765420503167425, Test Loss: 0.03228926203776274\n","Epoch: 5455, Train Loss: 0.037648395402354005, Test Loss: 0.03228433921891302\n","Epoch: 5456, Train Loss: 0.03764258485500089, Test Loss: 0.032279415614066886\n","Epoch: 5457, Train Loss: 0.03763677338955194, Test Loss: 0.03227449122315176\n","Epoch: 5458, Train Loss: 0.037630961005945, Test Loss: 0.032269566046096175\n","Epoch: 5459, Train Loss: 0.037625147704118574, Test Loss: 0.032264640082828804\n","Epoch: 5460, Train Loss: 0.03761933348401195, Test Loss: 0.03225971333327955\n","Epoch: 5461, Train Loss: 0.03761351834556516, Test Loss: 0.032254785797378296\n","Epoch: 5462, Train Loss: 0.03760770228871894, Test Loss: 0.03224985747505625\n","Epoch: 5463, Train Loss: 0.037601885313414726, Test Loss: 0.03224492836624457\n","Epoch: 5464, Train Loss: 0.03759606741959477, Test Loss: 0.03223999847087573\n","Epoch: 5465, Train Loss: 0.03759024860720203, Test Loss: 0.032235067788882237\n","Epoch: 5466, Train Loss: 0.03758442887618016, Test Loss: 0.0322301363201978\n","Epoch: 5467, Train Loss: 0.037578608226473624, Test Loss: 0.032225204064756215\n","Epoch: 5468, Train Loss: 0.03757278665802756, Test Loss: 0.032220271022492565\n","Epoch: 5469, Train Loss: 0.037566964170787864, Test Loss: 0.03221533719334181\n","Epoch: 5470, Train Loss: 0.03756114076470118, Test Loss: 0.03221040257724038\n","Epoch: 5471, Train Loss: 0.037555316439714874, Test Loss: 0.032205467174124515\n","Epoch: 5472, Train Loss: 0.03754949119577705, Test Loss: 0.032200530983931934\n","Epoch: 5473, Train Loss: 0.037543665032836585, Test Loss: 0.032195594006600166\n","Epoch: 5474, Train Loss: 0.037537837950843, Test Loss: 0.03219065624206821\n","Epoch: 5475, Train Loss: 0.03753200994974666, Test Loss: 0.03218571769027489\n","Epoch: 5476, Train Loss: 0.03752618102949863, Test Loss: 0.03218077835116053\n","Epoch: 5477, Train Loss: 0.03752035119005069, Test Loss: 0.0321758382246652\n","Epoch: 5478, Train Loss: 0.03751452043135538, Test Loss: 0.03217089731073054\n","Epoch: 5479, Train Loss: 0.037508688753365986, Test Loss: 0.03216595560929785\n","Epoch: 5480, Train Loss: 0.0375028561560365, Test Loss: 0.03216101312031016\n","Epoch: 5481, Train Loss: 0.0374970226393217, Test Loss: 0.032156069843709986\n","Epoch: 5482, Train Loss: 0.037491188203177074, Test Loss: 0.032151125779441674\n","Epoch: 5483, Train Loss: 0.03748535284755883, Test Loss: 0.03214618092744902\n","Epoch: 5484, Train Loss: 0.03747951657242396, Test Loss: 0.03214123528767769\n","Epoch: 5485, Train Loss: 0.03747367937773018, Test Loss: 0.03213628886007275\n","Epoch: 5486, Train Loss: 0.03746784126343594, Test Loss: 0.032131341644581174\n","Epoch: 5487, Train Loss: 0.03746200222950042, Test Loss: 0.032126393641149255\n","Epoch: 5488, Train Loss: 0.03745616227588356, Test Loss: 0.03212144484972539\n","Epoch: 5489, Train Loss: 0.03745032140254603, Test Loss: 0.032116495270257076\n","Epoch: 5490, Train Loss: 0.03744447960944926, Test Loss: 0.03211154490269403\n","Epoch: 5491, Train Loss: 0.03743863689655538, Test Loss: 0.03210659374698494\n","Epoch: 5492, Train Loss: 0.03743279326382731, Test Loss: 0.03210164180308098\n","Epoch: 5493, Train Loss: 0.03742694871122866, Test Loss: 0.0320966890709321\n","Epoch: 5494, Train Loss: 0.03742110323872385, Test Loss: 0.0320917355504907\n","Epoch: 5495, Train Loss: 0.03741525684627796, Test Loss: 0.03208678124170802\n","Epoch: 5496, Train Loss: 0.03740940953385687, Test Loss: 0.032081826144537834\n","Epoch: 5497, Train Loss: 0.0374035613014272, Test Loss: 0.032076870258932584\n","Epoch: 5498, Train Loss: 0.03739771214895626, Test Loss: 0.032071913584847477\n","Epoch: 5499, Train Loss: 0.037391862076412194, Test Loss: 0.03206695612223608\n","Epoch: 5500, Train Loss: 0.037386011083763784, Test Loss: 0.03206199787105505\n","Epoch: 5501, Train Loss: 0.03738015917098064, Test Loss: 0.032057038831259134\n","Epoch: 5502, Train Loss: 0.03737430633803306, Test Loss: 0.03205207900280639\n","Epoch: 5503, Train Loss: 0.03736845258489213, Test Loss: 0.03204711838565274\n","Epoch: 5504, Train Loss: 0.03736259791152964, Test Loss: 0.03204215697975766\n","Epoch: 5505, Train Loss: 0.037356742317918135, Test Loss: 0.03203719478507829\n","Epoch: 5506, Train Loss: 0.03735088580403092, Test Loss: 0.032032231801575474\n","Epoch: 5507, Train Loss: 0.03734502836984203, Test Loss: 0.032027268029207544\n","Epoch: 5508, Train Loss: 0.03733917001532623, Test Loss: 0.03202230346793678\n","Epoch: 5509, Train Loss: 0.03733331074045909, Test Loss: 0.03201733811772268\n","Epoch: 5510, Train Loss: 0.03732745054521682, Test Loss: 0.03201237197852898\n","Epoch: 5511, Train Loss: 0.03732158942957649, Test Loss: 0.03200740505031625\n","Epoch: 5512, Train Loss: 0.03731572739351585, Test Loss: 0.03200243733304974\n","Epoch: 5513, Train Loss: 0.03730986443701338, Test Loss: 0.031997468826691196\n","Epoch: 5514, Train Loss: 0.03730400056004835, Test Loss: 0.03199249953120731\n","Epoch: 5515, Train Loss: 0.037298135762600736, Test Loss: 0.03198752944656091\n","Epoch: 5516, Train Loss: 0.03729227004465133, Test Loss: 0.03198255857272018\n","Epoch: 5517, Train Loss: 0.03728640340618158, Test Loss: 0.03197758690964908\n","Epoch: 5518, Train Loss: 0.03728053584717371, Test Loss: 0.03197261445731736\n","Epoch: 5519, Train Loss: 0.037274667367610724, Test Loss: 0.031967641215689964\n","Epoch: 5520, Train Loss: 0.03726879796747636, Test Loss: 0.03196266718473823\n","Epoch: 5521, Train Loss: 0.03726292764675506, Test Loss: 0.03195769236442813\n","Epoch: 5522, Train Loss: 0.03725705640543207, Test Loss: 0.031952716754732616\n","Epoch: 5523, Train Loss: 0.03725118424349334, Test Loss: 0.0319477403556186\n","Epoch: 5524, Train Loss: 0.03724531116092561, Test Loss: 0.031942763167060684\n","Epoch: 5525, Train Loss: 0.0372394371577163, Test Loss: 0.03193778518902679\n","Epoch: 5526, Train Loss: 0.03723356223385365, Test Loss: 0.03193280642149311\n","Epoch: 5527, Train Loss: 0.03722768638932661, Test Loss: 0.03192782686442858\n","Epoch: 5528, Train Loss: 0.03722180962412487, Test Loss: 0.03192284651781101\n","Epoch: 5529, Train Loss: 0.03721593193823891, Test Loss: 0.031917865381610226\n","Epoch: 5530, Train Loss: 0.03721005333165992, Test Loss: 0.03191288345580586\n","Epoch: 5531, Train Loss: 0.03720417380437983, Test Loss: 0.03190790074036852\n","Epoch: 5532, Train Loss: 0.03719829335639135, Test Loss: 0.031902917235279646\n","Epoch: 5533, Train Loss: 0.03719241198768794, Test Loss: 0.031897932940510604\n","Epoch: 5534, Train Loss: 0.03718652969826379, Test Loss: 0.03189294785604473\n","Epoch: 5535, Train Loss: 0.03718064648811381, Test Loss: 0.031887961981854084\n","Epoch: 5536, Train Loss: 0.03717476235723373, Test Loss: 0.031882975317924\n","Epoch: 5537, Train Loss: 0.03716887730561999, Test Loss: 0.03187798786422711\n","Epoch: 5538, Train Loss: 0.03716299133326977, Test Loss: 0.03187299962075071\n","Epoch: 5539, Train Loss: 0.03715710444018102, Test Loss: 0.03186801058746809\n","Epoch: 5540, Train Loss: 0.03715121662635242, Test Loss: 0.03186302076436865\n","Epoch: 5541, Train Loss: 0.03714532789178341, Test Loss: 0.031858030151426096\n","Epoch: 5542, Train Loss: 0.037139438236474205, Test Loss: 0.03185303874863203\n","Epoch: 5543, Train Loss: 0.03713354766042572, Test Loss: 0.03184804655596049\n","Epoch: 5544, Train Loss: 0.03712765616363965, Test Loss: 0.031843053573405444\n","Epoch: 5545, Train Loss: 0.03712176374611845, Test Loss: 0.031838059800941225\n","Epoch: 5546, Train Loss: 0.03711587040786532, Test Loss: 0.031833065238564134\n","Epoch: 5547, Train Loss: 0.03710997614888417, Test Loss: 0.03182806988624865\n","Epoch: 5548, Train Loss: 0.037104080969179734, Test Loss: 0.031823073743993645\n","Epoch: 5549, Train Loss: 0.03709818486875743, Test Loss: 0.03181807681177359\n","Epoch: 5550, Train Loss: 0.037092287847623494, Test Loss: 0.0318130790895901\n","Epoch: 5551, Train Loss: 0.037086389905784826, Test Loss: 0.03180808057741742\n","Epoch: 5552, Train Loss: 0.03708049104324918, Test Loss: 0.03180308127526004\n","Epoch: 5553, Train Loss: 0.03707459126002495, Test Loss: 0.03179808118309188\n","Epoch: 5554, Train Loss: 0.03706869055612141, Test Loss: 0.0317930803009205\n","Epoch: 5555, Train Loss: 0.037062788931548456, Test Loss: 0.03178807862871928\n","Epoch: 5556, Train Loss: 0.03705688638631684, Test Loss: 0.031783076166499044\n","Epoch: 5557, Train Loss: 0.037050982920438, Test Loss: 0.03177807291423237\n","Epoch: 5558, Train Loss: 0.03704507853392416, Test Loss: 0.031773068871933687\n","Epoch: 5559, Train Loss: 0.03703917322678829, Test Loss: 0.031768064039574405\n","Epoch: 5560, Train Loss: 0.037033266999044114, Test Loss: 0.0317630584171729\n","Epoch: 5561, Train Loss: 0.03702735985070612, Test Loss: 0.03175805200469917\n","Epoch: 5562, Train Loss: 0.03702145178178952, Test Loss: 0.03175304480217572\n","Epoch: 5563, Train Loss: 0.037015542792310285, Test Loss: 0.03174803680957085\n","Epoch: 5564, Train Loss: 0.03700963288228517, Test Loss: 0.03174302802691168\n","Epoch: 5565, Train Loss: 0.03700372205173164, Test Loss: 0.031738018454164255\n","Epoch: 5566, Train Loss: 0.03699781030066796, Test Loss: 0.031733008091360766\n","Epoch: 5567, Train Loss: 0.036991897629113125, Test Loss: 0.031727996938464606\n","Epoch: 5568, Train Loss: 0.03698598403708688, Test Loss: 0.03172298499551354\n","Epoch: 5569, Train Loss: 0.036980069524609735, Test Loss: 0.03171797226246767\n","Epoch: 5570, Train Loss: 0.036974154091702946, Test Loss: 0.03171295873937097\n","Epoch: 5571, Train Loss: 0.03696823773838852, Test Loss: 0.03170794442617975\n","Epoch: 5572, Train Loss: 0.03696232046468924, Test Loss: 0.031702929322944676\n","Epoch: 5573, Train Loss: 0.036956402270628644, Test Loss: 0.031697913429617565\n","Epoch: 5574, Train Loss: 0.03695048315623098, Test Loss: 0.03169289674625668\n","Epoch: 5575, Train Loss: 0.0369445631215213, Test Loss: 0.03168787927280847\n","Epoch: 5576, Train Loss: 0.036938642166525384, Test Loss: 0.031682861009339626\n","Epoch: 5577, Train Loss: 0.0369327202912698, Test Loss: 0.03167784195579029\n","Epoch: 5578, Train Loss: 0.03692679749578183, Test Loss: 0.03167282211223657\n","Epoch: 5579, Train Loss: 0.036920873780089535, Test Loss: 0.03166780147861136\n","Epoch: 5580, Train Loss: 0.03691494914422172, Test Loss: 0.03166278005500121\n","Epoch: 5581, Train Loss: 0.03690902358820799, Test Loss: 0.03165775784133054\n","Epoch: 5582, Train Loss: 0.03690309711207865, Test Loss: 0.031652734837697726\n","Epoch: 5583, Train Loss: 0.03689716971586476, Test Loss: 0.03164771104401724\n","Epoch: 5584, Train Loss: 0.036891241399598186, Test Loss: 0.03164268646040084\n","Epoch: 5585, Train Loss: 0.036885312163311514, Test Loss: 0.03163766108675141\n","Epoch: 5586, Train Loss: 0.036879382007038115, Test Loss: 0.03163263492319582\n","Epoch: 5587, Train Loss: 0.03687345093081206, Test Loss: 0.03162760796962351\n","Epoch: 5588, Train Loss: 0.036867518934668254, Test Loss: 0.031622580226178476\n","Epoch: 5589, Train Loss: 0.0368615860186423, Test Loss: 0.03161755169273454\n","Epoch: 5590, Train Loss: 0.036855652182770586, Test Loss: 0.03161252236945516\n","Epoch: 5591, Train Loss: 0.03684971742709025, Test Loss: 0.03160749225619605\n","Epoch: 5592, Train Loss: 0.03684378175163919, Test Loss: 0.031602461353142784\n","Epoch: 5593, Train Loss: 0.03683784515645606, Test Loss: 0.031597429660130134\n","Epoch: 5594, Train Loss: 0.036831907641580265, Test Loss: 0.03159239717736882\n","Epoch: 5595, Train Loss: 0.036825969207052, Test Loss: 0.03158736390466938\n","Epoch: 5596, Train Loss: 0.03682002985291215, Test Loss: 0.03158232984227137\n","Epoch: 5597, Train Loss: 0.036814089579202444, Test Loss: 0.03157729498995703\n","Epoch: 5598, Train Loss: 0.0368081483859653, Test Loss: 0.03157225934799897\n","Epoch: 5599, Train Loss: 0.03680220627324395, Test Loss: 0.031567222916146775\n","Epoch: 5600, Train Loss: 0.03679626324108232, Test Loss: 0.0315621856947108\n","Epoch: 5601, Train Loss: 0.03679031928952517, Test Loss: 0.03155714768340286\n","Epoch: 5602, Train Loss: 0.036784374418617954, Test Loss: 0.03155210888257663\n","Epoch: 5603, Train Loss: 0.03677842862840691, Test Loss: 0.031547069291900105\n","Epoch: 5604, Train Loss: 0.036772481918939066, Test Loss: 0.03154202891177683\n","Epoch: 5605, Train Loss: 0.03676653429026215, Test Loss: 0.03153698774182389\n","Epoch: 5606, Train Loss: 0.036760585742424697, Test Loss: 0.03153194578250229\n","Epoch: 5607, Train Loss: 0.03675463627547598, Test Loss: 0.031526903033370114\n","Epoch: 5608, Train Loss: 0.03674868588946604, Test Loss: 0.031521859494954556\n","Epoch: 5609, Train Loss: 0.036742734584445644, Test Loss: 0.03151681516674522\n","Epoch: 5610, Train Loss: 0.03673678236046642, Test Loss: 0.03151177004934575\n","Epoch: 5611, Train Loss: 0.03673082921758061, Test Loss: 0.03150672414216619\n","Epoch: 5612, Train Loss: 0.036724875155841316, Test Loss: 0.03150167744589867\n","Epoch: 5613, Train Loss: 0.0367189201753024, Test Loss: 0.0314966299598606\n","Epoch: 5614, Train Loss: 0.03671296427601844, Test Loss: 0.03149158168484661\n","Epoch: 5615, Train Loss: 0.036707007458044796, Test Loss: 0.03148653262006648\n","Epoch: 5616, Train Loss: 0.03670104972143761, Test Loss: 0.0314814827664336\n","Epoch: 5617, Train Loss: 0.03669509106625371, Test Loss: 0.03147643212303244\n","Epoch: 5618, Train Loss: 0.03668913149255079, Test Loss: 0.03147138069091426\n","Epoch: 5619, Train Loss: 0.036683171000387226, Test Loss: 0.031466328469017646\n","Epoch: 5620, Train Loss: 0.03667720958982221, Test Loss: 0.03146127545855394\n","Epoch: 5621, Train Loss: 0.03667124726091562, Test Loss: 0.03145622165829167\n","Epoch: 5622, Train Loss: 0.0366652840137282, Test Loss: 0.0314511670696286\n","Epoch: 5623, Train Loss: 0.03665931984832136, Test Loss: 0.031446111691134676\n","Epoch: 5624, Train Loss: 0.036653354764757326, Test Loss: 0.031441055524424846\n","Epoch: 5625, Train Loss: 0.03664738876309905, Test Loss: 0.031435998567837305\n","Epoch: 5626, Train Loss: 0.0366414218434103, Test Loss: 0.031430940823240126\n","Epoch: 5627, Train Loss: 0.03663545400575553, Test Loss: 0.03142588228870066\n","Epoch: 5628, Train Loss: 0.03662948525020004, Test Loss: 0.03142082296638252\n","Epoch: 5629, Train Loss: 0.03662351557680983, Test Loss: 0.03141576285403628\n","Epoch: 5630, Train Loss: 0.03661754498565169, Test Loss: 0.03141070195417093\n","Epoch: 5631, Train Loss: 0.03661157347679316, Test Loss: 0.03140564026416613\n","Epoch: 5632, Train Loss: 0.036605601050302555, Test Loss: 0.031400577786935016\n","Epoch: 5633, Train Loss: 0.03659962770624891, Test Loss: 0.031395514519422546\n","Epoch: 5634, Train Loss: 0.03659365344470214, Test Loss: 0.03139045046501531\n","Epoch: 5635, Train Loss: 0.03658767826573275, Test Loss: 0.03138538562014832\n","Epoch: 5636, Train Loss: 0.03658170216941218, Test Loss: 0.031380319988763165\n","Epoch: 5637, Train Loss: 0.03657572515581247, Test Loss: 0.03137525356669643\n","Epoch: 5638, Train Loss: 0.036569747225006596, Test Loss: 0.031370186358541004\n","Epoch: 5639, Train Loss: 0.03656376837706813, Test Loss: 0.031365118359430164\n","Epoch: 5640, Train Loss: 0.036557788612071554, Test Loss: 0.03136004957472213\n","Epoch: 5641, Train Loss: 0.03655180793009195, Test Loss: 0.031354979998722964\n","Epoch: 5642, Train Loss: 0.03654582633120539, Test Loss: 0.03134990963769102\n","Epoch: 5643, Train Loss: 0.03653984381548842, Test Loss: 0.031344838484958365\n","Epoch: 5644, Train Loss: 0.03653386038301869, Test Loss: 0.03133976654784327\n","Epoch: 5645, Train Loss: 0.03652787603387424, Test Loss: 0.03133469381853\n","Epoch: 5646, Train Loss: 0.03652189076813425, Test Loss: 0.03132962030558585\n","Epoch: 5647, Train Loss: 0.036515904585878306, Test Loss: 0.031324545999841205\n","Epoch: 5648, Train Loss: 0.03650991748718713, Test Loss: 0.03131947091133707\n","Epoch: 5649, Train Loss: 0.03650392947214177, Test Loss: 0.03131439502930518\n","Epoch: 5650, Train Loss: 0.03649794054082454, Test Loss: 0.03130931836552698\n","Epoch: 5651, Train Loss: 0.036491950693318, Test Loss: 0.03130424090734466\n","Epoch: 5652, Train Loss: 0.036485959929706005, Test Loss: 0.0312991626685973\n","Epoch: 5653, Train Loss: 0.03647996825007262, Test Loss: 0.031294083634391665\n","Epoch: 5654, Train Loss: 0.036473975654503235, Test Loss: 0.031289003821001866\n","Epoch: 5655, Train Loss: 0.03646798214308345, Test Loss: 0.03128392321088736\n","Epoch: 5656, Train Loss: 0.03646198771590022, Test Loss: 0.031278841823206824\n","Epoch: 5657, Train Loss: 0.036455992373040584, Test Loss: 0.03127375963728161\n","Epoch: 5658, Train Loss: 0.03644999611459314, Test Loss: 0.03126867667569101\n","Epoch: 5659, Train Loss: 0.03644399894064636, Test Loss: 0.03126359291403271\n","Epoch: 5660, Train Loss: 0.03643800085129043, Test Loss: 0.031258508378946365\n","Epoch: 5661, Train Loss: 0.03643200184661534, Test Loss: 0.03125342304160683\n","Epoch: 5662, Train Loss: 0.036426001926712845, Test Loss: 0.03124833693347849\n","Epoch: 5663, Train Loss: 0.036420001091674335, Test Loss: 0.031243250020477554\n","Epoch: 5664, Train Loss: 0.03641399934159327, Test Loss: 0.031238162339807155\n","Epoch: 5665, Train Loss: 0.036407996676562464, Test Loss: 0.0312330738511251\n","Epoch: 5666, Train Loss: 0.03640199309667696, Test Loss: 0.031227984598467227\n","Epoch: 5667, Train Loss: 0.03639598860203101, Test Loss: 0.031222894534035497\n","Epoch: 5668, Train Loss: 0.03638998319272134, Test Loss: 0.031217803710009435\n","Epoch: 5669, Train Loss: 0.036383976868843565, Test Loss: 0.031212712069699673\n","Epoch: 5670, Train Loss: 0.03637796963049615, Test Loss: 0.031207619675001616\n","Epoch: 5671, Train Loss: 0.036371961477776014, Test Loss: 0.031202526458612045\n","Epoch: 5672, Train Loss: 0.03636595241078337, Test Loss: 0.031197432494029998\n","Epoch: 5673, Train Loss: 0.03635994242961647, Test Loss: 0.03119233770126925\n","Epoch: 5674, Train Loss: 0.036353931534377236, Test Loss: 0.031187242167700812\n","Epoch: 5675, Train Loss: 0.03634791972516526, Test Loss: 0.031182145798168102\n","Epoch: 5676, Train Loss: 0.0363419070020843, Test Loss: 0.031177048696642538\n","Epoch: 5677, Train Loss: 0.03633589336523509, Test Loss: 0.03117195074980343\n","Epoch: 5678, Train Loss: 0.036329878814723306, Test Loss: 0.031166852081508118\n","Epoch: 5679, Train Loss: 0.03632386335065085, Test Loss: 0.03116175255666538\n","Epoch: 5680, Train Loss: 0.036317846973125324, Test Loss: 0.031156652322978224\n","Epoch: 5681, Train Loss: 0.036311829682249754, Test Loss: 0.031151551219235993\n","Epoch: 5682, Train Loss: 0.03630581147813372, Test Loss: 0.031146449421764758\n","Epoch: 5683, Train Loss: 0.03629979236088127, Test Loss: 0.03114134673798515\n","Epoch: 5684, Train Loss: 0.036293772330604115, Test Loss: 0.031136243378615577\n","Epoch: 5685, Train Loss: 0.03628775138740715, Test Loss: 0.031131139113365465\n","Epoch: 5686, Train Loss: 0.03628172953140438, Test Loss: 0.031126034194319984\n","Epoch: 5687, Train Loss: 0.03627570676270144, Test Loss: 0.031120928345806338\n","Epoch: 5688, Train Loss: 0.03626968308141471, Test Loss: 0.031115821869715527\n","Epoch: 5689, Train Loss: 0.03626365848765045, Test Loss: 0.03111071443570603\n","Epoch: 5690, Train Loss: 0.036257632981527635, Test Loss: 0.031105606405696515\n","Epoch: 5691, Train Loss: 0.036251606563152836, Test Loss: 0.03110049738342268\n","Epoch: 5692, Train Loss: 0.03624557923264789, Test Loss: 0.031095387803224194\n","Epoch: 5693, Train Loss: 0.03623955099011948, Test Loss: 0.031090277189262706\n","Epoch: 5694, Train Loss: 0.036233521835692555, Test Loss: 0.031085166063339706\n","Epoch: 5695, Train Loss: 0.03622749176947365, Test Loss: 0.03108005385346661\n","Epoch: 5696, Train Loss: 0.036221460791591026, Test Loss: 0.031074941187179423\n","Epoch: 5697, Train Loss: 0.03621542890215084, Test Loss: 0.0310698273761919\n","Epoch: 5698, Train Loss: 0.036209396101285055, Test Loss: 0.031064713175994612\n","Epoch: 5699, Train Loss: 0.03620336238909893, Test Loss: 0.031059597757491286\n","Epoch: 5700, Train Loss: 0.036197327765728635, Test Loss: 0.031054482031175046\n","Epoch: 5701, Train Loss: 0.0361912922312781, Test Loss: 0.031049364997286468\n","Epoch: 5702, Train Loss: 0.03618525578588823, Test Loss: 0.03104424775427853\n","Epoch: 5703, Train Loss: 0.03617921842966094, Test Loss: 0.031039129095335412\n","Epoch: 5704, Train Loss: 0.03617318016274261, Test Loss: 0.03103401034706734\n","Epoch: 5705, Train Loss: 0.036167140985232396, Test Loss: 0.031028890051191745\n","Epoch: 5706, Train Loss: 0.036161100897283, Test Loss: 0.03102376981155338\n","Epoch: 5707, Train Loss: 0.03615505989898988, Test Loss: 0.031018647864154782\n","Epoch: 5708, Train Loss: 0.0361490179905131, Test Loss: 0.03101352615005401\n","Epoch: 5709, Train Loss: 0.03614297517194332, Test Loss: 0.03100840253320761\n","Epoch: 5710, Train Loss: 0.03613693144344922, Test Loss: 0.031003279365261396\n","Epoch: 5711, Train Loss: 0.036130886805115295, Test Loss: 0.030998154056939717\n","Epoch: 5712, Train Loss: 0.036124841257120364, Test Loss: 0.03099302946032848\n","Epoch: 5713, Train Loss: 0.03611879479954113, Test Loss: 0.030987902433451845\n","Epoch: 5714, Train Loss: 0.03611274743256847, Test Loss: 0.030982776438976217\n","Epoch: 5715, Train Loss: 0.03610669915626921, Test Loss: 0.030977647660237095\n","Epoch: 5716, Train Loss: 0.03610064997084867, Test Loss: 0.03097252030562637\n","Epoch: 5717, Train Loss: 0.03609459987636134, Test Loss: 0.030967389734033332\n","Epoch: 5718, Train Loss: 0.03608854887302982, Test Loss: 0.030962261065567525\n","Epoch: 5719, Train Loss: 0.03608249696089325, Test Loss: 0.030957128650639428\n","Epoch: 5720, Train Loss: 0.036076444140195105, Test Loss: 0.03095199872516158\n","Epoch: 5721, Train Loss: 0.03607039041095549, Test Loss: 0.0309468644046866\n","Epoch: 5722, Train Loss: 0.03606433577344314, Test Loss: 0.030941733292101904\n","Epoch: 5723, Train Loss: 0.03605828022765464, Test Loss: 0.030936596989352968\n","Epoch: 5724, Train Loss: 0.03605222377388951, Test Loss: 0.030931464775735606\n","Epoch: 5725, Train Loss: 0.03604616641211531, Test Loss: 0.030926326396007876\n","Epoch: 5726, Train Loss: 0.03604010814266919, Test Loss: 0.030921193187466976\n","Epoch: 5727, Train Loss: 0.03603404896548306, Test Loss: 0.03091605261376786\n","Epoch: 5728, Train Loss: 0.0360279888809402, Test Loss: 0.030910918541262634\n","Epoch: 5729, Train Loss: 0.03602192788892883, Test Loss: 0.030905775628941767\n","Epoch: 5730, Train Loss: 0.036015865989889144, Test Loss: 0.03090064085428544\n","Epoch: 5731, Train Loss: 0.03600980318365592, Test Loss: 0.030895495424337588\n","Epoch: 5732, Train Loss: 0.03600373947073977, Test Loss: 0.030890360147690584\n","Epoch: 5733, Train Loss: 0.035997674850910384, Test Loss: 0.03088521197839586\n","Epoch: 5734, Train Loss: 0.035991609324766034, Test Loss: 0.030880076447626942\n","Epoch: 5735, Train Loss: 0.0359855428919973, Test Loss: 0.030874925264105374\n","Epoch: 5736, Train Loss: 0.035979475553312094, Test Loss: 0.030869789786499086\n","Epoch: 5737, Train Loss: 0.03597340730830552, Test Loss: 0.030864635247647574\n","Epoch: 5738, Train Loss: 0.03596733815782329, Test Loss: 0.030859500204559995\n","Epoch: 5739, Train Loss: 0.03596126810134598, Test Loss: 0.030854341886700273\n","Epoch: 5740, Train Loss: 0.035955197139893695, Test Loss: 0.030849207751925838\n","Epoch: 5741, Train Loss: 0.03594912527281081, Test Loss: 0.030844045128316858\n","Epoch: 5742, Train Loss: 0.03594305250133978, Test Loss: 0.030838912491129276\n","Epoch: 5743, Train Loss: 0.03593697882466484, Test Loss: 0.03083374490627572\n","Epoch: 5744, Train Loss: 0.03593090424431426, Test Loss: 0.030828614500364313\n","Epoch: 5745, Train Loss: 0.035924828759287375, Test Loss: 0.030823441137771208\n","Epoch: 5746, Train Loss: 0.035918752371483134, Test Loss: 0.030818313877620986\n","Epoch: 5747, Train Loss: 0.035912675079692535, Test Loss: 0.030813133719288288\n","Epoch: 5748, Train Loss: 0.035906596886300476, Test Loss: 0.030808010745970886\n","Epoch: 5749, Train Loss: 0.03590051778987218, Test Loss: 0.030802822521471887\n","Epoch: 5750, Train Loss: 0.035894437793436956, Test Loss: 0.030797705260349122\n","Epoch: 5751, Train Loss: 0.03588835689533102, Test Loss: 0.030792507382764914\n","Epoch: 5752, Train Loss: 0.035882275099448746, Test Loss: 0.030787397616293635\n","Epoch: 5753, Train Loss: 0.03587619240392243, Test Loss: 0.030782188101554198\n","Epoch: 5754, Train Loss: 0.03587010881382374, Test Loss: 0.03077708806126233\n","Epoch: 5755, Train Loss: 0.03586402432715802, Test Loss: 0.030771864426532052\n","Epoch: 5756, Train Loss: 0.03585793895062185, Test Loss: 0.030766776909373135\n","Epoch: 5757, Train Loss: 0.03585185268226223, Test Loss: 0.03076153604496639\n","Epoch: 5758, Train Loss: 0.0358457655310513, Test Loss: 0.0307564645607282\n","Epoch: 5759, Train Loss: 0.035839677495403024, Test Loss: 0.03075120256859043\n","Epoch: 5760, Train Loss: 0.03583358858752349, Test Loss: 0.03074615152694168\n","Epoch: 5761, Train Loss: 0.03582749880678237, Test Loss: 0.030740863516909633\n","Epoch: 5762, Train Loss: 0.0358214081700481, Test Loss: 0.030735838465153376\n","Epoch: 5763, Train Loss: 0.03581531667867259, Test Loss: 0.030730518297932528\n","Epoch: 5764, Train Loss: 0.035809224356339396, Test Loss: 0.030725526223791225\n","Epoch: 5765, Train Loss: 0.03580313120812987, Test Loss: 0.030720166186760434\n","Epoch: 5766, Train Loss: 0.035797037267819964, Test Loss: 0.0307152159048083\n","Epoch: 5767, Train Loss: 0.03579094254714767, Test Loss: 0.030709806303278936\n","Epoch: 5768, Train Loss: 0.03578484709501483, Test Loss: 0.030704908949330492\n","Epoch: 5769, Train Loss: 0.035778750934668466, Test Loss: 0.03069943759164815\n","Epoch: 5770, Train Loss: 0.035772654137927795, Test Loss: 0.030694607257024997\n","Epoch: 5771, Train Loss: 0.035766556747535004, Test Loss: 0.030689058806831683\n","Epoch: 5772, Train Loss: 0.03576045887037037, Test Loss: 0.030684313354706973\n","Epoch: 5773, Train Loss: 0.035754360581751854, Test Loss: 0.030678668517770766\n","Epoch: 5774, Train Loss: 0.03574826204266104, Test Loss: 0.030674030637803124\n","Epoch: 5775, Train Loss: 0.03574216338235229, Test Loss: 0.030668265144206022\n","Epoch: 5776, Train Loss: 0.03573606484591612, Test Loss: 0.03066376372100791\n","Epoch: 5777, Train Loss: 0.035729966651366976, Test Loss: 0.03065784705657231\n","Epoch: 5778, Train Loss: 0.03572386917540646, Test Loss: 0.030653518954567426\n","Epoch: 5779, Train Loss: 0.03571777278154467, Test Loss: 0.030647412789124917\n","Epoch: 5780, Train Loss: 0.035711678053575883, Test Loss: 0.03064330519460628\n","Epoch: 5781, Train Loss: 0.03570558559295848, Test Loss: 0.03063696145091299\n","Epoch: 5782, Train Loss: 0.03569949631090446, Test Loss: 0.030633134967087815\n","Epoch: 5783, Train Loss: 0.03569341119760814, Test Loss: 0.03062649347632752\n","Epoch: 5784, Train Loss: 0.03568733168401683, Test Loss: 0.030623026247313827\n","Epoch: 5785, Train Loss: 0.03568125939535771, Test Loss: 0.030616011951441115\n","Epoch: 5786, Train Loss: 0.03567519659034412, Test Loss: 0.030613005209883266\n","Epoch: 5787, Train Loss: 0.03566914593236128, Test Loss: 0.030605524908554223\n","Epoch: 5788, Train Loss: 0.03566311100201794, Test Loss: 0.03060311051985955\n","Epoch: 5789, Train Loss: 0.035657096162375286, Test Loss: 0.030595049239483862\n","Epoch: 5790, Train Loss: 0.03565110710934904, Test Loss: 0.030593400087421833\n","Epoch: 5791, Train Loss: 0.035645150994609326, Test Loss: 0.030584617304766706\n","Epoch: 5792, Train Loss: 0.03563923690367515, Test Loss: 0.030583961782774397\n","Epoch: 5793, Train Loss: 0.035633376575987336, Test Loss: 0.030574288021494134\n","Epoch: 5794, Train Loss: 0.03562758453217338, Test Loss: 0.030574930549965653\n","Epoch: 5795, Train Loss: 0.03562188008499771, Test Loss: 0.0305641653802486\n","Epoch: 5796, Train Loss: 0.03561628646846356, Test Loss: 0.03056651590698238\n","Epoch: 5797, Train Loss: 0.03561083554802478, Test Loss: 0.030554429276340316\n","Epoch: 5798, Train Loss: 0.03560556450977741, Test Loss: 0.030559046373001178\n","Epoch: 5799, Train Loss: 0.035600526125351806, Test Loss: 0.0305453867497382\n","Epoch: 5800, Train Loss: 0.03559577986530203, Test Loss: 0.03055304158472828\n","Epoch: 5801, Train Loss: 0.03559141351661842, Test Loss: 0.03053755705790034\n","Epoch: 5802, Train Loss: 0.03558752199259753, Test Loss: 0.03054932985810571\n","Epoch: 5803, Train Loss: 0.03558425211265285, Test Loss: 0.03053181286854567\n","Epoch: 5804, Train Loss: 0.03558175479016386, Test Loss: 0.030549240567262135\n","Epoch: 5805, Train Loss: 0.0355802771256635, Test Loss: 0.03052961461472544\n","Epoch: 5806, Train Loss: 0.035580057635626286, Test Loss: 0.03055492004166291\n","Epoch: 5807, Train Loss: 0.035581515265926546, Test Loss: 0.03053339964467795\n","Epoch: 5808, Train Loss: 0.035585023530418314, Test Loss: 0.030569851902456726\n","Epoch: 5809, Train Loss: 0.0355912987991067, Test Loss: 0.03054722880606847\n","Epoch: 5810, Train Loss: 0.035600917880689024, Test Loss: 0.030599716535413\n","Epoch: 5811, Train Loss: 0.03561511772003756, Test Loss: 0.030577861492762964\n","Epoch: 5812, Train Loss: 0.03563477020989083, Test Loss: 0.03065381420229418\n","Epoch: 5813, Train Loss: 0.035662034863880206, Test Loss: 0.03063654412690916\n","Epoch: 5814, Train Loss: 0.03569818558035703, Test Loss: 0.03074742616997728\n","Epoch: 5815, Train Loss: 0.035747039230785815, Test Loss: 0.030741986395063727\n","Epoch: 5816, Train Loss: 0.03581035259012281, Test Loss: 0.030905737728856943\n","Epoch: 5817, Train Loss: 0.03589496342988144, Test Loss: 0.030925312476534923\n","Epoch: 5818, Train Loss: 0.03600303869314793, Test Loss: 0.03117035983882193\n","Epoch: 5819, Train Loss: 0.03614700619472593, Test Loss: 0.031238286299395835\n","Epoch: 5820, Train Loss: 0.03632887645262355, Test Loss: 0.0316101613826185\n","Epoch: 5821, Train Loss: 0.036571580257183434, Test Loss: 0.03176693110762049\n","Epoch: 5822, Train Loss: 0.0368750662317984, Test Loss: 0.03233920391374382\n","Epoch: 5823, Train Loss: 0.03728229330677038, Test Loss: 0.03265393547177882\n","Epoch: 5824, Train Loss: 0.03778589129300538, Test Loss: 0.033546224934820935\n","Epoch: 5825, Train Loss: 0.03846753738683708, Test Loss: 0.03413507457797153\n","Epoch: 5826, Train Loss: 0.03929927016959229, Test Loss: 0.03554243301042154\n","Epoch: 5827, Train Loss: 0.04043850230957825, Test Loss: 0.03659713839963082\n","Epoch: 5828, Train Loss: 0.04180480860545233, Test Loss: 0.0388369715217773\n","Epoch: 5829, Train Loss: 0.04370505772606041, Test Loss: 0.0406664564671816\n","Epoch: 5830, Train Loss: 0.04593234241684388, Test Loss: 0.04425022848596684\n","Epoch: 5831, Train Loss: 0.04908981276157218, Test Loss: 0.047334129135297255\n","Epoch: 5832, Train Loss: 0.05267681405268359, Test Loss: 0.0530674177853869\n","Epoch: 5833, Train Loss: 0.05788292629908851, Test Loss: 0.05810552141373595\n","Epoch: 5834, Train Loss: 0.0635464046154031, Test Loss: 0.06720034901875861\n","Epoch: 5835, Train Loss: 0.0720055958293871, Test Loss: 0.07510228487282741\n","Epoch: 5836, Train Loss: 0.08066089688479197, Test Loss: 0.08922553152785379\n","Epoch: 5837, Train Loss: 0.09404977769926191, Test Loss: 0.10090440954271687\n","Epoch: 5838, Train Loss: 0.10658581555218904, Test Loss: 0.12195288453247329\n","Epoch: 5839, Train Loss: 0.126846286129843, Test Loss: 0.13768933437667377\n","Epoch: 5840, Train Loss: 0.14345859999755867, Test Loss: 0.16690661335810758\n","Epoch: 5841, Train Loss: 0.17193854386387558, Test Loss: 0.18513118679237853\n","Epoch: 5842, Train Loss: 0.1908773190735148, Test Loss: 0.22128172424094364\n","Epoch: 5843, Train Loss: 0.22652212412679482, Test Loss: 0.23734033770840882\n","Epoch: 5844, Train Loss: 0.2428583484164764, Test Loss: 0.2747580483294319\n","Epoch: 5845, Train Loss: 0.2802401262906786, Test Loss: 0.28155504365977263\n","Epoch: 5846, Train Loss: 0.28660476244315647, Test Loss: 0.3108146350603393\n","Epoch: 5847, Train Loss: 0.3165019584213936, Test Loss: 0.3028532700753515\n","Epoch: 5848, Train Loss: 0.30730849464042154, Test Loss: 0.31598394391037304\n","Epoch: 5849, Train Loss: 0.3217761940113989, Test Loss: 0.2939151866178555\n","Epoch: 5850, Train Loss: 0.29787527288934434, Test Loss: 0.2900373588418074\n","Epoch: 5851, Train Loss: 0.29581453895284054, Test Loss: 0.26029161477890017\n","Epoch: 5852, Train Loss: 0.26401675815074105, Test Loss: 0.24522802610418462\n","Epoch: 5853, Train Loss: 0.25089633733644584, Test Loss: 0.21521009064974558\n","Epoch: 5854, Train Loss: 0.21895588487130216, Test Loss: 0.1963095510566257\n","Epoch: 5855, Train Loss: 0.20182202634602942, Test Loss: 0.17065677579954946\n","Epoch: 5856, Train Loss: 0.17457481116922124, Test Loss: 0.1529216953318683\n","Epoch: 5857, Train Loss: 0.158273885643188, Test Loss: 0.13309479897133428\n","Epoch: 5858, Train Loss: 0.137234069274644, Test Loss: 0.11863470882317832\n","Epoch: 5859, Train Loss: 0.12384857485107514, Test Loss: 0.10418954517001942\n","Epoch: 5860, Train Loss: 0.10853873618103356, Test Loss: 0.09327083277786925\n","Epoch: 5861, Train Loss: 0.09837877069913475, Test Loss: 0.0830786084491745\n","Epoch: 5862, Train Loss: 0.08760304723819479, Test Loss: 0.07517869454417034\n","Epoch: 5863, Train Loss: 0.08021300645164336, Test Loss: 0.06808968998958755\n","Epoch: 5864, Train Loss: 0.07275173755377033, Test Loss: 0.06250395998726449\n","Epoch: 5865, Train Loss: 0.06749185001665435, Test Loss: 0.057586729118543636\n","Epoch: 5866, Train Loss: 0.06235390245995167, Test Loss: 0.0536816676511067\n","Epoch: 5867, Train Loss: 0.058643884867541025, Test Loss: 0.050253910416985466\n","Epoch: 5868, Train Loss: 0.055100701898705626, Test Loss: 0.04753569040323819\n","Epoch: 5869, Train Loss: 0.05248700087328663, Test Loss: 0.04512198510130979\n","Epoch: 5870, Train Loss: 0.05002918736288378, Test Loss: 0.04323054118994379\n","Epoch: 5871, Train Loss: 0.04818086300624904, Test Loss: 0.04150763819847245\n","Epoch: 5872, Train Loss: 0.0464610097050106, Test Loss: 0.04018912295610327\n","Epoch: 5873, Train Loss: 0.04514473190550524, Test Loss: 0.03893947481552569\n","Epoch: 5874, Train Loss: 0.04392846201732935, Test Loss: 0.0380178564647943\n","Epoch: 5875, Train Loss: 0.042982423628179535, Test Loss: 0.03709531056896412\n","Epoch: 5876, Train Loss: 0.04211203055596937, Test Loss: 0.03644935700312361\n","Epoch: 5877, Train Loss: 0.04142475848492879, Test Loss: 0.03575538654021835\n","Epoch: 5878, Train Loss: 0.04079387714364718, Test Loss: 0.03530171007315868\n","Epoch: 5879, Train Loss: 0.040288624858517884, Test Loss: 0.034769447276400425\n","Epoch: 5880, Train Loss: 0.039825133176121276, Test Loss: 0.03445057010902211\n","Epoch: 5881, Train Loss: 0.039448906193717485, Test Loss: 0.03403423217128198\n","Epoch: 5882, Train Loss: 0.03910354979359147, Test Loss: 0.03381037390801429\n","Epoch: 5883, Train Loss: 0.038819564523189375, Test Loss: 0.033478294091907716\n","Epoch: 5884, Train Loss: 0.038558428302856884, Test Loss: 0.033321779605190775\n","Epoch: 5885, Train Loss: 0.03834098501377302, Test Loss: 0.033051811068256456\n","Epoch: 5886, Train Loss: 0.03814050910875906, Test Loss: 0.03294327620893425\n","Epoch: 5887, Train Loss: 0.03797151856431072, Test Loss: 0.032719748144766955\n","Epoch: 5888, Train Loss: 0.03781518665913344, Test Loss: 0.0326455637997911\n","Epoch: 5889, Train Loss: 0.03768181536103407, Test Loss: 0.032457252932928844\n","Epoch: 5890, Train Loss: 0.037557941607132876, Test Loss: 0.032407770759529116\n","Epoch: 5891, Train Loss: 0.0374510100234497, Test Loss: 0.03224653914569746\n","Epoch: 5892, Train Loss: 0.03735124980110782, Test Loss: 0.032214891296829785\n","Epoch: 5893, Train Loss: 0.037264137308346976, Test Loss: 0.032074763759593415\n","Epoch: 5894, Train Loss: 0.03718247735265952, Test Loss: 0.03205603705043044\n","Epoch: 5895, Train Loss: 0.03711036903822007, Test Loss: 0.031932570680605454\n","Epoch: 5896, Train Loss: 0.037042437069023555, Test Loss: 0.03192323490677543\n","Epoch: 5897, Train Loss: 0.03698180218023123, Test Loss: 0.03181308408621708\n","Epoch: 5898, Train Loss: 0.03692439102999828, Test Loss: 0.03181059376105922\n","Epoch: 5899, Train Loss: 0.03687261925480253, Test Loss: 0.03171120716668827\n","Epoch: 5900, Train Loss: 0.036823356109443525, Test Loss: 0.031713722293454046\n","Epoch: 5901, Train Loss: 0.036778502067968774, Test Loss: 0.03162312975814929\n","Epoch: 5902, Train Loss: 0.036735616660722586, Test Loss: 0.031629318823658936\n","Epoch: 5903, Train Loss: 0.03669621908524942, Test Loss: 0.03154597991626328\n","Epoch: 5904, Train Loss: 0.03665837991033259, Test Loss: 0.03155488004655668\n","Epoch: 5905, Train Loss: 0.036623332736227705, Test Loss: 0.03147757542451148\n","Epoch: 5906, Train Loss: 0.0365895303954249, Test Loss: 0.03148849254122505\n","Epoch: 5907, Train Loss: 0.03655799018929508, Test Loss: 0.03141624521305269\n","Epoch: 5908, Train Loss: 0.036527453659593714, Test Loss: 0.03142868236470444\n","Epoch: 5909, Train Loss: 0.036498772666379166, Test Loss: 0.03136070005669523\n","Epoch: 5910, Train Loss: 0.03647090874391009, Test Loss: 0.03137430572436013\n","Epoch: 5911, Train Loss: 0.036444586126660426, Test Loss: 0.03130993827245611\n","Epoch: 5912, Train Loss: 0.03641893531413594, Test Loss: 0.031324468927784524\n","Epoch: 5913, Train Loss: 0.036394581406288366, Test Loss: 0.03126317646257999\n","Epoch: 5914, Train Loss: 0.03637078555718598, Test Loss: 0.031278469361295615\n","Epoch: 5915, Train Loss: 0.036348095489638764, Test Loss: 0.03121979831509106\n","Epoch: 5916, Train Loss: 0.03632587392278812, Test Loss: 0.03123575168983716\n","Epoch: 5917, Train Loss: 0.03630460805443978, Test Loss: 0.031179316522567706\n","Epoch: 5918, Train Loss: 0.03628373981839341, Test Loss: 0.03119587516189851\n","Epoch: 5919, Train Loss: 0.03626370914108354, Test Loss: 0.031141344304893296\n","Epoch: 5920, Train Loss: 0.03624401977863109, Test Loss: 0.031158489082567117\n","Epoch: 5921, Train Loss: 0.03622507498768222, Test Loss: 0.031105574020259208\n","Epoch: 5922, Train Loss: 0.03620642662059187, Test Loss: 0.031123314346835577\n","Epoch: 5923, Train Loss: 0.03618844990942823, Test Loss: 0.031071761053573354\n","Epoch: 5924, Train Loss: 0.036170733794883884, Test Loss: 0.03109012951251298\n","Epoch: 5925, Train Loss: 0.036153632693440775, Test Loss: 0.03103971167291609\n","Epoch: 5926, Train Loss: 0.03613676363932747, Test Loss: 0.031058760311521213\n","Epoch: 5927, Train Loss: 0.036120466403312436, Test Loss: 0.031009273904515442\n","Epoch: 5928, Train Loss: 0.03610437859861281, Test Loss: 0.03102907180074453\n","Epoch: 5929, Train Loss: 0.03608883079231147, Test Loss: 0.030980330737440803\n","Epoch: 5930, Train Loss: 0.036073474731463294, Test Loss: 0.03100096257402337\n","Epoch: 5931, Train Loss: 0.036058636746115084, Test Loss: 0.030952795160402877\n","Epoch: 5932, Train Loss: 0.036043977016167426, Test Loss: 0.030974360619826093\n","Epoch: 5933, Train Loss: 0.036029822339792995, Test Loss: 0.030926606675459396\n","Epoch: 5934, Train Loss: 0.03601583610636075, Test Loss: 0.030949220531871095\n","Epoch: 5935, Train Loss: 0.03600235021698912, Test Loss: 0.03090172904180901\n","Epoch: 5936, Train Loss: 0.03598902629629188, Test Loss: 0.030925521874882875\n","Epoch: 5937, Train Loss: 0.03597620609436716, Test Loss: 0.030878149087883444\n","Epoch: 5938, Train Loss: 0.03596354453907969, Test Loss: 0.030903268583805522\n","Epoch: 5939, Train Loss: 0.035951398270533896, Test Loss: 0.03085587649952545\n","Epoch: 5940, Train Loss: 0.0359394104305174, Test Loss: 0.030882489339003255\n","Epoch: 5941, Train Loss: 0.03592795808273295, Test Loss: 0.030834944552264373\n","Epoch: 5942, Train Loss: 0.035916667130850385, Test Loss: 0.030863238917658773\n","Epoch: 5943, Train Loss: 0.03590594131205675, Test Loss: 0.03081541181151332\n","Epoch: 5944, Train Loss: 0.0358953832525708, Test Loss: 0.03084560057729244\n","Epoch: 5945, Train Loss: 0.035885430593344786, Test Loss: 0.03079736488024094\n","Epoch: 5946, Train Loss: 0.035875655797935675, Test Loss: 0.030829689585245842\n","Epoch: 5947, Train Loss: 0.03586653894351051, Test Loss: 0.030780922333427108\n","Epoch: 5948, Train Loss: 0.035857614289734614, Test Loss: 0.03081565807226828\n","Epoch: 5949, Train Loss: 0.03584941458595523, Test Loss: 0.030766240046684292\n","Epoch: 5950, Train Loss: 0.03584142630708499, Test Loss: 0.030803701463561452\n","Epoch: 5951, Train Loss: 0.0358342473235342, Test Loss: 0.0307535182076303\n","Epoch: 5952, Train Loss: 0.03582730471953637, Test Loss: 0.030794066832059208\n","Epoch: 5953, Train Loss: 0.03582127680353788, Test Loss: 0.03074301039867907\n","Epoch: 5954, Train Loss: 0.0358155170132701, Test Loss: 0.03078706363281446\n","Epoch: 5955, Train Loss: 0.035810803131814735, Test Loss: 0.030735035266012924\n","Epoch: 5956, Train Loss: 0.0358063972298265, Test Loss: 0.03078307742228702\n","Epoch: 5957, Train Loss: 0.035803200437665696, Test Loss: 0.03072999145070121\n","Epoch: 5958, Train Loss: 0.03580036119967926, Test Loss: 0.03078258735257119\n","Epoch: 5959, Train Loss: 0.0357989341770012, Test Loss: 0.03072837666598449\n","Epoch: 5960, Train Loss: 0.03579792596185544, Test Loss: 0.030786188471810173\n","Epoch: 5961, Train Loss: 0.03579858320215503, Test Loss: 0.030730812074967803\n","Epoch: 5962, Train Loss: 0.035799734531990424, Test Loss: 0.030794620176068908\n","Epoch: 5963, Train Loss: 0.03580286794060224, Test Loss: 0.03073807347538786\n","Epoch: 5964, Train Loss: 0.03580658753472595, Test Loss: 0.03080880256816594\n","Epoch: 5965, Train Loss: 0.03581268643511707, Test Loss: 0.03075113125894249\n","Epoch: 5966, Train Loss: 0.03581948367838709, Test Loss: 0.030829883016030544\n","Epoch: 5967, Train Loss: 0.03582916053540024, Test Loss: 0.030771201716096687\n","Epoch: 5968, Train Loss: 0.03583967165455165, Test Loss: 0.030859295907034224\n","Epoch: 5969, Train Loss: 0.03585369523609029, Test Loss: 0.03079981304770917\n","Epoch: 5970, Train Loss: 0.03586871683674332, Test Loss: 0.03089883951729196\n","Epoch: 5971, Train Loss: 0.0358880550805089, Test Loss: 0.03083889047941496\n","Epoch: 5972, Train Loss: 0.035908587188000024, Test Loss: 0.030950775122731138\n","Epoch: 5973, Train Loss: 0.03593446276051459, Test Loss: 0.030890866226073288\n","Epoch: 5974, Train Loss: 0.035961764138769385, Test Loss: 0.031017955056034174\n","Epoch: 5975, Train Loss: 0.03599572662536493, Test Loss: 0.030958821812224726\n","Epoch: 5976, Train Loss: 0.03603138595437967, Test Loss: 0.03110398846458299\n","Epoch: 5977, Train Loss: 0.03607540587167138, Test Loss: 0.03104667252973917\n","Epoch: 5978, Train Loss: 0.036121433383637824, Test Loss: 0.031213456173657478\n","Epoch: 5979, Train Loss: 0.03617802484812322, Test Loss: 0.031159406731466534\n","Epoch: 5980, Train Loss: 0.036236970291207427, Test Loss: 0.03135218944624281\n","Epoch: 5981, Train Loss: 0.03630935131446889, Test Loss: 0.031303396352115376\n","Epoch: 5982, Train Loss: 0.03638445565631448, Test Loss: 0.031527631696317886\n","Epoch: 5983, Train Loss: 0.03647675778739891, Test Loss: 0.031486799629334296\n","Epoch: 5984, Train Loss: 0.03657214787977679, Test Loss: 0.031749307462480336\n","Epoch: 5985, Train Loss: 0.036689690396219235, Test Loss: 0.0317200825138018\n","Epoch: 5986, Train Loss: 0.036810627819169314, Test Loss: 0.03202942918273925\n","Epoch: 5987, Train Loss: 0.03696027596295335, Test Loss: 0.03201669158026657\n","Epoch: 5988, Train Loss: 0.03711347323215765, Test Loss: 0.032383679283071926\n","Epoch: 5989, Train Loss: 0.03730410507478507, Test Loss: 0.032393917894520334\n","Epoch: 5990, Train Loss: 0.03749812384924583, Test Loss: 0.03283221205393921\n","Epoch: 5991, Train Loss: 0.0377412359908943, Test Loss: 0.03287399706872741\n","Epoch: 5992, Train Loss: 0.037986981912936174, Test Loss: 0.033400925037362195\n","Epoch: 5993, Train Loss: 0.03829746962509364, Test Loss: 0.033485493185250494\n","Epoch: 5994, Train Loss: 0.03860879521601888, Test Loss: 0.03412304975136111\n","Epoch: 5995, Train Loss: 0.03900594614396381, Test Loss: 0.03426500868513412\n","Epoch: 5996, Train Loss: 0.039400363703121506, Test Loss: 0.0350411001167977\n","Epoch: 5997, Train Loss: 0.03990910249183952, Test Loss: 0.03525924025509955\n","Epoch: 5998, Train Loss: 0.04040858806558231, Test Loss: 0.036209182562850814\n","Epoch: 5999, Train Loss: 0.04106099601386481, Test Loss: 0.036527347816023174\n","Epoch: 6000, Train Loss: 0.041692825009255655, Test Loss: 0.037695595416637064\n","Epoch: 6001, Train Loss: 0.04252992317614132, Test Loss: 0.03814349675527907\n","Epoch: 6002, Train Loss: 0.04332740580241441, Test Loss: 0.03958549677641731\n","Epoch: 6003, Train Loss: 0.044401113982864816, Test Loss: 0.04019923790339354\n","Epoch: 6004, Train Loss: 0.045403977694236826, Test Loss: 0.041983156068407815\n","Epoch: 6005, Train Loss: 0.046779018239343786, Test Loss: 0.04280505908691714\n","Epoch: 6006, Train Loss: 0.048032995717685795, Test Loss: 0.0450128700500892\n","Epoch: 6007, Train Loss: 0.04978826401588608, Test Loss: 0.0460899265810887\n","Epoch: 6008, Train Loss: 0.05134317611242467, Test Loss: 0.0488169689937547\n","Epoch: 6009, Train Loss: 0.05357171073216581, Test Loss: 0.05019690979902002\n","Epoch: 6010, Train Loss: 0.05547699899552079, Test Loss: 0.05354846459143886\n","Epoch: 6011, Train Loss: 0.05828313991447496, Test Loss: 0.05527211722844253\n","Epoch: 6012, Train Loss: 0.060579488953223744, Test Loss: 0.05935495362936349\n","Epoch: 6013, Train Loss: 0.06407118140970203, Test Loss: 0.06144345830857646\n","Epoch: 6014, Train Loss: 0.06677680466677334, Test Loss: 0.066349877786254\n","Epoch: 6015, Train Loss: 0.07105054987358635, Test Loss: 0.06878588318906645\n","Epoch: 6016, Train Loss: 0.07414133588719468, Test Loss: 0.07456817753359958\n","Epoch: 6017, Train Loss: 0.07925759741287826, Test Loss: 0.0772719723895661\n","Epoch: 6018, Train Loss: 0.08264227095529739, Test Loss: 0.08390736489912863\n","Epoch: 6019, Train Loss: 0.08859117988548047, Test Loss: 0.08671256924650905\n","Epoch: 6020, Train Loss: 0.0920864650744605, Test Loss: 0.09406359900141328\n","Epoch: 6021, Train Loss: 0.09874842127766034, Test Loss: 0.09670221695589255\n","Epoch: 6022, Train Loss: 0.10206451549202339, Test Loss: 0.1044849953281549\n","Epoch: 6023, Train Loss: 0.109177663999885, Test Loss: 0.10659592959604472\n","Epoch: 6024, Train Loss: 0.11192861860797218, Test Loss: 0.11437567186835002\n","Epoch: 6025, Train Loss: 0.1190822340836624, Test Loss: 0.11554935171048422\n","Epoch: 6026, Train Loss: 0.12083405749754557, Test Loss: 0.12278235082488469\n","Epoch: 6027, Train Loss: 0.12750700053800298, Test Loss: 0.12264216204718238\n","Epoch: 6028, Train Loss: 0.12786363669112336, Test Loss: 0.12876909334926026\n","Epoch: 6029, Train Loss: 0.13351341134747557, Test Loss: 0.12706933823959998\n","Epoch: 6030, Train Loss: 0.13221898609358432, Test Loss: 0.13163799088668046\n","Epoch: 6031, Train Loss: 0.13640078287606877, Test Loss: 0.12834039650941043\n","Epoch: 6032, Train Loss: 0.1334184691895383, Test Loss: 0.13111226148993296\n","Epoch: 6033, Train Loss: 0.13589007898619007, Test Loss: 0.12640508120662644\n","Epoch: 6034, Train Loss: 0.1314205457100208, Test Loss: 0.12740048279719454\n","Epoch: 6035, Train Loss: 0.13218864215409007, Test Loss: 0.121650109784585\n","Epoch: 6036, Train Loss: 0.12661823106147208, Test Loss: 0.12111569300175676\n","Epoch: 6037, Train Loss: 0.12590943563681506, Test Loss: 0.11477378407237775\n","Epoch: 6038, Train Loss: 0.11971248297858057, Test Loss: 0.11309425585627891\n","Epoch: 6039, Train Loss: 0.11788969862772532, Test Loss: 0.10660028428481357\n","Epoch: 6040, Train Loss: 0.11152666522520151, Test Loss: 0.10419659851837562\n","Epoch: 6041, Train Loss: 0.10899126937075118, Test Loss: 0.09790826301588475\n","Epoch: 6042, Train Loss: 0.10283628127221993, Test Loss: 0.09515806508321065\n","Epoch: 6043, Train Loss: 0.09995100240549194, Test Loss: 0.0893211457887172\n","Epoch: 6044, Train Loss: 0.09426066060003332, Test Loss: 0.08651635168417104\n","Epoch: 6045, Train Loss: 0.09130787802987506, Test Loss: 0.08126746607894948\n","Epoch: 6046, Train Loss: 0.0862243820296042, Test Loss: 0.07860511852305183\n","Epoch: 6047, Train Loss: 0.08339644784274422, Test Loss: 0.07399315646970098\n","Epoch: 6048, Train Loss: 0.07897016425113045, Test Loss: 0.07158712534486081\n","Epoch: 6049, Train Loss: 0.07637994388391864, Test Loss: 0.06760032087340995\n","Epoch: 6050, Train Loss: 0.07259781658028137, Test Loss: 0.06550156790717153\n","Epoch: 6051, Train Loss: 0.07029767832654715, Test Loss: 0.062092079952563876\n","Epoch: 6052, Train Loss: 0.06710900618205283, Test Loss: 0.0603092658287338\n","Epoch: 6053, Train Loss: 0.06511033258938875, Test Loss: 0.05741189043857652\n","Epoch: 6054, Train Loss: 0.06244639153091268, Test Loss: 0.05592835727913299\n","Epoch: 6055, Train Loss: 0.06073575451011601, Test Loss: 0.0534730567553453\n","Epoch: 6056, Train Loss: 0.05852293532085905, Test Loss: 0.052259139516344096\n","Epoch: 6057, Train Loss: 0.05707388311731012, Test Loss: 0.05017849051823877\n","Epoch: 6058, Train Loss: 0.05524149696554499, Test Loss: 0.049199611276207096\n","Epoch: 6059, Train Loss: 0.05402235084998766, Test Loss: 0.04743262604935226\n","Epoch: 6060, Train Loss: 0.0525066250817034, Test Loss: 0.04665415139960029\n","Epoch: 6061, Train Loss: 0.051485199215799214, Test Loss: 0.04514776498830713\n","Epoch: 6062, Train Loss: 0.05023081934675768, Test Loss: 0.04453762037110463\n","Epoch: 6063, Train Loss: 0.04937699945952927, Test Loss: 0.0432468123163303\n","Epoch: 6064, Train Loss: 0.04833721488230448, Test Loss: 0.04277665911013154\n","Epoch: 6065, Train Loss: 0.047624158030371976, Test Loss: 0.04166386201569574\n","Epoch: 6066, Train Loss: 0.04676013800338676, Test Loss: 0.0413094181690596\n","Epoch: 6067, Train Loss: 0.04616464468608991, Test Loss: 0.040343618156693774\n","Epoch: 6068, Train Loss: 0.04544451152483596, Test Loss: 0.0400845093772171\n","Epoch: 6069, Train Loss: 0.04494693878129942, Test Loss: 0.039240271287170204\n","Epoch: 6070, Train Loss: 0.04434472402865585, Test Loss: 0.039059655257864444\n","Epoch: 6071, Train Loss: 0.04392867144921053, Test Loss: 0.03831619474861824\n","Epoch: 6072, Train Loss: 0.043423324064184685, Test Loss: 0.038200301753533096\n","Epoch: 6073, Train Loss: 0.04307523069412976, Test Loss: 0.03754065969484924\n","Epoch: 6074, Train Loss: 0.04264973575107339, Test Loss: 0.037478329036492945\n","Epoch: 6075, Train Loss: 0.04235846500678275, Test Loss: 0.036888665784819114\n","Epoch: 6076, Train Loss: 0.041999091367925755, Test Loss: 0.03687091800486555\n","Epoch: 6077, Train Loss: 0.041755543509013164, Test Loss: 0.036339925319604165\n","Epoch: 6078, Train Loss: 0.041451217898599936, Test Loss: 0.03635958712227239\n","Epoch: 6079, Train Loss: 0.04124798742634776, Test Loss: 0.03587800625427452\n","Epoch: 6080, Train Loss: 0.040989782533216675, Test Loss: 0.03592939217739135\n","Epoch: 6081, Train Loss: 0.040820865453609204, Test Loss: 0.03548962337027383\n","Epoch: 6082, Train Loss: 0.04060158619207138, Test Loss: 0.0355682716219009\n","Epoch: 6083, Train Loss: 0.04046213558307513, Test Loss: 0.035164060089617914\n","Epoch: 6084, Train Loss: 0.04027598745992424, Test Loss: 0.03526651698081327\n","Epoch: 6085, Train Loss: 0.040162112767904434, Test Loss: 0.03489270172344489\n","Epoch: 6086, Train Loss: 0.0400044376967889, Test Loss: 0.0350163481765367\n","Epoch: 6087, Train Loss: 0.03991304213485208, Test Loss: 0.034668661861569465\n","Epoch: 6088, Train Loss: 0.03978010903989938, Test Loss: 0.03481157559475416\n","Epoch: 6089, Train Loss: 0.039708759454948984, Test Loss: 0.03448648568695475\n","Epoch: 6090, Train Loss: 0.03959759910443624, Test Loss: 0.03464733330933067\n","Epoch: 6091, Train Loss: 0.03954442318428555, Test Loss: 0.03434191645044023\n","Epoch: 6092, Train Loss: 0.03945269864732916, Test Loss: 0.034519870532465816\n","Epoch: 6093, Train Loss: 0.03941630505246243, Test Loss: 0.03423171376093418\n","Epoch: 6094, Train Loss: 0.03934221087820356, Test Loss: 0.03442639080651436\n","Epoch: 6095, Train Loss: 0.03932162864264349, Test Loss: 0.03415351454746127\n","Epoch: 6096, Train Loss: 0.039263813299942336, Test Loss: 0.034364930603491634\n","Epoch: 6097, Train Loss: 0.039258447571538006, Test Loss: 0.03410572946453781\n","Epoch: 6098, Train Loss: 0.03921595487245712, Test Loss: 0.03433427082767999\n","Epoch: 6099, Train Loss: 0.03922555672007118, Test Loss: 0.034087469135601664\n","Epoch: 6100, Train Loss: 0.03919778291212677, Test Loss: 0.03433387624559962\n","Epoch: 6101, Train Loss: 0.039222431505623774, Test Loss: 0.03409849598317083\n","Epoch: 6102, Train Loss: 0.039209095488791604, Test Loss: 0.03436385913058407\n","Epoch: 6103, Train Loss: 0.039249191459425375, Test Loss: 0.03413919851100423\n","Epoch: 6104, Train Loss: 0.03925031619420862, Test Loss: 0.03442496444256144\n","Epoch: 6105, Train Loss: 0.03930658541453457, Test Loss: 0.03421058581348448\n","Epoch: 6106, Train Loss: 0.03932248906108258, Test Loss: 0.03451857469767893\n","Epoch: 6107, Train Loss: 0.03939599645146734, Test Loss: 0.03431430081401082\n","Epoch: 6108, Train Loss: 0.03942729213327974, Test Loss: 0.03464673333676915\n","Epoch: 6109, Train Loss: 0.039519465409928595, Test Loss: 0.034452651288976746\n","Epoch: 6110, Train Loss: 0.03956706873695184, Test Loss: 0.034812185881777126\n","Epoch: 6111, Train Loss: 0.03967973226201673, Test Loss: 0.03462865811319967\n","Epoch: 6112, Train Loss: 0.03974487587503121, Test Loss: 0.03501843846264392\n","Epoch: 6113, Train Loss: 0.03988029494248031, Test Loss: 0.03484612034331556\n","Epoch: 6114, Train Loss: 0.03996454934040888, Test Loss: 0.03526983336870081\n","Epoch: 6115, Train Loss: 0.04012548531025662, Test Loss: 0.035109696689866486\n","Epoch: 6116, Train Loss: 0.04023078506791062, Test Loss: 0.035571641064618756\n","Epoch: 6117, Train Loss: 0.040420561708999145, Test Loss: 0.035425002537451344\n","Epoch: 6118, Train Loss: 0.0405492358424271, Test Loss: 0.0359301675111404\n","Epoch: 6119, Train Loss: 0.040771817002351135, Test Loss: 0.03579872083659662\n","Epoch: 6120, Train Loss: 0.04092662163112635, Test Loss: 0.03635287449896703\n","Epoch: 6121, Train Loss: 0.041186699835901015, Test Loss: 0.03623872374309892\n","Epoch: 6122, Train Loss: 0.04137085034356499, Test Loss: 0.036848508837834686\n","Epoch: 6123, Train Loss: 0.04167394501904984, Test Loss: 0.03675419959450899\n","Epoch: 6124, Train Loss: 0.041891143518904425, Test Loss: 0.03742723337447688\n","Epoch: 6125, Train Loss: 0.042243706057869514, Test Loss: 0.03735577639948167\n","Epoch: 6126, Train Loss: 0.04249815800540758, Test Loss: 0.038100748607274824\n","Epoch: 6127, Train Loss: 0.04290767866710465, Test Loss: 0.03805562812455014\n","Epoch: 6128, Train Loss: 0.04320408978643168, Test Loss: 0.03888238772963166\n","Epoch: 6129, Train Loss: 0.043679198150717295, Test Loss: 0.03886754330979387\n","Epoch: 6130, Train Loss: 0.04402273933812157, Test Loss: 0.03978715985900166\n","Epoch: 6131, Train Loss: 0.04457328545234056, Test Loss: 0.03980692657161671\n","Epoch: 6132, Train Loss: 0.044969508924934745, Test Loss: 0.040831705657645725\n","Epoch: 6133, Train Loss: 0.0456066060978312, Test Loss: 0.04089069215059322\n","Epoch: 6134, Train Loss: 0.046061290853233176, Test Loss: 0.042034116433905144\n","Epoch: 6135, Train Loss: 0.04679729308427648, Test Loss: 0.04213699500202559\n","Epoch: 6136, Train Loss: 0.04731619209259607, Test Loss: 0.043413552579869355\n","Epoch: 6137, Train Loss: 0.04816456945634843, Test Loss: 0.04356472990545333\n","Epoch: 6138, Train Loss: 0.048753025759529764, Test Loss: 0.04498958130396601\n","Epoch: 6139, Train Loss: 0.04972809029589304, Test Loss: 0.04519271483997027\n","Epoch: 6140, Train Loss: 0.050390485919215196, Test Loss: 0.046781140146280174\n","Epoch: 6141, Train Loss: 0.05150691022156826, Test Loss: 0.04703846551188954\n","Epoch: 6142, Train Loss: 0.0522459131051342, Test Loss: 0.04880502721629976\n","Epoch: 6143, Train Loss: 0.053517976753150835, Test Loss: 0.04911647012827049\n","Epoch: 6144, Train Loss: 0.05433356060711314, Test Loss: 0.05107383005101147\n","Epoch: 6145, Train Loss: 0.05577406064402655, Test Loss: 0.05143589701178975\n","Epoch: 6146, Train Loss: 0.05666229566660158, Test Loss: 0.053593244255453386\n","Epoch: 6147, Train Loss: 0.058281073369097505, Test Loss: 0.05399772471639159\n","Epoch: 6148, Train Loss: 0.05923272746537409, Test Loss: 0.056358814417996\n","Epoch: 6149, Train Loss: 0.06103480320761413, Test Loss: 0.05679138741224751\n","Epoch: 6150, Train Loss: 0.0620338575525427, Test Loss: 0.059352265127632226\n","Epoch: 6151, Train Loss: 0.06401723682123782, Test Loss: 0.0597911849424968\n","Epoch: 6152, Train Loss: 0.06503950535258064, Test Loss: 0.06253778201843142\n","Epoch: 6153, Train Loss: 0.06719282576455793, Test Loss: 0.06295291092812266\n","Epoch: 6154, Train Loss: 0.06820496506737413, Test Loss: 0.06585883431636735\n","Epoch: 6155, Train Loss: 0.07050528974240021, Test Loss: 0.06621137288129053\n","Epoch: 6156, Train Loss: 0.07146456944736877, Test Loss: 0.06923635236666245\n","Epoch: 6157, Train Loss: 0.07387577161970424, Test Loss: 0.06947965165702433\n","Epoch: 6158, Train Loss: 0.07473100639195181, Test Loss: 0.07256920012313473\n","Epoch: 6159, Train Loss: 0.0772032870458162, Test Loss: 0.07265097912151233\n","Epoch: 6160, Train Loss: 0.07789726160308537, Test Loss: 0.07573780265088786\n","Epoch: 6161, Train Loss: 0.08036833274674166, Test Loss: 0.07560390047140096\n","Epoch: 6162, Train Loss: 0.08084184319044156, Test Loss: 0.07861140507859088\n","Epoch: 6163, Train Loss: 0.08324013429324748, Test Loss: 0.07821086969934993\n","Epoch: 6164, Train Loss: 0.08343742238837451, Test Loss: 0.08105873125393152\n","Epoch: 6165, Train Loss: 0.08568730513719383, Test Loss: 0.08034964326653128\n","Epoch: 6166, Train Loss: 0.08556224059315551, Test Loss: 0.08296089426763864\n","Epoch: 6167, Train Loss: 0.08759077047928199, Test Loss: 0.08191596893749968\n","Epoch: 6168, Train Loss: 0.08711276931344986, Test Loss: 0.08422455464454003\n","Epoch: 6169, Train Loss: 0.08885695015260692, Test Loss: 0.08283541014149484\n","Epoch: 6170, Train Loss: 0.08801546249309124, Test Loss: 0.08479286910224199\n","Epoch: 6171, Train Loss: 0.08942873885920637, Test Loss: 0.08307200317881656\n","Epoch: 6172, Train Loss: 0.08823530948883518, Test Loss: 0.08465198752543172\n","Epoch: 6173, Train Loss: 0.08929203503427655, Test Loss: 0.08263196729540695\n","Epoch: 6174, Train Loss: 0.08777942954401595, Test Loss: 0.0838317629383309\n","Epoch: 6175, Train Loss: 0.0884764767892, Test Loss: 0.08156176735974557\n","Epoch: 6176, Train Loss: 0.08669503192975588, Test Loss: 0.08240066900530467\n","Epoch: 6177, Train Loss: 0.08705037500766534, Test Loss: 0.07994111483971418\n","Epoch: 6178, Train Loss: 0.08506234761830433, Test Loss: 0.08045621480460836\n","Epoch: 6179, Train Loss: 0.0851111318534034, Test Loss: 0.07787254829320854\n","Epoch: 6180, Train Loss: 0.08298418361104148, Test Loss: 0.07811297903002215\n","Epoch: 6181, Train Loss: 0.08277326833377545, Test Loss: 0.07546974048152029\n","Epoch: 6182, Train Loss: 0.08057424504609091, Test Loss: 0.07549054860897442\n","Epoch: 6183, Train Loss: 0.08015634944908356, Test Loss: 0.07284656202693779\n","Epoch: 6184, Train Loss: 0.07794624389447946, Test Loss: 0.07270321257966482\n","Epoch: 6185, Train Loss: 0.07737466201144463, Test Loss: 0.07010835242703392\n","Epoch: 6186, Train Loss: 0.07520522998012481, Test Loss: 0.06985249518442206\n","Epoch: 6187, Train Loss: 0.07452973280605853, Test Loss: 0.06734608581065431\n","Epoch: 6188, Train Loss: 0.07244181767051616, Test Loss: 0.06702281471440973\n","Epoch: 6189, Train Loss: 0.07170597607845025, Test Loss: 0.0646334272932421\n","Epoch: 6190, Train Loss: 0.06972929421827526, Test Loss: 0.0642799431681433\n","Epoch: 6191, Train Loss: 0.06896914641342528, Test Loss: 0.062026206270971614\n","Epoch: 6192, Train Loss: 0.06712313123253019, Test Loss: 0.06167160160399621\n","Epoch: 6193, Train Loss: 0.06636693169242364, Test Loss: 0.05956361758091767\n","Epoch: 6194, Train Loss: 0.06466220956001598, Test Loss: 0.05922943422393563\n","Epoch: 6195, Train Loss: 0.06393092822758693, Test Loss: 0.05727045114920395\n","Epoch: 6196, Train Loss: 0.062371060231217565, Test Loss: 0.05697168200563444\n","Epoch: 6197, Train Loss: 0.06167931766122711, Test Loss: 0.05515976489130445\n","Epoch: 6198, Train Loss: 0.06026253944105155, Test Loss: 0.054906037460290984\n","Epoch: 6199, Train Loss: 0.059619726032638806, Test Loss: 0.053235579066442766\n","Epoch: 6200, Train Loss: 0.058340519047500095, Test Loss: 0.05303233886061335\n","Epoch: 6201, Train Loss: 0.05775192242814346, Test Loss: 0.051495330884813166\n","Epoch: 6202, Train Loss: 0.05660233413824912, Test Loss: 0.0513449156831448\n","Epoch: 6203, Train Loss: 0.056070168341393854, Test Loss: 0.04993195876453104\n","Epoch: 6204, Train Loss: 0.055040859069053864, Test Loss: 0.04983451123848333\n","Epoch: 6205, Train Loss: 0.0545651433902769, Test Loss: 0.04853557807246133\n","Epoch: 6206, Train Loss: 0.053646175098832356, Test Loss: 0.04848978301657381\n","Epoch: 6207, Train Loss: 0.053225447816042715, Test Loss: 0.047294767484704396\n","Epoch: 6208, Train Loss: 0.052406849473723975, Test Loss: 0.047298423196032835\n","Epoch: 6209, Train Loss: 0.05203872426290138, Test Loss: 0.046197515087242916\n","Epoch: 6210, Train Loss: 0.05131087539080526, Test Loss: 0.04624796028103225\n","Epoch: 6211, Train Loss: 0.05099245993515892, Test Loss: 0.0452318845663953\n","Epoch: 6212, Train Loss: 0.05034633324427079, Test Loss: 0.04532630641020959\n","Epoch: 6213, Train Loss: 0.050074533843774235, Test Loss: 0.044386461612243695\n","Epoch: 6214, Train Loss: 0.049501833187005836, Test Loss: 0.044522110091104246\n","Epoch: 6215, Train Loss: 0.04927356906378221, Test Loss: 0.043650634372366506\n","Epoch: 6216, Train Loss: 0.048766792688706304, Test Loss: 0.043824965466707264\n","Epoch: 6217, Train Loss: 0.04857914125869838, Test Loss: 0.043014753026680584\n","Epoch: 6218, Train Loss: 0.04813159398707225, Test Loss: 0.04322551952040821\n","Epoch: 6219, Train Loss: 0.047981885000883316, Test Loss: 0.042470204446354305\n","Epoch: 6220, Train Loss: 0.04758765722905841, Test Loss: 0.042715509421995757\n","Epoch: 6221, Train Loss: 0.04747353018482994, Test Loss: 0.04200942957653333\n","Epoch: 6222, Train Loss: 0.0471274567965641, Test Loss: 0.0422877542320056\n","Epoch: 6223, Train Loss: 0.04704689281894764, Test Loss: 0.0416259041199624\n","Epoch: 6224, Train Loss: 0.046744501272734995, Test Loss: 0.04193611863566256\n","Epoch: 6225, Train Loss: 0.04669583791354084, Test Loss: 0.04131409739209059\n","Epoch: 6226, Train Loss: 0.04643329182220437, Test Loss: 0.041655461219117174\n","Epoch: 6227, Train Loss: 0.04641522700700955, Test Loss: 0.0410694197647587\n","Epoch: 6228, Train Loss: 0.04618926932579492, Test Loss: 0.04144157585009777\n","Epoch: 6229, Train Loss: 0.046200858909050964, Test Loss: 0.04088816572593648\n","Epoch: 6230, Train Loss: 0.04600875723755308, Test Loss: 0.0412911317605425\n","Epoch: 6231, Train Loss: 0.04604940926624656, Test Loss: 0.040767457046603305\n","Epoch: 6232, Train Loss: 0.0458889046089437, Test Loss: 0.04120161573146552\n","Epoch: 6233, Train Loss: 0.04595837235236246, Test Loss: 0.04070518866371516\n","Epoch: 6234, Train Loss: 0.04582763185297627, Test Loss: 0.04117127815572915\n","Epoch: 6235, Train Loss: 0.04592600685771235, Test Loss: 0.040699978487199966\n","Epoch: 6236, Train Loss: 0.045823580427444555, Test Loss: 0.04119908353931587\n","Epoch: 6237, Train Loss: 0.04595128623514976, Test Loss: 0.04075112127562108\n","Epoch: 6238, Train Loss: 0.04587606655846322, Test Loss: 0.0412846650639924\n","Epoch: 6239, Train Loss: 0.04603385322197191, Test Loss: 0.04085854588436531\n","Epoch: 6240, Train Loss: 0.045985038288438314, Test Loss: 0.04142828206959601\n","Epoch: 6241, Train Loss: 0.0461739773924115, Test Loss: 0.041022774481637575\n","Epoch: 6242, Train Loss: 0.04615103442671231, Test Loss: 0.0416307786419487\n","Epoch: 6243, Train Loss: 0.0463725139234505, Test Loss: 0.041244881682250115\n","Epoch: 6244, Train Loss: 0.046375143337926715, Test Loss: 0.04189354085242295\n","Epoch: 6245, Train Loss: 0.0466308611168525, Test Loss: 0.04152645091714756\n","Epoch: 6246, Train Loss: 0.04665895887309377, Test Loss: 0.04221844954572207\n","Epoch: 6247, Train Loss: 0.046950913570514174, Test Loss: 0.041869524705756916\n","Epoch: 6248, Train Loss: 0.04700453009991422, Test Loss: 0.042607824890345355\n","Epoch: 6249, Train Loss: 0.047335007208998664, Test Loss: 0.042276544815663515\n","Epoch: 6250, Train Loss: 0.04741430080964479, Test Loss: 0.04306435818913298\n","Epoch: 6251, Train Loss: 0.04778585166375849, Test Loss: 0.04275027758932894\n","Epoch: 6252, Train Loss: 0.04789103407797889, Test Loss: 0.043591025718581464\n","Epoch: 6253, Train Loss: 0.04830644476110959, Test Loss: 0.043293719028435936\n","Epoch: 6254, Train Loss: 0.04843771647550714, Test Loss: 0.04419097868148885\n","Epoch: 6255, Train Loss: 0.04889996318639757, Test Loss: 0.04390997362700643\n","Epoch: 6256, Train Loss: 0.04905743593416581, Test Loss: 0.04486740281638422\n","Epoch: 6257, Train Loss: 0.049569622844231526, Test Loss: 0.044602100554455446\n","Epoch: 6258, Train Loss: 0.049753226900175064, Test Loss: 0.04562334096011165\n","Epoch: 6259, Train Loss: 0.05031850217807391, Test Loss: 0.045372920783976055\n","Epoch: 6260, Train Loss: 0.050527876416678955, Test Loss: 0.04646147211942566\n","Epoch: 6261, Train Loss: 0.05114932196059658, Test Loss: 0.046224779376252675\n","Epoch: 6262, Train Loss: 0.05138368541705747, Test Loss: 0.04738384165148439\n","Epoch: 6263, Train Loss: 0.0520641760975651, Test Loss: 0.047159258659484116\n","Epoch: 6264, Train Loss: 0.052322181068319325, Test Loss: 0.04839153931768673\n","Epoch: 6265, Train Loss: 0.053064210139676944, Test Loss: 0.048176840833943926\n","Epoch: 6266, Train Loss: 0.053343778821559364, Test Loss: 0.04948432562751469\n","Epoch: 6267, Train Loss: 0.05414924783762006, Test Loss: 0.04927652291712849\n","Epoch: 6268, Train Loss: 0.0544473972445315, Test Loss: 0.05066021237093279\n","Epoch: 6269, Train Loss: 0.05531737155013758, Test Loss: 0.05045539321085479\n","Epoch: 6270, Train Loss: 0.05563003500190059, Test Loss: 0.05191501077165617\n","Epoch: 6271, Train Loss: 0.05656446984843315, Test Loss: 0.05170818671307517\n","Epoch: 6272, Train Loss: 0.05688632760305489, Test Loss: 0.053241870244318056\n","Epoch: 6273, Train Loss: 0.05788377522145594, Test Loss: 0.05302684688623378\n","Epoch: 6274, Train Loss: 0.058208111517638356, Test Loss: 0.054630841842017004\n","Epoch: 6275, Train Loss: 0.059265425914596125, Test Loss: 0.05440013220544325\n","Epoch: 6276, Train Loss: 0.059584034229307824, Test Loss: 0.05606851206469218\n","Epoch: 6277, Train Loss: 0.06069609755912558, Test Loss: 0.055813316570253804\n","Epoch: 6278, Train Loss: 0.060999259375387874, Test Loss: 0.057537762953730934\n","Epoch: 6279, Train Loss: 0.06215876056176766, Test Loss: 0.0572480408861556\n","Epoch: 6280, Train Loss: 0.062435324211065166, Test Loss: 0.05901772078215176\n","Epoch: 6281, Train Loss: 0.06363262567751185, Test Loss: 0.0586823762135703\n","Epoch: 6282, Train Loss: 0.06387020955133826, Test Loss: 0.06048395512594015\n","Epoch: 6283, Train Loss: 0.06509333973961585, Test Loss: 0.060091153914568214\n","Epoch: 6284, Train Loss: 0.06527867716883828, Test Loss: 0.06190897968898053\n","Epoch: 6285, Train Loss: 0.06651348317478374, Test Loss: 0.061446602711060665\n","Epoch: 6286, Train Loss: 0.06663291390157598, Test Loss: 0.06326308388876062\n","Epoch: 6287, Train Loss: 0.06786339861096334, Test Loss: 0.06271930537269392\n","Epoch: 6288, Train Loss: 0.06790349436649804, Test Loss: 0.06451548979307477\n","Epoch: 6289, Train Loss: 0.06911234547566986, Test Loss: 0.06387945007181148\n","Epoch: 6290, Train Loss: 0.06906063642857818, Test Loss: 0.065635785316001\n","Epoch: 6291, Train Loss: 0.0702299317605077, Test Loss: 0.06489830745108484\n","Epoch: 6292, Train Loss: 0.07007567966222362, Test Loss: 0.06659553768246045\n","Epoch: 6293, Train Loss: 0.07118772713900341, Test Loss: 0.06574982124285736\n","Epoch: 6294, Train Loss: 0.07092267408216321, Test Loss: 0.06736994978249856\n","Epoch: 6295, Train Loss: 0.07196092010776295, Test Loss: 0.06641216680540997\n","Epoch: 6296, Train Loss: 0.07157993334784694, Test Loss: 0.06793939585741784\n","Epoch: 6297, Train Loss: 0.07252985549492927, Test Loss: 0.06686911694450343\n","Epoch: 6298, Train Loss: 0.07203139214346692, Test Loss: 0.06829067008926917\n","Epoch: 6299, Train Loss: 0.07288128567257232, Test Loss: 0.06711106384537412\n","Epoch: 6300, Train Loss: 0.0722676173849644, Test Loss: 0.06841780591904088\n","Epoch: 6301, Train Loss: 0.07300919296545319, Test Loss: 0.06713558083764128\n","Epoch: 6302, Train Loss: 0.07228635818205133, Test Loss: 0.06832237304484812\n","Epoch: 6303, Train Loss: 0.07291508983562263, Test Loss: 0.06694746308959497\n","Epoch: 6304, Train Loss: 0.07209257503184227, Test Loss: 0.0680132246889572\n","Epoch: 6305, Train Loss: 0.07260776908632136, Test Loss: 0.06655825212214746\n","Epoch: 6306, Train Loss: 0.07169795444094262, Test Loss: 0.06750573734305414\n","Epoch: 6307, Train Loss: 0.07210254603174995, Test Loss: 0.0659853126348811\n","Epoch: 6308, Train Loss: 0.07111997847320987, Test Loss: 0.06682064545294919\n","Epoch: 6309, Train Loss: 0.07142009494774834, Test Loss: 0.06525057976043681\n","Epoch: 6310, Train Loss: 0.07038066789052193, Test Loss: 0.06598261378191943\n","Epoch: 6311, Train Loss: 0.0705850225197735, Test Loss: 0.06437912257201314\n","Epoch: 6312, Train Loss: 0.0695051447641832, Test Loss: 0.06501870517709173\n","Epoch: 6313, Train Loss: 0.0696243360944242, Test Loss: 0.06339767303057729\n","Epoch: 6314, Train Loss: 0.06852016335886353, Test Loss: 0.06395689185456091\n","Epoch: 6315, Train Loss: 0.06856595500107475, Test Loss: 0.06233325152600846\n","Epoch: 6316, Train Loss: 0.06745273975652438, Test Loss: 0.06282472974931372\n","Epoch: 6317, Train Loss: 0.06743738466805572, Test Loss: 0.06121198747656977\n","Epoch: 6318, Train Loss: 0.06632897786174652, Test Loss: 0.06164827626706481\n","Epoch: 6319, Train Loss: 0.06626463403915385, Test Loss: 0.060058194320063596\n","Epoch: 6320, Train Loss: 0.06517315031630179, Test Loss: 0.060451290461289206\n","Epoch: 6321, Train Loss: 0.06507141544659197, Test Loss: 0.0588937201913168\n","Epoch: 6322, Train Loss: 0.06400705494207512, Test Loss: 0.05925471806938951\n","Epoch: 6323, Train Loss: 0.06387862945897953, Test Loss: 0.0577375640530223\n","Epoch: 6324, Train Loss: 0.06284963599387956, Test Loss: 0.05807643613238741\n","Epoch: 6325, Train Loss: 0.06270410946460045, Test Loss: 0.05660572482012211\n","Epoch: 6326, Train Loss: 0.0617168374785813, Test Loss: 0.05693121452959599\n","Epoch: 6327, Train Loss: 0.061562583317337015, Test Loss: 0.05551123850458411\n","Epoch: 6328, Train Loss: 0.06062164346185539, Test Loss: 0.055830843988436955\n","Epoch: 6329, Train Loss: 0.06046580157392685, Test Loss: 0.05446435433780943\n","Epoch: 6330, Train Loss: 0.05957425634989875, Test Loss: 0.054784380000086584\n","Epoch: 6331, Train Loss: 0.059422781707899615, Test Loss: 0.053472803056494794\n","Epoch: 6332, Train Loss: 0.058582366449944454, Test Loss: 0.05379845712588172\n","Epoch: 6333, Train Loss: 0.058440122737353176, Test Loss: 0.052542116730216096\n","Epoch: 6334, Train Loss: 0.057651472350709235, Test Loss: 0.05287763607636314\n","Epoch: 6335, Train Loss: 0.057522352606382825, Test Loss: 0.05167596760157229\n","Epoch: 6336, Train Loss: 0.05678521976789782, Test Loss: 0.052024754799139136\n","Epoch: 6337, Train Loss: 0.056672279524624516, Test Loss: 0.05087650183565574\n","Epoch: 6338, Train Loss: 0.05598573491474693, Test Loss: 0.05124126331159328\n","Epoch: 6339, Train Loss: 0.05589132698117085, Test Loss: 0.0501446518100467\n","Epoch: 6340, Train Loss: 0.05525393616832684, Test Loss: 0.050527529389810054\n","Epoch: 6341, Train Loss: 0.05517983953585287, Test Loss: 0.04948041706980375\n","Epoch: 6342, Train Loss: 0.054589814266876514, Test Loss: 0.04988310813850195\n","Epoch: 6343, Train Loss: 0.054537352413780694, Test Loss: 0.048883109133334206\n","Epoch: 6344, Train Loss: 0.05399267630628714, Test Loss: 0.0493069728816722\n","Epoch: 6345, Train Loss: 0.05396282235100224, Test Loss: 0.04835155901038714\n","Epoch: 6346, Train Loss: 0.05346135245377418, Test Loss: 0.04879770787406917\n","Epoch: 6347, Train Loss: 0.05345482020396783, Test Loss: 0.047884288760477116\n","Epoch: 6348, Train Loss: 0.05299436674291389, Test Loss: 0.04835366526943786\n","Epoch: 6349, Train Loss: 0.05301168777372758, Test Loss: 0.047479649911189664\n","Epoch: 6350, Train Loss: 0.05259007478920165, Test Loss: 0.047973089845450984\n","Epoch: 6351, Train Loss: 0.052631662360188125, Test Loss: 0.04713593230742358\n","Epoch: 6352, Train Loss: 0.05224677200513028, Test Loss: 0.04765421541272957\n","Epoch: 6353, Train Loss: 0.05231297298853527, Test Loss: 0.04685144718664375\n","Epoch: 6354, Train Loss: 0.051962776109755394, Test Loss: 0.04739533682670957\n","Epoch: 6355, Train Loss: 0.052053912239694126, Test Loss: 0.04662458814663558\n","Epoch: 6356, Train Loss: 0.051736487594045005, Test Loss: 0.047194861234828474\n","Epoch: 6357, Train Loss: 0.051852887328370366, Test Loss: 0.0464538733267712\n","Epoch: 6358, Train Loss: 0.05156643145498458, Test Loss: 0.04705134174641195\n","Epoch: 6359, Train Loss: 0.05170845362487773, Test Loss: 0.04633797166129716\n","Epoch: 6360, Train Loss: 0.051451283047708754, Test Loss: 0.046963496193483126\n","Epoch: 6361, Train Loss: 0.05161933329553475, Test Loss: 0.04627571555384184\n","Epoch: 6362, Train Loss: 0.05138988039559217, Test Loss: 0.04693021311534693\n","Epoch: 6363, Train Loss: 0.05158442119886207, Test Loss: 0.04626610181355827\n","Epoch: 6364, Train Loss: 0.051381224790245934, Test Loss: 0.0469505465862016\n","Epoch: 6365, Train Loss: 0.05160277965915039, Test Loss: 0.046308282216005456\n","Epoch: 6366, Train Loss: 0.0514244710374436, Test Loss: 0.0470237010375841\n","Epoch: 6367, Train Loss: 0.051673623269591344, Test Loss: 0.04640154462611729\n","Epoch: 6368, Train Loss: 0.05151890828107176, Test Loss: 0.04714900682215161\n","Epoch: 6369, Train Loss: 0.05179629447001697, Test Loss: 0.046545285260154286\n","Epoch: 6370, Train Loss: 0.0516639319789295, Test Loss: 0.047325886933718087\n","Epoch: 6371, Train Loss: 0.05197023031093144, Test Loss: 0.046738972379232906\n","Epoch: 6372, Train Loss: 0.05185900732239213, Test Loss: 0.047553815050914806\n","Epoch: 6373, Train Loss: 0.052194920566205026, Test Loss: 0.046982101509374\n","Epoch: 6374, Train Loss: 0.052103624197079354, Test Loss: 0.047832264919320765\n","Epoch: 6375, Train Loss: 0.052469857202504806, Test Loss: 0.04727414218371003\n","Epoch: 6376, Train Loss: 0.052397243685412605, Test Loss: 0.04816065104240434\n","Epoch: 6377, Train Loss: 0.0527944751672439, Test Loss: 0.047614476215275585\n","Epoch: 6378, Train Loss: 0.05273923612799439, Test Loss: 0.04853826073031782\n","Epoch: 6379, Train Loss: 0.05316808453374639, Test Loss: 0.04800232764918241\n","Epoch: 6380, Train Loss: 0.053128810904524135, Test Loss: 0.04896417777422081\n","Epoch: 6381, Train Loss: 0.053589794259243956, Test Loss: 0.048436684827140336\n","Epoch: 6382, Train Loss: 0.053564938382517496, Test Loss: 0.04943719838882604\n","Epoch: 6383, Train Loss: 0.05405842818479285, Test Loss: 0.04891621543965325\n","Epoch: 6384, Train Loss: 0.054046264927618236, Test Loss: 0.049955740610320355\n","Epoch: 6385, Train Loss: 0.05457243444944036, Test Loss: 0.049439176050946804\n","Epoch: 6386, Train Loss: 0.05457102248160624, Test Loss: 0.050517749055949385\n","Epoch: 6387, Train Loss: 0.05512979020941178, Test Loss: 0.05000331835758649\n","Epoch: 6388, Train Loss: 0.055136934991699214, Test Loss: 0.051120597836943504\n","Epoch: 6389, Train Loss: 0.05572790443854656, Test Loss: 0.05060579536633391\n","Epoch: 6390, Train Loss: 0.055741124899399835, Test Loss: 0.05176099543930416\n","Epoch: 6391, Train Loss: 0.05636352261013546, Test Loss: 0.05124307170952638\n","Epoch: 6392, Train Loss: 0.056380023927828746, Test Loss: 0.052434896490959976\n","Epoch: 6393, Train Loss: 0.05703263816664388, Test Loss: 0.051910843387147194\n","Epoch: 6394, Train Loss: 0.0570492934726078, Test Loss: 0.05313742642919603\n","Epoch: 6395, Train Loss: 0.05773041678298355, Test Loss: 0.05260397322985224\n","Epoch: 6396, Train Loss: 0.05774376089847937, Test Loss: 0.053862826042629784\n","Epoch: 6397, Train Loss: 0.05845114039467811, Test Loss: 0.05331644917784308\n","Epoch: 6398, Train Loss: 0.058457378833003726, Test Loss: 0.0546044235263364\n","Epoch: 6399, Train Loss: 0.05918817863335205, Test Loss: 0.054041372898704115\n","Epoch: 6400, Train Loss: 0.05918321496202574, Test Loss: 0.05535464187103476\n","Epoch: 6401, Train Loss: 0.05993399550219128, Test Loss: 0.05477098613961786\n","Epoch: 6402, Train Loss: 0.05991347968624508, Test Loss: 0.05610504891589258\n","Epoch: 6403, Train Loss: 0.060680198641201444, Test Loss: 0.05549674135005172\n","Epoch: 6404, Train Loss: 0.0606395981197464, Test Loss: 0.05684645606164581\n","Epoch: 6405, Train Loss: 0.06141763720761129, Test Loss: 0.0562094213845905\n","Epoch: 6406, Train Loss: 0.06135233116624481, Test Loss: 0.057569069360280264\n","Epoch: 6407, Train Loss: 0.062136552123602244, Test Loss: 0.05689931044534588\n","Epoch: 6408, Train Loss: 0.062041947742929, Test Loss: 0.05826269346647849\n","Epoch: 6409, Train Loss: 0.06282677921765256, Test Loss: 0.057556414909433425\n","Epoch: 6410, Train Loss: 0.06269844669819229, Test Loss: 0.058916984891218195\n","Epoch: 6411, Train Loss: 0.06347800174302268, Test Loss: 0.058170728515063504\n","Epoch: 6412, Train Loss: 0.06331182279675099, Test Loss: 0.0595217464368959\n","Epoch: 6413, Train Loss: 0.06408004419415997, Test Loss: 0.05873253190967181\n","Epoch: 6414, Train Loss: 0.06387236668565789, Test Loss: 0.060067250069132216\n","Epoch: 6415, Train Loss: 0.06462319471177769, Test Loss: 0.05923271228851727\n","Epoch: 6416, Train Loss: 0.06437098450170604, Test Loss: 0.06054457136158488\n","Epoch: 6417, Train Loss: 0.06509853923894626, Test Loss: 0.05966308534439701\n","Epoch: 6418, Train Loss: 0.06479951930590756, Test Loss: 0.060945915645770454\n","Epoch: 6419, Train Loss: 0.0654982875737161, Test Loss: 0.0600166995743806\n","Epoch: 6420, Train Loss: 0.06515105439704519, Test Loss: 0.06126491465603815\n","Epoch: 6421, Train Loss: 0.06581607010791544, Test Loss: 0.06028810260435535\n","Epoch: 6422, Train Loss: 0.06542017821566305, Test Loss: 0.06149687315735723\n","Epoch: 6423, Train Loss: 0.06604718472543948, Test Loss: 0.06047355084466263\n","Epoch: 6424, Train Loss: 0.06560319224437464, Test Loss: 0.06163894789336394\n","Epoch: 6425, Train Loss: 0.0661887761711708, Test Loss: 0.06057114744790878\n","Epoch: 6426, Train Loss: 0.06569824700170881, Test Loss: 0.06169024599246941\n","Epoch: 6427, Train Loss: 0.06623993499339594, Test Loss: 0.06058089886840216\n","Epoch: 6428, Train Loss: 0.06570539657508494, Test Loss: 0.06165183621386328\n","Epoch: 6429, Train Loss: 0.0662017094022084, Test Loss: 0.060504686722423436\n","Epoch: 6430, Train Loss: 0.06562656854169656, Test Loss: 0.06152667336388549\n","Epoch: 6431, Train Loss: 0.06607703033505177, Test Loss: 0.060346158349930595\n","Epoch: 6432, Train Loss: 0.06546545281398314, Test Loss: 0.06131944301834622\n","Epoch: 6433, Train Loss: 0.06587055682964449, Test Loss: 0.06011054567172105\n","Epoch: 6434, Train Loss: 0.06522731911172985, Test Loss: 0.061036339536880035\n","Epoch: 6435, Train Loss: 0.06558845466192943, Test Loss: 0.05980442690961129\n","Epoch: 6436, Train Loss: 0.06491877769831529, Test Loss: 0.06068479461268379\n","Epoch: 6437, Train Loss: 0.0652381254724226, Test Loss: 0.059435448990368324\n","Epoch: 6438, Train Loss: 0.0645475012295535, Test Loss: 0.0602731758937667\n","Epoch: 6439, Train Loss: 0.06482790590576636, Test Loss: 0.05901202976372111\n","Epoch: 6440, Train Loss: 0.06412192683261003, Test Loss: 0.05981047547270426\n","Epoch: 6441, Train Loss: 0.06436675655682707, Test Loss: 0.058543058592757095\n","Epoch: 6442, Train Loss: 0.06365095692649035, Test Loss: 0.05930600648322927\n","Epoch: 6443, Train Loss: 0.0638639589639097, Test Loss: 0.058037611723300936\n","Epoch: 6444, Train Loss: 0.06314367511976944, Test Loss: 0.05876912308453677\n","Epoch: 6445, Train Loss: 0.06332883593597793, Test Loss: 0.0575046955679779\n","Epoch: 6446, Train Loss: 0.06260909023732933, Test Loss: 0.0582089752899257\n","Epoch: 6447, Train Loss: 0.06277050667822319, Test Loss: 0.05695302717427985\n","Epoch: 6448, Train Loss: 0.062055917659072965, Test Loss: 0.057634305950045595\n","Epoch: 6449, Train Loss: 0.062197684033986164, Test Loss: 0.05639085718317279\n","Epoch: 6450, Train Loss: 0.06149240319712282, Test Loss: 0.05705329320927526\n","Epoch: 6451, Train Loss: 0.061618517168148715, Test Loss: 0.05582583693485695\n","Epoch: 6452, Train Loss: 0.0609261911005184, Test Loss: 0.05647343827377276\n","Epoch: 6453, Train Loss: 0.061040479535387694, Test Loss: 0.05526492832478757\n","Epoch: 6454, Train Loss: 0.06036423473869586, Test Loss: 0.05590149557882796\n","Epoch: 6455, Train Loss: 0.0604702992239398, Test Loss: 0.054714352706546496\n","Epoch: 6456, Train Loss: 0.059812746225126, Test Loss: 0.055343440505370566\n","Epoch: 6457, Train Loss: 0.05991392682617933, Test Loss: 0.05417957361073647\n","Epoch: 6458, Train Loss: 0.059277179730586535, Test Loss: 0.05480446864610702\n","Epoch: 6459, Train Loss: 0.05937653483683822, Test Loss: 0.05366530724235191\n","Epoch: 6460, Train Loss: 0.05876224244199727, Test Loss: 0.054289020160542996\n","Epoch: 6461, Train Loss: 0.05886254211792808, Test Loss: 0.05317555451668887\n","Epoch: 6462, Train Loss: 0.058271926930547197, Test Loss: 0.05380082284309555\n","Epoch: 6463, Train Loss: 0.058375657054316125, Test Loss: 0.05271364865202159\n","Epoch: 6464, Train Loss: 0.05780955895812672, Test Loss: 0.05334294800476954\n","Epoch: 6465, Train Loss: 0.05791893350050998, Test Loss: 0.052282312909508626\n","Epoch: 6466, Train Loss: 0.057377855327618535, Test Loss: 0.052917873990097654\n","Epoch: 6467, Train Loss: 0.05749483434100104, Test Loss: 0.05188372382475187\n","Epoch: 6468, Train Loss: 0.05697898713852944, Test Loss: 0.052527552992506076\n","Epoch: 6469, Train Loss: 0.057105298328656046, Test Loss: 0.05151957610264007\n","Epoch: 6470, Train Loss: 0.056614644636847546, Test Loss: 0.05217347769597559\n","Epoch: 6471, Train Loss: 0.056751806731000214, Test Loss: 0.05119114616620671\n","Epoch: 6472, Train Loss: 0.056286100666075485, Test Loss: 0.051856745090292165\n","Epoch: 6473, Train Loss: 0.056435447134200435, Test Loss: 0.05089935210634303\n","Epoch: 6474, Train Loss: 0.055994270480733986, Test Loss: 0.05157811553850315\n","Epoch: 6475, Train Loss: 0.05615697248624525, Test Loss: 0.050644808440184666\n","Epoch: 6476, Train Loss: 0.055739766342493204, Test Loss: 0.05133806579668268\n","Epoch: 6477, Train Loss: 0.05591685408247457, Test Loss: 0.050427874637937006\n","Epoch: 6478, Train Loss: 0.05552294586886731, Test Loss: 0.05113683519200851\n","Epoch: 6479, Train Loss: 0.05571532770248486, Test Loss: 0.05024869682015621\n","Epoch: 6480, Train Loss: 0.055343953544639654, Test Loss: 0.05097446456081049\n","Epoch: 6481, Train Loss: 0.055552432502878585, Test Loss: 0.05010724236831009\n","Epoch: 6482, Train Loss: 0.05520275514523693, Test Loss: 0.05085082784591043\n","Epoch: 6483, Train Loss: 0.05542804256761957, Test Loss: 0.05000332744432776\n","Epoch: 6484, Train Loss: 0.055099165072731875, Test Loss: 0.05076565646822241\n","Epoch: 6485, Train Loss: 0.05534189123290816, Test Loss: 0.04993663759585877\n","Epoch: 6486, Train Loss: 0.055032866785087536, Test Loss: 0.05071855673845993\n","Epoch: 6487, Train Loss: 0.055293588453779324, Test Loss: 0.04990674174973524\n","Epoch: 6488, Train Loss: 0.05500342662428599, Test Loss: 0.05070902067797048\n","Epoch: 6489, Train Loss: 0.05528263158215048, Test Loss: 0.049913099982621056\n","Epoch: 6490, Train Loss: 0.05501030143503292, Test Loss: 0.05073643068892054\n","Epoch: 6491, Train Loss: 0.05530840999656223, Test Loss: 0.04995506551954423\n","Epoch: 6492, Train Loss: 0.05505284042725791, Test Loss: 0.05080005856728865\n","Epoch: 6493, Train Loss: 0.05537020407640065, Test Loss: 0.05003188146059217\n","Epoch: 6494, Train Loss: 0.05513028178523511, Test Loss: 0.05089905939930881\n","Epoch: 6495, Train Loss: 0.05546717905989969, Test Loss: 0.050142672784111136\n","Epoch: 6496, Train Loss: 0.05524174457438046, Test Loss: 0.051032460932834395\n","Epoch: 6497, Train Loss: 0.05559837437542137, Test Loss: 0.05028643422968894\n","Epoch: 6498, Train Loss: 0.05538621655197505, Test Loss: 0.051199149077030444\n","Epoch: 6499, Train Loss: 0.055762689096885754, Test Loss: 0.050462014732190995\n","Epoch: 6500, Train Loss: 0.0555625385563336, Test Loss: 0.05139785026185812\n","Epoch: 6501, Train Loss: 0.055958864251811145, Test Loss: 0.05066809916290327\n","Epoch: 6502, Train Loss: 0.055769386233951126, Test Loss: 0.05162711148550693\n","Epoch: 6503, Train Loss: 0.056185462806770656, Test Loss: 0.05090318823658016\n","Epoch: 6504, Train Loss: 0.056005249967013254, Test Loss: 0.05188527899292314\n","Epoch: 6505, Train Loss: 0.05644084826987134, Test Loss: 0.051165577562175206\n","Epoch: 6506, Train Loss: 0.056268413982551314, Test Loss: 0.052170476658469175\n","Epoch: 6507, Train Loss: 0.056723162979738874, Test Loss: 0.051453336945439195\n","Epoch: 6508, Train Loss: 0.05655693575457176, Test Loss: 0.0524805852837768\n","Epoch: 6509, Train Loss: 0.05703030728872045, Test Loss: 0.05176429118515912\n","Epoch: 6510, Train Loss: 0.05686862694338565, Test Loss: 0.05281322415764902\n","Epoch: 6511, Train Loss: 0.057359920984251335, Test Loss: 0.05209600372991444\n","Epoch: 6512, Train Loss: 0.057201037240373405, Test Loss: 0.05316573634452351\n","Epoch: 6513, Train Loss: 0.05770936841267802, Test Loss: 0.05244576466379732\n","Epoch: 6514, Train Loss: 0.05755144258647314, Test Loss: 0.05353517925434434\n","Epoch: 6515, Train Loss: 0.05807572885711518, Test Loss: 0.05281058454974833\n","Epoch: 6516, Train Loss: 0.057916839290928265, Test Loss: 0.05391832208003138\n","Epoch: 6517, Train Loss: 0.058455793755445046, Test Loss: 0.053187195658293426\n","Epoch: 6518, Train Loss: 0.05829394557361651, Test Loss: 0.05431165164825316\n","Epoch: 6519, Train Loss: 0.058846072305480175, Test Loss: 0.053572062027622186\n","Epoch: 6520, Train Loss: 0.058679211969781035, Test Loss: 0.05471138809482395\n","Epoch: 6521, Train Loss: 0.059242806871471446, Test Loss: 0.053961399620045515\n","Epoch: 6522, Train Loss: 0.05906884185227141, Test Loss: 0.055113511530990304\n","Epoch: 6523, Train Loss: 0.05964199936269269, Test Loss: 0.0543512075464112\n","Epoch: 6524, Train Loss: 0.05945882303017815, Test Loss: 0.05551380050057942\n","Epoch: 6525, Train Loss: 0.06003944939011036, Test Loss: 0.05473731091797568\n","Epoch: 6526, Train Loss: 0.05984497096820799, Test Loss: 0.05590788253926007\n","Epoch: 6527, Train Loss: 0.06043080451982965, Test Loss: 0.0551154153588188\n","Epoch: 6528, Train Loss: 0.06022298364286979, Test Loss: 0.056291296546858924\n","Epoch: 6529, Train Loss: 0.060811622342820275, Test Loss: 0.05548117258807847\n","Epoch: 6530, Train Loss: 0.06058850742675477, Test Loss: 0.05665956599673417\n","Epoch: 6531, Train Loss: 0.06117744339420085, Test Loss: 0.05583025579091363\n","Epoch: 6532, Train Loss: 0.060937212702001343, Test Loss: 0.057008281272079554\n","Epoch: 6533, Train Loss: 0.061523873221443495, Test Loss: 0.056158442784553377\n","Epoch: 6534, Train Loss: 0.061264877192998723, Test Loss: 0.05733318869034233\n","Epoch: 6535, Train Loss: 0.061846671171822826, Test Loss: 0.056461704306588714\n","Epoch: 6536, Train Loss: 0.061567474332115436, Test Loss: 0.057630283115809436\n","Epoch: 6537, Train Loss: 0.06214184280737703, Test Loss: 0.056736294169003165\n","Epoch: 6538, Train Loss: 0.061841263392862604, Test Loss: 0.05789590053271558\n","Epoch: 6539, Train Loss: 0.062405732326485344, Test Loss: 0.05697883759577491\n","Epoch: 6540, Train Loss: 0.06208287770463459, Test Loss: 0.05812680661907814\n","Epoch: 6541, Train Loss: 0.062635111037112, Test Loss: 0.05718641384931742\n","Epoch: 6542, Train Loss: 0.06228940705673536, Test Loss: 0.058320277275043374\n","Epoch: 6543, Train Loss: 0.06282725783813704, Test Loss: 0.05735662929148544\n","Epoch: 6544, Train Loss: 0.06245847044619751, Test Loss: 0.05847416724922212\n","Epoch: 6545, Train Loss: 0.06298002785255295, Test Loss: 0.05748767733633647\n","Epoch: 6546, Train Loss: 0.06258827564133213, Test Loss: 0.05858696347701075\n","Epoch: 6547, Train Loss: 0.06309190582454054, Test Loss: 0.05757838232594606\n","Epoch: 6548, Train Loss: 0.06267766261206231, Test Loss: 0.05865782047230488\n","Epoch: 6549, Train Loss: 0.06316204161780352, Test Loss: 0.05762822516150056\n","Epoch: 6550, Train Loss: 0.06272612868255492, Test Loss: 0.05868657604671333\n","Epoch: 6551, Train Loss: 0.06319026608356887, Test Loss: 0.057637349489068776\n","Epoch: 6552, Train Loss: 0.06273383423055431, Test Loss: 0.05867374669400136\n","Epoch: 6553, Train Loss: 0.06317708662909083, Test Loss: 0.057606548293954916\n","Epoch: 6554, Train Loss: 0.0627015888120486, Test Loss: 0.05862050308326907\n","Epoch: 6555, Train Loss: 0.06312366292260092, Test Loss: 0.057537231811104504\n","Epoch: 6556, Train Loss: 0.06263081864127754, Test Loss: 0.05852862715956193\n","Epoch: 6557, Train Loss: 0.06303176422567522, Test Loss: 0.05743137862461696\n","Epoch: 6558, Train Loss: 0.06252351731784468, Test Loss: 0.05840045326961805\n","Epoch: 6559, Train Loss: 0.06290371076328533, Test Loss: 0.0572914726312356\n","Epoch: 6560, Train Loss: 0.06238218248948512, Test Loss: 0.058238796444398014\n","Epoch: 6561, Train Loss: 0.06274230225637775, Test Loss: 0.057120429124549864\n","Epoch: 6562, Train Loss: 0.062209741715232594, Test Loss: 0.05804687143398442\n","Epoch: 6563, Train Loss: 0.06255073720664538, Test Loss: 0.05692151358711273\n","Epoch: 6564, Train Loss: 0.06200947111844209, Test Loss: 0.05782820628659901\n","Epoch: 6565, Train Loss: 0.06233252672031397, Test Loss: 0.05669825685164144\n","Epoch: 6566, Train Loss: 0.061784910487780974, Test Loss: 0.05758655420091593\n","Epoch: 6567, Train Loss: 0.06209140659621457, Test Loss: 0.05645437012913115\n","Epoch: 6568, Train Loss: 0.06153977831651108, Test Loss: 0.057325807091549025\n","Epoch: 6569, Train Loss: 0.06183125111506892, Test Loss: 0.056193663039785305\n","Epoch: 6570, Train Loss: 0.06127788990513771, Test Loss: 0.05704991384018823\n","Epoch: 6571, Train Loss: 0.061555991500353985, Test Loss: 0.05591996727367433\n","Epoch: 6572, Train Loss: 0.06100308114143797, Test Loss: 0.056762805617276584\n","Epoch: 6573, Train Loss: 0.061269541434230884, Test Loss: 0.05563706790914307\n","Epoch: 6574, Train Loss: 0.06071913997214361, Test Loss: 0.0564683300110323\n","Epoch: 6575, Train Loss: 0.06097573136449663, Test Loss: 0.05534864378420784\n","Epoch: 6576, Train Loss: 0.06042974694801116, Test Loss: 0.056170195047227596\n","Epoch: 6577, Train Loss: 0.060678252685554525, Test Loss: 0.05505821769905147\n","Epoch: 6578, Train Loss: 0.06013842560798939, Test Loss: 0.055871923570431456\n","Epoch: 6579, Train Loss: 0.060380612264023255, Test Loss: 0.05476911666558847\n","Epoch: 6580, Train Loss: 0.05984850290777935, Test Loss: 0.05557681791956796\n","Epoch: 6581, Train Loss: 0.060086097241995304, Test Loss: 0.05448444194042326\n","Epoch: 6582, Train Loss: 0.05956307942051672, Test Loss: 0.05528793438912567\n","Epoch: 6583, Train Loss: 0.059797749609634444, Test Loss: 0.05420704819589624\n","Epoch: 6584, Train Loss: 0.05928500865788834, Test Loss: 0.05500806663182118\n","Epoch: 6585, Train Loss: 0.059518349703429466, Test Loss: 0.053939530905094464\n","Epoch: 6586, Train Loss: 0.05901688458332636, Test Loss: 0.0547397369285044\n","Epoch: 6587, Train Loss: 0.0592504075565416, Test Loss: 0.053684220836824184\n","Epoch: 6588, Train Loss: 0.058761036210996084, Test Loss: 0.05448519411851969\n","Epoch: 6589, Train Loss: 0.05899616089523153, Test Loss: 0.05344318446545941\n","Epoch: 6590, Train Loss: 0.058519528094878294, Test Loss: 0.05424641693548278\n","Epoch: 6591, Train Loss: 0.05875757852720123, Test Loss: 0.05321822908432996\n","Epoch: 6592, Train Loss: 0.058294165497339394, Test Loss: 0.05402512151364879\n","Epoch: 6593, Train Loss: 0.05853636788798781, Test Loss: 0.0530109114542096\n","Epoch: 6594, Train Loss: 0.05808650307049304, Test Loss: 0.05382277190227774\n","Epoch: 6595, Train Loss: 0.05833398558383669, Test Loss: 0.052822548904893164\n","Epoch: 6596, Train Loss: 0.05789785597079506, Test Loss: 0.053640592533981936\n","Epoch: 6597, Train Loss: 0.05815164987809242, Test Loss: 0.0526542319234276\n","Epoch: 6598, Train Loss: 0.05772931244333445, Test Loss: 0.053479581724169334\n","Epoch: 6599, Train Loss: 0.057990354199270684, Test Loss: 0.05250683739495263\n","Epoch: 6600, Train Loss: 0.0575817470449003, Test Loss: 0.053340525420957574\n","Epoch: 6601, Train Loss: 0.057850880891196596, Test Loss: 0.05238104180138703\n","Epoch: 6602, Train Loss: 0.05745583381424993, Test Loss: 0.05322401056949082\n","Epoch: 6603, Train Loss: 0.05773381457007567, Test Loss: 0.05227733382188121\n","Epoch: 6604, Train Loss: 0.05735205883662928, Test Loss: 0.05313043759507882\n","Epoch: 6605, Train Loss: 0.0576395545937392, Test Loss: 0.05219602591186729\n","Epoch: 6606, Train Loss: 0.05727073178237569, Test Loss: 0.05306003164184361\n","Epoch: 6607, Train Loss: 0.05756832628043143, Test Loss: 0.05213726456144691\n","Epoch: 6608, Train Loss: 0.05721199612316066, Test Loss: 0.05301285232528784\n","Epoch: 6609, Train Loss: 0.05752019063606875, Test Loss: 0.052101039047155545\n","Epoch: 6610, Train Loss: 0.057175837842527846, Test Loss: 0.05298880186748552\n","Epoch: 6611, Train Loss: 0.05749505245900641, Test Loss: 0.0520871885933913\n","Epoch: 6612, Train Loss: 0.057162092559425975, Test Loss: 0.05298763158254379\n","Epoch: 6613, Train Loss: 0.05749266679010726, Test Loss: 0.052095407951374226\n","Epoch: 6614, Train Loss: 0.057170451074805693, Test Loss: 0.05300894676832108\n","Epoch: 6615, Train Loss: 0.057512643764058, Test Loss: 0.05212525148524116\n","Epoch: 6616, Train Loss: 0.05720046343289056, Test Loss: 0.05305221013912917\n","Epoch: 6617, Train Loss: 0.05755445199644383, Test Loss: 0.05217613592775527\n","Epoch: 6618, Train Loss: 0.057251541661402484, Test Loss: 0.05311674400430585\n","Epoch: 6619, Train Loss: 0.057617420711102624, Test Loss: 0.05224734203301024\n","Epoch: 6620, Train Loss: 0.05732296141971413, Test Loss: 0.053201731459925836\n","Epoch: 6621, Train Loss: 0.057700740874536936, Test Loss: 0.05233801541102215\n","Epoch: 6622, Train Loss: 0.057413862841185166, Test Loss: 0.05330621691590397\n","Epoch: 6623, Train Loss: 0.057803465659072045, Test Loss: 0.05244716687936238\n","Epoch: 6624, Train Loss: 0.05752325090594191, Test Loss: 0.053429106328229586\n","Epoch: 6625, Train Loss: 0.05792451060390094, Test Loss: 0.05257367270962781\n","Epoch: 6626, Train Loss: 0.057649995722703025, Test Loss: 0.05356916754532428\n","Epoch: 6627, Train Loss: 0.058062653882437976, Test Loss: 0.05271627518057936\n","Epoch: 6628, Train Loss: 0.05779283313193784, Test Loss: 0.05372503120726499\n","Epoch: 6629, Train Loss: 0.05821653711423793, Test Loss: 0.05287358387371155\n","Epoch: 6630, Train Loss: 0.05795036606616642, Test Loss: 0.05389519265498002\n","Epoch: 6631, Train Loss: 0.05838466717825378, Test Loss: 0.053044078158845535\n","Epoch: 6632, Train Loss: 0.05812106711457124, Test Loss: 0.0540780153112644\n","Epoch: 6633, Train Loss: 0.05856541948915699, Test Loss: 0.05322611131472685\n","Epoch: 6634, Train Loss: 0.05830328273595211, Test Loss: 0.05427173598402627\n","Epoch: 6635, Train Loss: 0.05875704318728149, Test Loss: 0.053417916710113096\n","Epoch: 6636, Train Loss: 0.058495239543989004, Test Loss: 0.054474472512057615\n","Epoch: 6637, Train Loss: 0.05895766866295468, Test Loss: 0.053617616432129686\n","Epoch: 6638, Train Loss: 0.0586950530494683, Test Loss: 0.05468423412261251\n","Epoch: 6639, Train Loss: 0.0591653177853306, Test Loss: 0.05382323268890048\n","Epoch: 6640, Train Loss: 0.058900739183755005, Test Loss: 0.054898934796600116\n","Epoch: 6641, Train Loss: 0.0593779171327495, Test Loss: 0.05403270223157201\n","Epoch: 6642, Train Loss: 0.05911022884533475, Test Loss: 0.055116409840758704\n","Epoch: 6643, Train Loss: 0.05959331442559504, Test Loss: 0.05424389393696728\n","Epoch: 6644, Train Loss: 0.059321385606865056, Test Loss: 0.05533443574762361\n","Epoch: 6645, Train Loss: 0.05980929824443768, Test Loss: 0.05445462956782494\n","Epoch: 6646, Train Loss: 0.059532026595509865, Test Loss: 0.05555075328597881\n","Epoch: 6647, Train Loss: 0.060023620978463164, Test Loss: 0.054662707586245106\n","Epoch: 6648, Train Loss: 0.0597399464177641, Test Loss: 0.05576309361124461\n","Epoch: 6649, Train Loss: 0.060234024796211844, Test Loss: 0.054865929742749134\n","Epoch: 6650, Train Loss: 0.05994294384672739, Test Loss: 0.055969207023232076\n","Epoch: 6651, Train Loss: 0.06043827026881721, Test Loss: 0.05506213000525396\n","Epoch: 6652, Train Loss: 0.06013885083186702, Test Loss: 0.05616689383601231\n","Epoch: 6653, Train Loss: 0.060634167113326344, Test Loss: 0.05524920523778557\n","Epoch: 6654, Train Loss: 0.0603255632372683, Test Loss: 0.056354036670798045\n","Epoch: 6655, Train Loss: 0.06081960636980218, Test Loss: 0.05542514689751786\n","Epoch: 6656, Train Loss: 0.06050107257380869, Test Loss: 0.056528633348013384\n","Epoch: 6657, Train Loss: 0.0609925931910261, Test Loss: 0.05558807290075508\n","Epoch: 6658, Train Loss: 0.06066349787362311, Test Loss: 0.056688829449427124\n","Epoch: 6659, Train Loss: 0.06115127931804787, Test Loss: 0.055736258723380644\n","Epoch: 6660, Train Loss: 0.06081111677122007, Test Loss: 0.05683294955472108\n","Epoch: 6661, Train Loss: 0.06129399424794356, Test Loss: 0.05586816675737602\n","Epoch: 6662, Train Loss: 0.06094239481290158, Test Loss: 0.05695952613669059\n","Epoch: 6663, Train Loss: 0.06141927407949657, Test Loss: 0.055982472948353804\n","Epoch: 6664, Train Loss: 0.061056012020743844, Test Loss: 0.057067325130195856\n","Epoch: 6665, Train Loss: 0.06152588705290382, Test Loss: 0.05607808979274876\n","Epoch: 6666, Train Loss: 0.061150885792331224, Test Loss: 0.057155367273344644\n","Epoch: 6667, Train Loss: 0.06161285488239628, Test Loss: 0.056154184876951924\n","Epoch: 6668, Train Loss: 0.06122618932219012, Test Loss: 0.057222944452701985\n","Epoch: 6669, Train Loss: 0.061679469113401324, Test Loss: 0.05621019429006798\n","Epoch: 6670, Train Loss: 0.061281364881184686, Test Loss: 0.05726963046122648\n","Epoch: 6671, Train Loss: 0.0617253019122236, Test Loss: 0.05624583042939013\n","Epoch: 6672, Train Loss: 0.06131613147820169, Test Loss: 0.05729528578830218\n","Epoch: 6673, Train Loss: 0.061750210906386716, Test Loss: 0.05626108393227876\n","Epoch: 6674, Train Loss: 0.06133048664338678, Test Loss: 0.0573000562930734\n","Epoch: 6675, Train Loss: 0.06175433792518793, Test Loss: 0.056256219696906674\n","Epoch: 6676, Train Loss: 0.06132470230098497, Test Loss: 0.05728436585105218\n","Epoch: 6677, Train Loss: 0.06173810172843951, Test Loss: 0.05623176718320296\n","Epoch: 6678, Train Loss: 0.061299314928396416, Test Loss: 0.05724890329492228\n","Epoch: 6679, Train Loss: 0.061702185042082365, Test Loss: 0.056188505400338755\n","Epoch: 6680, Train Loss: 0.0612551104124722, Test Loss: 0.05719460417975816\n","Epoch: 6681, Train Loss: 0.061647516428508145, Test Loss: 0.056127443175667854\n","Epoch: 6682, Train Loss: 0.061193104201811635, Test Loss: 0.05712262807862165\n","Epoch: 6683, Train Loss: 0.061575247695121446, Test Loss: 0.05604979545190993\n","Epoch: 6684, Train Loss: 0.06111451750470396, Test Loss: 0.05703433224775378\n","Epoch: 6685, Train Loss: 0.06148672767796272, Test Loss: 0.05595695646744998\n","Epoch: 6686, Train Loss: 0.06102075038936669, Test Loss: 0.05693124258593385\n","Epoch: 6687, Train Loss: 0.06138347332267317, Test Loss: 0.05585047073533396\n","Epoch: 6688, Train Loss: 0.06091335270276045, Test Loss: 0.056815022848342236\n","Epoch: 6689, Train Loss: 0.06126713902100749, Test Loss: 0.055732002749850525\n","Epoch: 6690, Train Loss: 0.06079399373656042, Test Loss: 0.056687443063337214\n","Epoch: 6691, Train Loss: 0.06113948514937406, Test Loss: 0.05560330631873838\n","Epoch: 6692, Train Loss: 0.06066443153713469, Test Loss: 0.05655034804591196\n","Epoch: 6693, Train Loss: 0.06100234670146509, Test Loss: 0.05546619434996032\n","Epoch: 6694, Train Loss: 0.06052648268657233, Test Loss: 0.05640562681157358\n","Epoch: 6695, Train Loss: 0.06085760281724815, Test Loss: 0.05532250982245968\n","Epoch: 6696, Train Loss: 0.06038199328176706, Test Loss: 0.05625518357784692\n","Epoch: 6697, Train Loss: 0.060707147894293746, Test Loss: 0.0551740985492731\n","Epoch: 6698, Train Loss: 0.060232811717209636, Test Loss: 0.05610091090701999\n","Epoch: 6699, Train Loss: 0.06055286483405563, Test Loss: 0.055022784207956144\n","Epoch: 6700, Train Loss: 0.06008076374360846, Test Loss: 0.05594466540245153\n","Epoch: 6701, Train Loss: 0.0603966008346438, Test Loss: 0.05487034597617431\n","Epoch: 6702, Train Loss: 0.05992763013740718, Test Loss: 0.055788246230217525\n","Epoch: 6703, Train Loss: 0.06024014600128802, Test Loss: 0.054718498977174714\n","Epoch: 6704, Train Loss: 0.05977512718332571, Test Loss: 0.05563337660525034\n","Epoch: 6705, Train Loss: 0.06008521491324704, Test Loss: 0.05456887761691842\n","Epoch: 6706, Train Loss: 0.05962489004941756, Test Loss: 0.05548168826197504\n","Epoch: 6707, Train Loss: 0.05993343116693363, Test Loss: 0.05442302178654907\n","Epoch: 6708, Train Loss: 0.05947845902640303, Test Loss: 0.05533470882761183\n","Epoch: 6709, Train Loss: 0.05978631481332795, Test Loss: 0.054282365813604315\n","Epoch: 6710, Train Loss: 0.059337268513181, Test Loss: 0.05519385193399932\n","Epoch: 6711, Train Loss: 0.05964527252555961, Test Loss: 0.05414822997440057\n","Epoch: 6712, Train Loss: 0.05920263855986173, Test Loss: 0.05506040984169997\n","Epoch: 6713, Train Loss: 0.05951159027054314, Test Loss: 0.05402181432850032\n","Epoch: 6714, Train Loss: 0.05907576872854832, Test Loss: 0.054935548307806205\n","Epoch: 6715, Train Loss: 0.05938642821628794, Test Loss: 0.0539041946031949\n","Epoch: 6716, Train Loss: 0.05895773399948875, Test Loss: 0.05482030340480719\n","Epoch: 6717, Train Loss: 0.05927081758250657, Test Loss: 0.05379631983982179\n","Epoch: 6718, Train Loss: 0.058849482434432866, Test Loss: 0.054715579990049196\n","Epoch: 6719, Train Loss: 0.059165659134357486, Test Loss: 0.053699011512296516\n","Epoch: 6720, Train Loss: 0.05875183430786759, Test Loss: 0.05462215153127377\n","Epoch: 6721, Train Loss: 0.05907172302513516, Test Loss: 0.053612963839040656\n","Epoch: 6722, Train Loss: 0.058665482427826286, Test Loss: 0.0545406610108437\n","Epoch: 6723, Train Loss: 0.0589896497108551, Test Loss: 0.053538745030018105\n","Epoch: 6724, Train Loss: 0.058590993388669535, Test Loss: 0.05447162265702988\n","Epoch: 6725, Train Loss: 0.05891995168543749, Test Loss: 0.05347679923843534\n","Epoch: 6726, Train Loss: 0.058528809526199224, Test Loss: 0.0544154242827153\n","Epoch: 6727, Train Loss: 0.058863015817160455, Test Loss: 0.05342744901960903\n","Epoch: 6728, Train Loss: 0.058479251378490055, Test Loss: 0.054372330047940946\n","Epoch: 6729, Train Loss: 0.05881910610309575, Test Loss: 0.05339089813558836\n","Epoch: 6730, Train Loss: 0.05844252049194252, Test Loss: 0.054342483500994625\n","Epoch: 6731, Train Loss: 0.058788366696482894, Test Loss: 0.053367234581635274\n","Epoch: 6732, Train Loss: 0.05841870244957777, Test Loss: 0.05432591079164117\n","Epoch: 6733, Train Loss: 0.05877082510085886, Test Loss: 0.05335643374822445\n","Epoch: 6734, Train Loss: 0.05840777003612215, Test Loss: 0.05432252398830762\n","Epoch: 6735, Train Loss: 0.05876639546294052, Test Loss: 0.05335836166861481\n","Epoch: 6736, Train Loss: 0.0584095864907631, Test Loss: 0.054332124467471675\n","Epoch: 6737, Train Loss: 0.05877488193265722, Test Loss: 0.053372778336327614\n","Epoch: 6738, Train Loss: 0.0584239088326596, Test Loss: 0.05435440637728194\n","Epoch: 6739, Train Loss: 0.05879598209248481, Test Loss: 0.053399341108224654\n","Epoch: 6740, Train Loss: 0.05845039127554874, Test Loss: 0.054388960207797134\n","Epoch: 6741, Train Loss: 0.05882929048857333, Test Loss: 0.053437608236623656\n","Epoch: 6742, Train Loss: 0.05848858877541029, Test Loss: 0.05443527652654275\n","Epoch: 6743, Train Loss: 0.0588743023224667, Test Loss: 0.05348704259745137\n","Epoch: 6744, Train Loss: 0.058537960778579964, Test Loss: 0.054492749959771096\n","Epoch: 6745, Train Loss: 0.058930417383907495, Test Loss: 0.053547015700250264\n","Epoch: 6746, Train Loss: 0.058597875256359684, Test Loss: 0.054560683516357025\n","Epoch: 6747, Train Loss: 0.05899694432179014, Test Loss: 0.0536168120794689\n","Epoch: 6748, Train Loss: 0.05866761312562324, Test Loss: 0.05463829336219781\n","Epoch: 6749, Train Loss: 0.059073105361303065, Test Loss: 0.05369563417441819\n","Epoch: 6750, Train Loss: 0.05874637316268749, Test Loss: 0.05472471415788389\n","Epoch: 6751, Train Loss: 0.05915804158025361, Test Loss: 0.053782607807197756\n","Epoch: 6752, Train Loss: 0.05883327751945532, Test Loss: 0.05481900507092907\n","Epoch: 6753, Train Loss: 0.05925081885616348, Test Loss: 0.05387678836349435\n","Epoch: 6754, Train Loss: 0.05892737794624537, Test Loss: 0.05492015656573563\n","Epoch: 6755, Train Loss: 0.059350434587695435, Test Loss: 0.053977167770243535\n","Epoch: 6756, Train Loss: 0.05902766281462395, Test Loss: 0.055027098059629506\n","Epoch: 6757, Train Loss: 0.059455825279226084, Test Loss: 0.05408268234672214\n","Epoch: 6758, Train Loss: 0.05913306501595387, Test Loss: 0.055138706511819846\n","Epoch: 6759, Train Loss: 0.05956587505600157, Test Loss: 0.054192221581900644\n","Epoch: 6760, Train Loss: 0.05924247078747208, Test Loss: 0.05525381598435775\n","Epoch: 6761, Train Loss: 0.05967942514963978, Test Loss: 0.05430463786127819\n","Epoch: 6762, Train Loss: 0.05935472948796487, Test Loss: 0.055371228180695696\n","Epoch: 6763, Train Loss: 0.05979528436036953, Test Loss: 0.05441875713169731\n","Epoch: 6764, Train Loss: 0.05946866431027631, Test Loss: 0.05548972392923323\n","Epoch: 6765, Train Loss: 0.05991224046427666, Test Loss: 0.054533390453851804\n","Epoch: 6766, Train Loss: 0.05958308387903784, Test Loss: 0.05560807553752869\n","Epoch: 6767, Train Loss: 0.06002907249220024, Test Loss: 0.05464734635073199\n","Epoch: 6768, Train Loss: 0.0596967946405199, Test Loss: 0.0557250598992491\n","Epoch: 6769, Train Loss: 0.060144563763375396, Test Loss: 0.05475944381775972\n","Epoch: 6770, Train Loss: 0.059808613909049554, Test Loss: 0.0558394721922915\n","Epoch: 6771, Train Loss: 0.06025751551331698, Test Loss: 0.05486852581879309\n","Epoch: 6772, Train Loss: 0.05991738339295713, Test Loss: 0.05595013996492478\n","Epoch: 6773, Train Loss: 0.06036676091386177, Test Loss: 0.05497347305357851\n","Epoch: 6774, Train Loss: 0.06002198298456123, Test Loss: 0.05605593736945982\n","Epoch: 6775, Train Loss: 0.0604711792459244, Test Loss: 0.05507321774875282\n","Epoch: 6776, Train Loss: 0.06012134456542721, Test Loss: 0.0561557992720644\n","Epoch: 6777, Train Loss: 0.06056970995457579, Test Loss: 0.05516675719822214\n","Epoch: 6778, Train Loss: 0.06021446555210602, Test Loss: 0.056248734944938025\n","Epoch: 6779, Train Loss: 0.06066136629356475, Test Loss: 0.055253166761514406\n","Epoch: 6780, Train Loss: 0.06030042189061757, Test Loss: 0.0563338410348963\n","Epoch: 6781, Train Loss: 0.0607452482541237, Test Loss: 0.05533161202203485\n","Epoch: 6782, Train Loss: 0.06037838020158421, Test Loss: 0.056410313501809965\n","Epoch: 6783, Train Loss: 0.060820554472150255, Test Loss: 0.055401359812066675\n","Epoch: 6784, Train Loss: 0.06044760878315145, Test Loss: 0.05647745823200928\n","Epoch: 6785, Train Loss: 0.06088659281935989, Test Loss: 0.05546178782826736\n","Epoch: 6786, Train Loss: 0.06050748719605624, Test Loss: 0.05653470005583061\n","Epoch: 6787, Train Loss: 0.06094278940789106, Test Loss: 0.05551239259009705\n","Epoch: 6788, Train Loss: 0.06055751418417478, Test Loss: 0.05658158993428373\n","Epoch: 6789, Train Loss: 0.06098869577345734, Test Loss: 0.05555279553315657\n","Epoch: 6790, Train Loss: 0.060597313723675585, Test Loss: 0.05661781012607502\n","Epoch: 6791, Train Loss: 0.06102399404820873, Test Loss: 0.055582747078217226\n","Epoch: 6792, Train Loss: 0.06062663904290331, Test Loss: 0.05664317720093988\n","Epoch: 6793, Train Loss: 0.061048499988997335, Test Loss: 0.05560212857268207\n","Epoch: 6794, Train Loss: 0.06064537451121204, Test Loss: 0.056657642825966206\n","Epoch: 6795, Train Loss: 0.06106216378729843, Test Loss: 0.05561095206171825\n","Epoch: 6796, Train Loss: 0.06065353535553081, Test Loss: 0.05666129231545208\n","Epoch: 6797, Train Loss: 0.061065068650743966, Test Loss: 0.05560935790849913\n","Epoch: 6798, Train Loss: 0.060651265225639364, Test Loss: 0.056654340998819934\n","Epoch: 6799, Train Loss: 0.0610574272100706, Test Loss: 0.05559761034394484\n","Epoch: 6800, Train Loss: 0.060638831690007465, Test Loss: 0.056637128522189384\n","Epoch: 6801, Train Loss: 0.06103957586625591, Test Loss: 0.05557609108323748\n","Epoch: 6802, Train Loss: 0.060616619800806286, Test Loss: 0.05661011125462054\n","Epoch: 6803, Train Loss: 0.061011967247944554, Test Loss: 0.055545291196673334\n","Epoch: 6804, Train Loss: 0.060585123916799745, Test Loss: 0.0565738530173781\n","Epoch: 6805, Train Loss: 0.06097516099654938, Test Loss: 0.05550580146402287\n","Epoch: 6806, Train Loss: 0.060544938014212735, Test Loss: 0.05652901439201366\n","Epoch: 6807, Train Loss: 0.060929813133833735, Test Loss: 0.05545830147300961\n","Epoch: 6808, Train Loss: 0.060496744746866386, Test Loss: 0.05647634088939368\n","Epoch: 6809, Train Loss: 0.06087666429310633, Test Loss: 0.05540354774290179\n","Epoch: 6810, Train Loss: 0.06044130353700314, Test Loss: 0.05641665027650287\n","Epoch: 6811, Train Loss: 0.060816527109884715, Test Loss: 0.05534236116334261\n","Epoch: 6812, Train Loss: 0.06037943798710762, Test Loss: 0.0563508193610455\n","Epoch: 6813, Train Loss: 0.060750273071113234, Test Loss: 0.055275614036811915\n","Epoch: 6814, Train Loss: 0.06031202290107134, Test Loss: 0.056279770526298914\n","Epoch: 6815, Train Loss: 0.060678819114510237, Test Loss: 0.055204217001463884\n","Epoch: 6816, Train Loss: 0.06023997119119954, Test Loss: 0.056204458291582805\n","Epoch: 6817, Train Loss: 0.060603114252595416, Test Loss: 0.05512910609091738\n","Epoch: 6818, Train Loss: 0.060164220927230616, Test Loss: 0.05612585614873022\n","Epoch: 6819, Train Loss: 0.06052412647104725, Test Loss: 0.05505123016055934\n","Epoch: 6820, Train Loss: 0.060085722756397286, Test Loss: 0.056044943893924476\n","Epoch: 6821, Train Loss: 0.06044283012010189, Test Loss: 0.05497153887793448\n","Epoch: 6822, Train Loss: 0.06000542789149248, Test Loss: 0.05596269563921414\n","Epoch: 6823, Train Loss: 0.06036019398273774, Test Loss: 0.054890971439743017\n","Epoch: 6824, Train Loss: 0.05992427682881039, Test Loss: 0.05588006865083777\n","Epoch: 6825, Train Loss: 0.06027717016629474, Test Loss: 0.05481044614168307\n","Epoch: 6826, Train Loss: 0.05984318892154459, Test Loss: 0.05579799312404138\n","Epoch: 6827, Train Loss: 0.06019468392681249, Test Loss: 0.05473085089153427\n","Epoch: 6828, Train Loss: 0.05976305289841447, Test Loss: 0.05571736296791691\n","Epoch: 6829, Train Loss: 0.06011362449929586, Test Loss: 0.05465303472190276\n","Epoch: 6830, Train Loss: 0.0596847183833687, Test Loss: 0.055639027640282446\n","Epoch: 6831, Train Loss: 0.0600348369736837, Test Loss: 0.05457780032809907\n","Epoch: 6832, Train Loss: 0.059608988441342194, Test Loss: 0.055563785042762485\n","Epoch: 6833, Train Loss: 0.05995911522650206, Test Loss: 0.054505897629535456\n","Epoch: 6834, Train Loss: 0.05953661314804299, Test Loss: 0.055492375460718835\n","Epoch: 6835, Train Loss: 0.05988719589273604, Test Loss: 0.054438018330362874\n","Epoch: 6836, Train Loss: 0.05946828415917685, Test Loss: 0.055425476511942004\n","Epoch: 6837, Train Loss: 0.05981975334176835, Test Loss: 0.05437479143709365\n","Epoch: 6838, Train Loss: 0.059404630236636155, Test Loss: 0.05536369905215028\n","Epoch: 6839, Train Loss: 0.05975739560541537, Test Loss: 0.05431677967768851\n","Epoch: 6840, Train Loss: 0.05934621367600704, Test Loss: 0.055307583974282155\n","Epoch: 6841, Train Loss: 0.059700661195069775, Test Loss: 0.05426447675784798\n","Epoch: 6842, Train Loss: 0.0592935275710941, Test Loss: 0.0552575998319939\n","Epoch: 6843, Train Loss: 0.05965001673841684, Test Loss: 0.05421830538568748\n","Epoch: 6844, Train Loss: 0.0592469938466768, Test Loss: 0.0552141412152636\n","Epoch: 6845, Train Loss: 0.05960585536370716, Test Loss: 0.05417861599512399\n","Epoch: 6846, Train Loss: 0.05920696198992403, Test Loss: 0.055177527807018595\n","Epoch: 6847, Train Loss: 0.059568495760604565, Test Loss: 0.054145686100606376\n","Epoch: 6848, Train Loss: 0.05917370841324554, Test Loss: 0.05514800405363128\n","Epoch: 6849, Train Loss: 0.059538181850571754, Test Loss: 0.05411972022068706\n","Epoch: 6850, Train Loss: 0.05914743638626166, Test Loss: 0.055125739388364334\n","Epoch: 6851, Train Loss: 0.05951508300600559, Test Loss: 0.05410085031474756\n","Epoch: 6852, Train Loss: 0.059128276481406915, Test Loss: 0.055110828954727846\n","Epoch: 6853, Train Loss: 0.05949929476522415, Test Loss: 0.0540891366853461\n","Epoch: 6854, Train Loss: 0.05911628748584785, Test Loss: 0.05510329478565543\n","Epoch: 6855, Train Loss: 0.05949083999936134, Test Loss: 0.05408456930756741\n","Epoch: 6856, Train Loss: 0.05911145774129971, Test Loss: 0.05510308740380797\n","Epoch: 6857, Train Loss: 0.05948967049663452, Test Loss: 0.05408706955589044\n","Epoch: 6858, Train Loss: 0.059113706882444765, Test Loss: 0.055110087817653405\n","Epoch: 6859, Train Loss: 0.05949566893879948, Test Loss: 0.05409649230793551\n","Epoch: 6860, Train Loss: 0.05912288795347104, Test Loss: 0.05512410989676364\n","Epoch: 6861, Train Loss: 0.05950865125340924, Test Loss: 0.05411262841257391\n","Epoch: 6862, Train Loss: 0.05913878989033223, Test Loss: 0.05514490311757347\n","Epoch: 6863, Train Loss: 0.05952836933331075, Test Loss: 0.05413520751688963\n","Epoch: 6864, Train Loss: 0.059161140363289046, Test Loss: 0.05517215567731895\n","Epoch: 6865, Train Loss: 0.059554514121301104, Test Loss: 0.05416390125203833\n","Epoch: 6866, Train Loss: 0.05918960897979136, Test Loss: 0.055205497978681246\n","Epoch: 6867, Train Loss: 0.05958671906269177, Test Loss: 0.05419832678189246\n","Epoch: 6868, Train Loss: 0.05922381085154739, Test Loss: 0.055244506490589804\n","Epoch: 6869, Train Loss: 0.05962456393148102, Test Loss: 0.05423805072029077\n","Epoch: 6870, Train Loss: 0.0592633105314897, Test Loss: 0.05528870799150135\n","Epoch: 6871, Train Loss: 0.05966757903671993, Test Loss: 0.05428259342258197\n","Epoch: 6872, Train Loss: 0.05930762632615828, Test Loss: 0.05533758420018049\n","Epoch: 6873, Train Loss: 0.0597152498143968, Test Loss: 0.05433143365492758\n","Epoch: 6874, Train Loss: 0.05935623498673181, Test Loss: 0.05539057679553891\n","Epoch: 6875, Train Loss: 0.05976702180672476, Test Loss: 0.05438401364050565\n","Epoch: 6876, Train Loss: 0.059408576777557204, Test Loss: 0.055447092821517545\n","Epoch: 6877, Train Loss: 0.059822306025175576, Test Loss: 0.05443974447544481\n","Epoch: 6878, Train Loss: 0.05946406091466144, Test Loss: 0.05550651046544475\n","Epoch: 6879, Train Loss: 0.059880484686082036, Test Loss: 0.054498011899203884\n","Epoch: 6880, Train Loss: 0.059522071358570114, Test Loss: 0.05556818518904713\n","Epoch: 6881, Train Loss: 0.05994091729840435, Test Loss: 0.05455818239445041\n","Epoch: 6882, Train Loss: 0.05958197293606256, Test Loss: 0.055631456180601335\n","Epoch: 6883, Train Loss: 0.060002947072592835, Test Loss: 0.054619609580649\n","Epoch: 6884, Train Loss: 0.05964311775462717, Test Loss: 0.05569565308504297\n","Epoch: 6885, Train Loss: 0.06006590760782983, Test Loss: 0.054681640853949214\n","Epoch: 6886, Train Loss: 0.05970485186175209, Test Loss: 0.05576010295663032\n","Epoch: 6887, Train Loss: 0.060129129802729456, Test Loss: 0.05474362421407242\n","Epoch: 6888, Train Loss: 0.0597665220893041, Test Loss: 0.05582413736655122\n","Epoch: 6889, Train Loss: 0.06019194892237399, Test Loss: 0.05480491520725179\n","Epoch: 6890, Train Loss: 0.059827483011625875, Test Loss: 0.055887099586216585\n","Epoch: 6891, Train Loss: 0.06025371174291938, Test Loss: 0.05486488390346143\n","Epoch: 6892, Train Loss: 0.059887103935203675, Test Loss: 0.05594835175649931\n","Epoch: 6893, Train Loss: 0.060313783684509306, Test Loss: 0.054922921816729586\n","Epoch: 6894, Train Loss: 0.05994477582837065, Test Loss: 0.05600728194443325\n","Epoch: 6895, Train Loss: 0.060371555834471693, Test Loss: 0.054978448669828345\n","Epoch: 6896, Train Loss: 0.059999918092076554, Test Loss: 0.05606331098240304\n","Epoch: 6897, Train Loss: 0.060426451756259526, Test Loss: 0.05503091889953705\n","Epoch: 6898, Train Loss: 0.06005198506774056, Test Loss: 0.056115898981115984\n","Epoch: 6899, Train Loss: 0.06047793397581698, Test Loss: 0.05507982779642809\n","Epoch: 6900, Train Loss: 0.06010047217604132, Test Loss: 0.056164551407017455\n","Epoch: 6901, Train Loss: 0.060525510036374075, Test Loss: 0.055124717173986086\n","Epoch: 6902, Train Loss: 0.06014492158145799, Test Loss: 0.05620882461751664\n","Epoch: 6903, Train Loss: 0.06056873801531904, Test Loss: 0.05516518046605495\n","Epoch: 6904, Train Loss: 0.060184927281650605, Test Loss: 0.05624833075354662\n","Epoch: 6905, Train Loss: 0.06060723140289234, Test Loss: 0.05520086715910052\n","Epoch: 6906, Train Loss: 0.060220139528358224, Test Loss: 0.056282741898529734\n","Epoch: 6907, Train Loss: 0.06064066325192099, Test Loss: 0.05523148647648077\n","Epoch: 6908, Train Loss: 0.06025026849728027, Test Loss: 0.05631179342551685\n","Epoch: 6909, Train Loss: 0.060668769520437094, Test Loss: 0.05525681024551808\n","Epoch: 6910, Train Loss: 0.060275087138087755, Test Loss: 0.056335286469772566\n","Epoch: 6911, Train Loss: 0.06069135154445265, Test Loss: 0.05527667489426361\n","Epoch: 6912, Train Loss: 0.06029443315186864, Test Loss: 0.05635308948183976\n","Epoch: 6913, Train Loss: 0.06070827759584884, Test Loss: 0.055290982542866726\n","Epoch: 6914, Train Loss: 0.060308210061381845, Test Loss: 0.05636513883553905\n","Epoch: 6915, Train Loss: 0.0607194834996894, Test Loss: 0.05529970117377038\n","Epoch: 6916, Train Loss: 0.06031638735883317, Test Loss: 0.056371438485705576\n","Epoch: 6917, Train Loss: 0.06072497230554519, Test Loss: 0.05530286388483146\n","Epoch: 6918, Train Loss: 0.06031899973577573, Test Loss: 0.05637205869099899\n","Epoch: 6919, Train Loss: 0.06072481302789014, Test Loss: 0.055300567249181906\n","Epoch: 6920, Train Loss: 0.06031614541944897, Test Loss: 0.05636713383707999\n","Epoch: 6921, Train Loss: 0.06071913849053137, Test Loss: 0.05529296882447856\n","Epoch: 6922, Train Loss: 0.06030798365867999, Test Loss: 0.05635685941410485\n","Epoch: 6923, Train Loss: 0.060708142328648775, Test Loss: 0.05528028387146814\n","Epoch: 6924, Train Loss: 0.06029473141971148, Test Loss: 0.056341488219206516\n","Epoch: 6925, Train Loss: 0.06069207521869877, Test Loss: 0.05526278135694454\n","Epoch: 6926, Train Loss: 0.060276659367422034, Test Loss: 0.05632132586886907\n","Epoch: 6927, Train Loss: 0.06067124042064991, Test Loss: 0.05524077932872082\n","Epoch: 6928, Train Loss: 0.060254087219890264, Test Loss: 0.05629672571743401\n","Epoch: 6929, Train Loss: 0.0606459887283292, Test Loss: 0.05521463975985388\n","Epoch: 6930, Train Loss: 0.06022737857380501, Test Loss: 0.05626808328617436\n","Epoch: 6931, Train Loss: 0.06061671293184978, Test Loss: 0.0551847629658554\n","Epoch: 6932, Train Loss: 0.06019693530465186, Test Loss: 0.05623583031227799\n","Epoch: 6933, Train Loss: 0.06058384190099379, Test Loss: 0.05515158170195052\n","Epoch: 6934, Train Loss: 0.060163191648865987, Test Loss: 0.05620042852876677\n","Epoch: 6935, Train Loss: 0.06054783440011971, Test Loss: 0.05511555504770119\n","Epoch: 6936, Train Loss: 0.0601266080753349, Test Loss: 0.05616236328499242\n","Epoch: 6937, Train Loss: 0.060509172743792194, Test Loss: 0.05507716218372798\n","Epoch: 6938, Train Loss: 0.06008766505099043, Test Loss: 0.05612213711319702\n","Epoch: 6939, Train Loss: 0.06046835639820005, Test Loss: 0.055036896160137634\n","Epoch: 6940, Train Loss: 0.060046856800056364, Test Loss: 0.056080263340067396\n","Epoch: 6941, Train Loss: 0.06042589562689787, Test Loss: 0.05499525774901878\n","Epoch: 6942, Train Loss: 0.06000468514922851, Test Loss: 0.05603725983371353\n","Epoch: 6943, Train Loss: 0.06038230527093601, Test Loss: 0.054952749464441265\n","Epoch: 6944, Train Loss: 0.059961653542108126, Test Loss: 0.05599364296653502\n","Epoch: 6945, Train Loss: 0.0603380987435118, Test Loss: 0.05490986982326809\n","Epoch: 6946, Train Loss: 0.059918261296060364, Test Loss: 0.055949921863501416\n","Epoch: 6947, Train Loss: 0.06029378230837194, Test Loss: 0.054867107909233104\n","Epoch: 6948, Train Loss: 0.05987499816380137, Test Loss: 0.055906592993951516\n","Epoch: 6949, Train Loss: 0.06024984969981041, Test Loss: 0.05482493829161041\n","Epoch: 6950, Train Loss: 0.05983233925088849, Test Loss: 0.05586413515354316\n","Epoch: 6951, Train Loss: 0.06020677713066997, Test Loss: 0.05478381633880512\n","Epoch: 6952, Train Loss: 0.05979074032929328, Test Loss: 0.05582300487186669\n","Epoch: 6953, Train Loss: 0.0601650187236719, Test Loss: 0.05474417395668774\n","Epoch: 6954, Train Loss: 0.059750633576743255, Test Loss: 0.05578363227079836\n","Epoch: 6955, Train Loss: 0.06012500239099641, Test Loss: 0.054706415771769885\n","Epoch: 6956, Train Loss: 0.05971242376180613, Test Loss: 0.05574641738916369\n","Epoch: 6957, Train Loss: 0.060087126177563295, Test Loss: 0.05467091577058305\n","Epoch: 6958, Train Loss: 0.05967648488597412, Test Loss: 0.05571172698090777\n","Epoch: 6959, Train Loss: 0.06005175507511933, Test Loss: 0.054638014399040674\n","Epoch: 6960, Train Loss: 0.059643157286443926, Test Loss: 0.05567989178683225\n","Epoch: 6961, Train Loss: 0.06001921830713243, Test Loss: 0.054608016119205335\n","Epoch: 6962, Train Loss: 0.05961274519695013, Test Loss: 0.05565120427410887\n","Epoch: 6963, Train Loss: 0.059989807078667336, Test Loss: 0.054581187415767826\n","Epoch: 6964, Train Loss: 0.05958551475891373, Test Loss: 0.0556259168331993\n","Epoch: 6965, Train Loss: 0.05996377278086446, Test Loss: 0.054557755240644046\n","Epoch: 6966, Train Loss: 0.05956169247128498, Test Loss: 0.055604240418444584\n","Epoch: 6967, Train Loss: 0.05994132563629751, Test Loss: 0.05453790588131611\n","Epoch: 6968, Train Loss: 0.05954146406469382, Test Loss: 0.055586343616314884\n","Epoch: 6969, Train Loss: 0.05992263376923525, Test Loss: 0.05452178423677284\n","Epoch: 6970, Train Loss: 0.05952497378376236, Test Loss: 0.05557235212399507\n","Epoch: 6971, Train Loss: 0.05990782268353541, Test Loss: 0.05450949348398756\n","Epoch: 6972, Train Loss: 0.0595123240605229, Test Loss: 0.05556234862046547\n","Epoch: 6973, Train Loss: 0.059896975130400605, Test Loss: 0.05450109511765123\n","Epoch: 6974, Train Loss: 0.05950357556166963, Test Loss: 0.05555637301232492\n","Epoch: 6975, Train Loss: 0.059890131348330426, Test Loss: 0.054496609346169386\n","Epoch: 6976, Train Loss: 0.05949874759266009, Test Loss: 0.05555442303714021\n","Epoch: 6977, Train Loss: 0.059887289658155554, Test Loss: 0.05449601582756078\n","Epoch: 6978, Train Loss: 0.05949781884230886, Test Loss: 0.05555645520786842\n","Epoch: 6979, Train Loss: 0.05988840739681724, Test Loss: 0.05449925472968696\n","Epoch: 6980, Train Loss: 0.059500728452296966, Test Loss: 0.05556238608275164\n","Epoch: 6981, Train Loss: 0.05989340217442371, Test Loss: 0.05450622810003531\n","Epoch: 6982, Train Loss: 0.05950737739680299, Test Loss: 0.055572093845852044\n","Epoch: 6983, Train Loss: 0.05990215343989479, Test Loss: 0.05451680153092378\n","Epoch: 6984, Train Loss: 0.059517630158091944, Test Loss: 0.05558542018392238\n","Epoch: 6985, Train Loss: 0.05991450434105063, Test Loss: 0.0545308061063715\n","Epoch: 6986, Train Loss: 0.05953131668425753, Test Loss: 0.05560217244551547\n","Epoch: 6987, Train Loss: 0.05993026386521939, Test Loss: 0.054548040616872955\n","Epoch: 6988, Train Loss: 0.0595482346152881, Test Loss: 0.055622126067984695\n","Epoch: 6989, Train Loss: 0.059949209246202076, Test Loss: 0.05456827402786671\n","Epoch: 6990, Train Loss: 0.05956815176316066, Test Loss: 0.055645027257305205\n","Epoch: 6991, Train Loss: 0.05997108862272404, Test Loss: 0.05459124818673703\n","Epoch: 6992, Train Loss: 0.059590808830696375, Test Loss: 0.05567059590437579\n","Epoch: 6993, Train Loss: 0.059995623932243856, Test Loss: 0.05461668075172411\n","Epoch: 6994, Train Loss: 0.059615922352429455, Test Loss: 0.055698528719675205\n","Epoch: 6995, Train Loss: 0.060022514022218296, Test Loss: 0.05464426832415247\n","Epoch: 6996, Train Loss: 0.0596431878387589, Test Loss: 0.0557285025658589\n","Epoch: 6997, Train Loss: 0.0600514379586435, Test Loss: 0.0546736897629674\n","Epoch: 6998, Train Loss: 0.05967228310222035, Test Loss: 0.055760177965164574\n","Epoch: 6999, Train Loss: 0.060082058508985, Test Loss: 0.0547046096577661\n","Epoch: 7000, Train Loss: 0.059702871741900206, Test Loss: 0.05579320275542429\n","Epoch: 7001, Train Loss: 0.06011402577354681, Test Loss: 0.054736681933415535\n","Epoch: 7002, Train Loss: 0.05973460675891363, Test Loss: 0.05582721586518543\n","Epoch: 7003, Train Loss: 0.06014698093603931, Test Loss: 0.05476955355610348\n","Epoch: 7004, Train Loss: 0.059767134272621196, Test Loss: 0.055861851175061816\n","Epoch: 7005, Train Loss: 0.06018056010072741, Test Loss: 0.054802868307393476\n","Epoch: 7006, Train Loss: 0.059800097303984466, Test Loss: 0.05589674142910581\n","Epoch: 7007, Train Loss: 0.06021439818021046, Test Loss: 0.05483627058971603\n","Epoch: 7008, Train Loss: 0.05983313958933152, Test Loss: 0.05593152215689391\n","Epoch: 7009, Train Loss: 0.0602481327947842, Test Loss: 0.05486940922388851\n","Epoch: 7010, Train Loss: 0.059865909384975914, Test Loss: 0.05596583556432396\n","Epoch: 7011, Train Loss: 0.060281408141631396, Test Loss: 0.05490194119686616\n","Epoch: 7012, Train Loss: 0.05989806322075936, Test Loss: 0.055999334348964744\n","Epoch: 7013, Train Loss: 0.06031387878992347, Test Loss: 0.05493353531615567\n","Epoch: 7014, Train Loss: 0.059929269558840856, Test Loss: 0.05603168539437779\n","Epoch: 7015, Train Loss: 0.060345213356476704, Test Loss: 0.054963875726303804\n","Epoch: 7016, Train Loss: 0.05995921231306072, Test Loss: 0.05606257329724123\n","Epoch: 7017, Train Loss: 0.06037509801600019, Test Loss: 0.054992665242718224\n","Epoch: 7018, Train Loss: 0.059987594184082, Test Loss: 0.05609170368146434\n","Epoch: 7019, Train Loss: 0.06040323980030985, Test Loss: 0.05501962845888817\n","Epoch: 7020, Train Loss: 0.060014139766353294, Test Loss: 0.05611880625488302\n","Epoch: 7021, Train Loss: 0.060429369642258514, Test Loss: 0.05504451458490234\n","Epoch: 7022, Train Loss: 0.0600385983847989, Test Loss: 0.0561436375665726\n","Epoch: 7023, Train Loss: 0.06045324512255118, Test Loss: 0.05506709997801025\n","Epoch: 7024, Train Loss: 0.0600607466220234, Test Loss: 0.05616598342633677\n","Epoch: 7025, Train Loss: 0.06047465288110593, Test Loss: 0.0550871903298491\n","Epoch: 7026, Train Loss: 0.06008039050072525, Test Loss: 0.05618566095244823\n","Epoch: 7027, Train Loss: 0.0604934106591066, Test Loss: 0.05510462247976356\n","Epoch: 7028, Train Loss: 0.060097367290847574, Test Loss: 0.056202520219175794\n","Epoch: 7029, Train Loss: 0.06050936894331789, Test Loss: 0.05511926582931061\n","Epoch: 7030, Train Loss: 0.06011154691668625, Test Loss: 0.056216445481876084\n","Epoch: 7031, Train Loss: 0.060522412190444165, Test Loss: 0.0551310233393912\n","Epoch: 7032, Train Loss: 0.060122832945546774, Test Loss: 0.056227355964311346\n","Epoch: 7033, Train Loss: 0.060532459616162486, Test Loss: 0.05513983209835125\n","Epoch: 7034, Train Loss: 0.06013116314646007, Test Loss: 0.05623520620018475\n","Epoch: 7035, Train Loss: 0.06053946554075769, Test Loss: 0.05514566345662419\n","Epoch: 7036, Train Loss: 0.06013650961470853, Test Loss: 0.05623998592845529\n","Epoch: 7037, Train Loss: 0.060543419290826114, Test Loss: 0.055148522730862966\n","Epoch: 7038, Train Loss: 0.06013887846529975, Test Loss: 0.056241719549586504\n","Epoch: 7039, Train Loss: 0.06054434466407893, Test Loss: 0.05514844848780085\n","Epoch: 7040, Train Loss: 0.060138309105814476, Test Loss: 0.056240465157277104\n","Epoch: 7041, Train Loss: 0.06054229897164194, Test Loss: 0.055145511425096494\n","Epoch: 7042, Train Loss: 0.06013487310607271, Test Loss: 0.05623631316721468\n","Epoch: 7043, Train Loss: 0.06053737167921939, Test Loss: 0.05513981287294418\n","Epoch: 7044, Train Loss: 0.0601286726885765, Test Loss: 0.056229384570790145\n","Epoch: 7045, Train Loss: 0.060529682674865554, Test Loss: 0.055131482946117805\n","Epoch: 7046, Train Loss: 0.06011983886956569, Test Loss: 0.05621982884734486\n","Epoch: 7047, Train Loss: 0.06051938019672607, Test Loss: 0.05512067838119499\n","Epoch: 7048, Train Loss: 0.06010852928558437, Test Loss: 0.05620782157327298\n","Epoch: 7049, Train Loss: 0.060506638458849155, Test Loss: 0.05510758009787914\n","Epoch: 7050, Train Loss: 0.06009492574461021, Test Loss: 0.05619356177004955\n","Epoch: 7051, Train Loss: 0.060491655016905815, Test Loss: 0.05509239052652416\n","Epoch: 7052, Train Loss: 0.060079231543965286, Test Loss: 0.05617726903597687\n","Epoch: 7053, Train Loss: 0.06047464791837659, Test Loss: 0.05507533074612745\n","Epoch: 7054, Train Loss: 0.06006166859936829, Test Loss: 0.05615918050808342\n","Epoch: 7055, Train Loss: 0.0604558526834018, Test Loss: 0.05505663747819337\n","Epoch: 7056, Train Loss: 0.06004247443060352, Test Loss: 0.056139547701221204\n","Epoch: 7057, Train Loss: 0.060435519163106924, Test Loss: 0.05503655998201469\n","Epoch: 7058, Train Loss: 0.06002189904940742, Test Loss: 0.056118633271023836\n","Epoch: 7059, Train Loss: 0.06041390832183663, Test Loss: 0.055015356896147886\n","Epoch: 7060, Train Loss: 0.06000020179438347, Test Loss: 0.05609670774610358\n","Epoch: 7061, Train Loss: 0.06039128898845507, Test Loss: 0.05499329306924858\n","Epoch: 7062, Train Loss: 0.05997764815612941, Test Loss: 0.056074046272782854\n","Epoch: 7063, Train Loss: 0.060367934619797774, Test Loss: 0.05497063642110729\n","Epoch: 7064, Train Loss: 0.05995450663342047, Test Loss: 0.0560509254129019\n","Epoch: 7065, Train Loss: 0.060344120116621255, Test Loss: 0.05494765487181085\n","Epoch: 7066, Train Loss: 0.05993104565836444, Test Loss: 0.05602762003195326\n","Epoch: 7067, Train Loss: 0.0603201187291169, Test Loss: 0.05492461337357712\n","Epoch: 7068, Train Loss: 0.05990753062505722, Test Loss: 0.056004400311113256\n","Epoch: 7069, Train Loss: 0.06029619908539487, Test Loss: 0.05490177107611662\n","Epoch: 7070, Train Loss: 0.05988422105256669, Test Loss: 0.05598152891279188\n","Epoch: 7071, Train Loss: 0.06027262237240879, Test Loss: 0.05487937865248103\n","Epoch: 7072, Train Loss: 0.059861367909172146, Test Loss: 0.055959258325255966\n","Epoch: 7073, Train Loss: 0.06024963969474511, Test Loss: 0.05485767580840136\n","Epoch: 7074, Train Loss: 0.059839211120828635, Test Loss: 0.0559378284078021\n","Epoch: 7075, Train Loss: 0.060227489632639464, Test Loss: 0.05483688899419782\n","Epoch: 7076, Train Loss: 0.05981797728290103, Test Loss: 0.05591746415396939\n","Epoch: 7077, Train Loss: 0.060206396016614386, Test Loss: 0.0548172293345525\n","Epoch: 7078, Train Loss: 0.059797877590424195, Test Loss: 0.05589837368648434\n","Epoch: 7079, Train Loss: 0.06018656593235306, Test Loss: 0.054798890787852536\n","Epoch: 7080, Train Loss: 0.05977910599856092, Test Loss: 0.05588074649406891\n","Epoch: 7081, Train Loss: 0.06016818796587951, Test Loss: 0.05478204854349196\n","Epoch: 7082, Train Loss: 0.05976183762161328, Test Loss: 0.05586475191699106\n","Epoch: 7083, Train Loss: 0.060151430695881976, Test Loss: 0.05476685766249529\n","Epoch: 7084, Train Loss: 0.059746227375918705, Test Loss: 0.05585053788529147\n","Epoch: 7085, Train Loss: 0.060136441437086546, Test Loss: 0.054753451964129264\n","Epoch: 7086, Train Loss: 0.05973240886926827, Test Loss: 0.05583822991102399\n","Epoch: 7087, Train Loss: 0.06012334523600753, Test Loss: 0.054741943158780206\n","Epoch: 7088, Train Loss: 0.05972049353709661, Test Loss: 0.055827930333571724\n","Epoch: 7089, Train Loss: 0.06011224411814237, Test Loss: 0.05473242022531978\n","Epoch: 7090, Train Loss: 0.05971057002363917, Test Loss: 0.055819717815137095\n","Epoch: 7091, Train Loss: 0.06010321658372782, Test Loss: 0.054724949029391054\n","Epoch: 7092, Train Loss: 0.05970270380446235, Test Loss: 0.055813647081822994\n","Epoch: 7093, Train Loss: 0.0600963173475082, Test Loss: 0.05471957217752931\n","Epoch: 7094, Train Loss: 0.05969693704525362, Test Loss: 0.055809748904284175\n","Epoch: 7095, Train Loss: 0.06009157731654096, Test Loss: 0.054716309100730505\n","Epoch: 7096, Train Loss: 0.05969328869045485, Test Loss: 0.05580803031069235\n","Epoch: 7097, Train Loss: 0.06008900379884253, Test Loss: 0.05471515635994488\n","Epoch: 7098, Train Loss: 0.0596917547741857, Test Loss: 0.05580847502367464\n","Epoch: 7099, Train Loss: 0.0600885809346032, Test Loss: 0.05471608816497703\n","Epoch: 7100, Train Loss: 0.05969230894490246, Test Loss: 0.05581104411191569\n","Epoch: 7101, Train Loss: 0.06009027034074509, Test Loss: 0.054719057097362196\n","Epoch: 7102, Train Loss: 0.05969490319432312, Test Loss: 0.05581567684620867\n","Epoch: 7103, Train Loss: 0.06009401195869866, Test Loss: 0.054723995026922954\n","Epoch: 7104, Train Loss: 0.05969946878027857, Test Loss: 0.055822291748864196\n","Epoch: 7105, Train Loss: 0.0600997250944094, Test Loss: 0.054730814210861636\n","Epoch: 7106, Train Loss: 0.059705917332296334, Test Loss: 0.05583078782451139\n","Epoch: 7107, Train Loss: 0.06010730963871999, Test Loss: 0.054739408563375135\n","Epoch: 7108, Train Loss: 0.05971414212784779, Test Loss: 0.055841045959410994\n","Epoch: 7109, Train Loss: 0.06011664745536584, Test Loss: 0.05474965508288041\n","Epoch: 7110, Train Loss: 0.059724019526288086, Test Loss: 0.05585293047545517\n","Epoch: 7111, Train Loss: 0.06012760392288602, Test Loss: 0.054761415422989686\n","Epoch: 7112, Train Loss: 0.05973541054656225, Test Loss: 0.055866290824027394\n","Epoch: 7113, Train Loss: 0.060140029615754736, Test Loss: 0.05477453759238257\n","Epoch: 7114, Train Loss: 0.05974816257375514, Test Loss: 0.05588096340384156\n","Epoch: 7115, Train Loss: 0.06015376210899164, Test Loss: 0.05478885776768023\n","Epoch: 7116, Train Loss: 0.059762111178518124, Test Loss: 0.055896773485804636\n","Epoch: 7117, Train Loss: 0.06016862788943725, Test Loss: 0.054804202202380366\n","Epoch: 7118, Train Loss: 0.05977708203235559, Test Loss: 0.05591353722685052\n","Epoch: 7119, Train Loss: 0.060184444355786744, Test Loss: 0.05482038921383689\n","Epoch: 7120, Train Loss: 0.05979289290067888, Test Loss: 0.05593106375360892\n","Epoch: 7121, Train Loss: 0.06020102188839404, Test Loss: 0.054837231229257384\n","Epoch: 7122, Train Loss: 0.05980935569452508, Test Loss: 0.05594915729576423\n","Epoch: 7123, Train Loss: 0.06021816596884789, Test Loss: 0.0548545368707301\n","Epoch: 7124, Train Loss: 0.05982627856087856, Test Loss: 0.0559676193480176\n","Epoch: 7125, Train Loss: 0.060235679328381336, Test Loss: 0.054872113058454935\n","Epoch: 7126, Train Loss: 0.05984346799069927, Test Loss: 0.05598625083879504\n","Epoch: 7127, Train Loss: 0.06025336410339956, Test Loss: 0.054889767110668175\n","Epoch: 7128, Train Loss: 0.05986073092308274, Test Loss: 0.056004854283232625\n","Epoch: 7129, Train Loss: 0.060271023975798484, Test Loss: 0.05490730881827016\n","Epoch: 7130, Train Loss: 0.05987787682350347, Test Loss: 0.05602323589761592\n","Epoch: 7131, Train Loss: 0.06028846627538336, Test Loss: 0.054924552471935444\n","Epoch: 7132, Train Loss: 0.0598947197138707, Test Loss: 0.056041207652354194\n","Epoch: 7133, Train Loss: 0.060305504021594646, Test Loss: 0.054941318819521724\n","Epoch: 7134, Train Loss: 0.05991108013217438, Test Loss: 0.05605858924079006\n","Epoch: 7135, Train Loss: 0.0603219578819578, Test Loss: 0.05495743693196831\n","Epoch: 7136, Train Loss: 0.05992678699988227, Test Loss: 0.05607520994170996\n","Epoch: 7137, Train Loss: 0.06033765802522824, Test Loss: 0.054972745956570264\n","Epoch: 7138, Train Loss: 0.059941679375957395, Test Loss: 0.05609091035433418\n","Epoch: 7139, Train Loss: 0.06035244584810344, Test Loss: 0.05498709673757412\n","Epoch: 7140, Train Loss: 0.05995560807743703, Test Loss: 0.05610554398586024\n","Epoch: 7141, Train Loss: 0.060366175555655564, Test Loss: 0.05500035328545394\n","Epoch: 7142, Train Loss: 0.059968437147939736, Test Loss: 0.056118978673281\n","Epoch: 7143, Train Loss: 0.06037871557727201, Test Loss: 0.05501239407798869\n","Epoch: 7144, Train Loss: 0.05998004515724046, Test Loss: 0.05613109782321101\n","Epoch: 7145, Train Loss: 0.060389949801885355, Test Loss: 0.05502311317835826\n","Epoch: 7146, Train Loss: 0.059990326317164035, Test Loss: 0.0561418014557781\n","Epoch: 7147, Train Loss: 0.060399778618584976, Test Loss: 0.05503242115786307\n","Epoch: 7148, Train Loss: 0.059999191401444225, Test Loss: 0.056151007041255904\n","Epoch: 7149, Train Loss: 0.06040811975130208, Test Loss: 0.05504024581352014\n","Epoch: 7150, Train Loss: 0.0600065684598541, Test Loss: 0.05615865012096536\n","Epoch: 7151, Train Loss: 0.060414908879096285, Test Loss: 0.05504653267364568\n","Epoch: 7152, Train Loss: 0.06001240331978047, Test Loss: 0.05616468470700433\n","Epoch: 7153, Train Loss: 0.060420100036587626, Test Loss: 0.05505124528752447\n","Epoch: 7154, Train Loss: 0.06001665987141038, Test Loss: 0.056169083458509995\n","Epoch: 7155, Train Loss: 0.06042366579220604, Test Loss: 0.0550543652983536\n","Epoch: 7156, Train Loss: 0.060019320135792746, Test Loss: 0.056171837635353286\n","Epoch: 7157, Train Loss: 0.060425597205109774, Test Loss: 0.055055892301734084\n","Epoch: 7158, Train Loss: 0.06002038411812681, Test Loss: 0.056172956833331435\n","Epoch: 7159, Train Loss: 0.06042590356477794, Test Loss: 0.05505584349502751\n","Epoch: 7160, Train Loss: 0.06001986945167465, Test Loss: 0.05617246850799752\n","Epoch: 7161, Train Loss: 0.0604246119203399, Test Loss: 0.05505425312580078\n","Epoch: 7162, Train Loss: 0.060017810840602266, Test Loss: 0.0561704172971761\n","Epoch: 7163, Train Loss: 0.060421766409604276, Test Loss: 0.05505117175032216\n","Epoch: 7164, Train Loss: 0.06001425931279098, Test Loss: 0.056166864154901956\n","Epoch: 7165, Train Loss: 0.06041742740042682, Test Loss: 0.0550466653155502\n","Epoch: 7166, Train Loss: 0.06000928129613868, Test Loss: 0.056161885311926105\n","Epoch: 7167, Train Loss: 0.06041167045945502, Test Loss: 0.05504081408025892\n","Epoch: 7168, Train Loss: 0.060002957534065954, Test Loss: 0.056155571080018805\n","Epoch: 7169, Train Loss: 0.06040458516536587, Test Loss: 0.05503371139280913\n","Epoch: 7170, Train Loss: 0.0599953818578046, Test Loss: 0.05614802451902998\n","Epoch: 7171, Train Loss: 0.06039627378543815, Test Loss: 0.05502546234458845\n","Epoch: 7172, Train Loss: 0.059986659834552765, Test Loss: 0.05613935998701491\n","Epoch: 7173, Train Loss: 0.06038684983564471, Test Loss: 0.05501618231927683\n","Epoch: 7174, Train Loss: 0.05997690731170829, Test Loss: 0.056129701594690454\n","Epoch: 7175, Train Loss: 0.0603764365454057, Test Loss: 0.055005995458848836\n","Epoch: 7176, Train Loss: 0.059966248878138784, Test Loss: 0.056119181586051545\n","Epoch: 7177, Train Loss: 0.0603651652487073, Test Loss: 0.05499503306760736\n","Epoch: 7178, Train Loss: 0.05995481626382404, Test Loss: 0.056107938667165624\n","Epoch: 7179, Train Loss: 0.060353173723480695, Test Loss: 0.05498343197555607\n","Epoch: 7180, Train Loss: 0.059942746699211444, Test Loss: 0.05609611630497857\n","Epoch: 7181, Train Loss: 0.06034060450095366, Test Loss: 0.05497133288209759\n","Epoch: 7182, Train Loss: 0.05993018125529764, Test Loss: 0.05608386101746198\n","Epoch: 7183, Train Loss: 0.06032760316618908, Test Loss: 0.054958878700419336\n","Epoch: 7184, Train Loss: 0.0599172631848187, Test Loss: 0.05607132067561789\n","Epoch: 7185, Train Loss: 0.06031431667021422, Test Loss: 0.05494621292201923\n","Epoch: 7186, Train Loss: 0.059904136284014496, Test Loss: 0.056058642836791295\n","Epoch: 7187, Train Loss: 0.06030089167308677, Test Loss: 0.054933478019694205\n","Epoch: 7188, Train Loss: 0.05989094329329906, Test Loss: 0.05604597312745218\n","Epoch: 7189, Train Loss: 0.06028747293596321, Test Loss: 0.0549208139059845\n","Epoch: 7190, Train Loss: 0.059877824353833146, Test Loss: 0.0560334536921552\n","Epoch: 7191, Train Loss: 0.060274201778786805, Test Loss: 0.054908356462598354\n","Epoch: 7192, Train Loss: 0.05986491553552308, Test Loss: 0.05602122172379486\n","Epoch: 7193, Train Loss: 0.060261214618634186, Test Loss: 0.054896236154749686\n","Epoch: 7194, Train Loss: 0.05985234745037364, Test Loss: 0.05600940808859818\n","Epoch: 7195, Train Loss: 0.06024864160209076, Test Loss: 0.054884576742704735\n","Epoch: 7196, Train Loss: 0.05984024396348418, Test Loss: 0.05599813605757639\n","Epoch: 7197, Train Loss: 0.06023660534331527, Test Loss: 0.05487349410114031\n","Epoch: 7198, Train Loss: 0.059828721012279304, Test Loss: 0.05598752015441028\n","Epoch: 7199, Train Loss: 0.060225219777717075, Test Loss: 0.05486309515524146\n","Epoch: 7200, Train Loss: 0.059817885542889984, Test Loss: 0.05597766512802989\n","Epoch: 7201, Train Loss: 0.06021458913946492, Test Loss: 0.054853476940806584\n","Epoch: 7202, Train Loss: 0.059807834570938835, Test Loss: 0.05596866505645544\n","Epoch: 7203, Train Loss: 0.06020480706936282, Test Loss: 0.05484472579401885\n","Epoch: 7204, Train Loss: 0.05979865437237101, Test Loss: 0.05596060258684742\n","Epoch: 7205, Train Loss: 0.06019595585801983, Test Loss: 0.05483691667499932\n","Epoch: 7206, Train Loss: 0.05979041980842965, Test Loss: 0.055953548315161326\n","Epoch: 7207, Train Loss: 0.060188105827696854, Test Loss: 0.054830112627787295\n","Epoch: 7208, Train Loss: 0.059783193787400955, Test Loss: 0.05594756030733291\n","Epoch: 7209, Train Loss: 0.060181314854755065, Test Loss: 0.054824364378006876\n","Epoch: 7210, Train Loss: 0.059777026864367484, Test Loss: 0.05594268376253996\n","Epoch: 7211, Train Loss: 0.06017562803325843, Test Loss: 0.05481971006817857\n","Epoch: 7212, Train Loss: 0.05977195697890599, Test Loss: 0.055938950817795936\n","Epoch: 7213, Train Loss: 0.060171077479002216, Test Loss: 0.05481617512941984\n","Epoch: 7214, Train Loss: 0.05976800932944931, Test Loss: 0.05593638049192282\n","Epoch: 7215, Train Loss: 0.06016768227203946, Test Loss: 0.054813772287142785\n","Epoch: 7216, Train Loss: 0.05976519638189564, Test Loss: 0.05593497876583155\n","Epoch: 7217, Train Loss: 0.06016544853466633, Test Loss: 0.05481250169730627\n","Epoch: 7218, Train Loss: 0.059763518008993946, Test Loss: 0.05593473879498607\n","Epoch: 7219, Train Loss: 0.0601643696407836, Test Loss: 0.05481235120878777\n","Epoch: 7220, Train Loss: 0.059762961756042184, Test Loss: 0.05593564124895655\n","Epoch: 7221, Train Loss: 0.06016442655158639, Test Loss: 0.05481329674652777\n","Epoch: 7222, Train Loss: 0.059763503227520234, Test Loss: 0.055937654772048095\n","Epoch: 7223, Train Loss: 0.06016558827162351, Test Loss: 0.0548153028092266\n","Epoch: 7224, Train Loss: 0.05976510658840328, Test Loss: 0.05594073655813233\n","Epoch: 7225, Train Loss: 0.060167812418414346, Test Loss: 0.05481832307457613\n","Epoch: 7226, Train Loss: 0.059767725173105475, Test Loss: 0.05594483303201031\n","Epoch: 7227, Train Loss: 0.06017104589801661, Test Loss: 0.05482230110423533\n","Epoch: 7228, Train Loss: 0.0597713021942242, Test Loss: 0.055949880628869395\n","Epoch: 7229, Train Loss: 0.06017522567817887, Test Loss: 0.05482717114005338\n","Epoch: 7230, Train Loss: 0.059775771542553105, Test Loss: 0.055955806662694535\n","Epoch: 7231, Train Loss: 0.06018027965001288, Test Loss: 0.05483285898237189\n","Epoch: 7232, Train Loss: 0.059781058669155895, Test Loss: 0.0559625302738276\n","Epoch: 7233, Train Loss: 0.0601861275684572, Test Loss: 0.05483928294061901\n","Epoch: 7234, Train Loss: 0.05978708153967436, Test Loss: 0.055969963445264795\n","Epoch: 7235, Train Loss: 0.0601926820612031, Test Loss: 0.05484635484584404\n","Epoch: 7236, Train Loss: 0.05979375165048056, Test Loss: 0.05597801207673092\n","Epoch: 7237, Train Loss: 0.06019984969520406, Test Loss: 0.0548539811143331\n","Epoch: 7238, Train Loss: 0.05980097509577347, Test Loss: 0.055986577105080794\n","Epoch: 7239, Train Loss: 0.06020753208940293, Test Loss: 0.05486206385101149\n","Epoch: 7240, Train Loss: 0.059808653674288594, Test Loss: 0.05599555565917715\n","Epoch: 7241, Train Loss: 0.06021562706190963, Test Loss: 0.05487050198097345\n","Epoch: 7242, Train Loss: 0.059816686023923364, Test Loss: 0.056004842237063625\n","Epoch: 7243, Train Loss: 0.06022402979953187, Test Loss: 0.054879192397213984\n","Epoch: 7244, Train Loss: 0.05982496877231931, Test Loss: 0.056014329893031074\n","Epoch: 7245, Train Loss: 0.060232634037338036, Test Loss: 0.0548880311124641\n","Epoch: 7246, Train Loss: 0.059833397691269, Test Loss: 0.05602391142205888\n","Epoch: 7247, Train Loss: 0.060241333235814355, Test Loss: 0.054896914402968594\n","Epoch: 7248, Train Loss: 0.059841868842758966, Test Loss: 0.05603348052911443\n","Epoch: 7249, Train Loss: 0.060250021743174335, Test Loss: 0.05490573993210847\n","Epoch: 7250, Train Loss: 0.0598502797045249, Test Loss: 0.056042932970934475\n","Epoch: 7251, Train Loss: 0.06025859593051786, Test Loss: 0.054914407841965523\n","Epoch: 7252, Train Loss: 0.05985853026319654, Test Loss: 0.056052167658181676\n","Epoch: 7253, Train Loss: 0.06026695528779906, Test Loss: 0.05492282180125054\n","Epoch: 7254, Train Loss: 0.05986652406343643, Test Loss: 0.056061087706290896\n","Epoch: 7255, Train Loss: 0.06027500346897983, Test Loss: 0.05493088999849193\n","Epoch: 7256, Train Loss: 0.059874169201959426, Test Loss: 0.056069601423885625\n","Epoch: 7257, Train Loss: 0.06028264927530488, Test Loss: 0.05493852606999729\n","Epoch: 7258, Train Loss: 0.05988137925593858, Test Loss: 0.056077623228360025\n","Epoch: 7259, Train Loss: 0.06028980756634162, Test Loss: 0.05494564995285672\n","Epoch: 7260, Train Loss: 0.05988807413606642, Test Loss: 0.056085074479081364\n","Epoch: 7261, Train Loss: 0.060296400089281124, Test Loss: 0.05495218865415388\n","Epoch: 7262, Train Loss: 0.059894180855441674, Test Loss: 0.05609188421966346\n","Epoch: 7263, Train Loss: 0.06030235621798344, Test Loss: 0.054958076928573046\n","Epoch: 7264, Train Loss: 0.0598996342064807, Test Loss: 0.05609798982188324\n","Epoch: 7265, Train Loss: 0.06030761359436418, Test Loss: 0.05496325785772904\n","Epoch: 7266, Train Loss: 0.05990437733919502, Test Loss: 0.056103337525046384\n","Epoch: 7267, Train Loss: 0.06031211866594505, Test Loss: 0.05496768332578683\n","Epoch: 7268, Train Loss: 0.05990836223542186, Test Loss: 0.05610788286593188\n","Epoch: 7269, Train Loss: 0.06031582711470553, Test Loss: 0.05497131438726105\n","Epoch: 7270, Train Loss: 0.05991155007492208, Test Loss: 0.05611159099585097\n","Epoch: 7271, Train Loss: 0.060318704173770415, Test Loss: 0.054974121524262835\n","Epoch: 7272, Train Loss: 0.05991391149064165, Test Loss: 0.05611443688279493\n","Epoch: 7273, Train Loss: 0.06032072482989869, Test Loss: 0.05497608479188367\n","Epoch: 7274, Train Loss: 0.0599154267118561, Test Loss: 0.05611640539813075\n","Epoch: 7275, Train Loss: 0.06032187391121512, Test Loss: 0.054977193851838496\n","Epoch: 7276, Train Loss: 0.05991608559535612, Test Loss: 0.056117491288767835\n","Epoch: 7277, Train Loss: 0.06032214606108269, Test Loss: 0.05497744789590633\n","Epoch: 7278, Train Loss: 0.05991588754624694, Test Loss: 0.05611769903717236\n","Epoch: 7279, Train Loss: 0.060321545600459804, Test Loss: 0.05497685546209515\n","Epoch: 7280, Train Loss: 0.059914841331326255, Test Loss: 0.05611704261299744\n","Epoch: 7281, Train Loss: 0.06032008628247092, Test Loss: 0.05497543414777413\n","Epoch: 7282, Train Loss: 0.05991296478932172, Test Loss: 0.056115545121417795\n","Epoch: 7283, Train Loss: 0.06031779094423383, Test Loss: 0.05497321022525907\n","Epoch: 7284, Train Loss: 0.059910284443511416, Test Loss: 0.05611323835447814\n","Epoch: 7285, Train Loss: 0.060314691062201956, Test Loss: 0.054970218166471435\n","Epoch: 7286, Train Loss: 0.05990683502338566, Test Loss: 0.056110162252874504\n","Epoch: 7287, Train Loss: 0.06031082621838459, Test Loss: 0.05496650008430479\n","Epoch: 7288, Train Loss: 0.05990265890301994, Test Loss: 0.05610636428654785\n","Epoch: 7289, Train Loss: 0.06030624348576528, Test Loss: 0.05496210509920582\n","Epoch: 7290, Train Loss: 0.05989780546470123, Test Loss: 0.05610189876330689\n","Epoch: 7291, Train Loss: 0.060300996742072316, Test Loss: 0.05495708864022359\n","Epoch: 7292, Train Loss: 0.05989233039709352, Test Loss: 0.0560968260753553\n","Epoch: 7293, Train Loss: 0.06029514592171165, Test Loss: 0.05495151169033741\n","Epoch: 7294, Train Loss: 0.05988629493778401, Test Loss: 0.05609121189411252\n","Epoch: 7295, Train Loss: 0.06028875621618505, Test Loss: 0.05494543998631521\n","Epoch: 7296, Train Loss: 0.05987976507048938, Test Loss: 0.0560851263240572\n","Epoch: 7297, Train Loss: 0.060281897233655755, Test Loss: 0.05493894318359897\n","Epoch: 7298, Train Loss: 0.05987281068744345, Test Loss: 0.05607864302650618\n","Epoch: 7299, Train Loss: 0.06027464212850787, Test Loss: 0.05493209399683403\n","Epoch: 7300, Train Loss: 0.059865504727605476, Test Loss: 0.05607183832426956\n","Epoch: 7301, Train Loss: 0.06026706671177483, Test Loss: 0.05492496732660867\n","Epoch: 7302, Train Loss: 0.05985792230127446, Test Loss: 0.05606479029799461\n","Epoch: 7303, Train Loss: 0.060259248553185214, Test Loss: 0.05491763938279797\n","Epoch: 7304, Train Loss: 0.059850139811519217, Test Loss: 0.05605757788475671\n","Epoch: 7305, Train Loss: 0.06025126608532415, Test Loss: 0.054910186814585545\n","Epoch: 7306, Train Loss: 0.05984223408251128, Test Loss: 0.05605027998906219\n","Epoch: 7307, Train Loss: 0.06024319772001513, Test Loss: 0.054902685856824104\n","Epoch: 7308, Train Loss: 0.059834281504431645, Test Loss: 0.05604297461593302\n","Epoch: 7309, Train Loss: 0.06023512098653949, Test Loss: 0.05489521150185361\n","Epoch: 7310, Train Loss: 0.05982635720407787, Test Loss: 0.05603573803514588\n","Epoch: 7311, Train Loss: 0.06022711170071409, Test Loss: 0.0548878367052934\n","Epoch: 7312, Train Loss: 0.0598185342496917, Test Loss: 0.05602864398502035\n","Epoch: 7313, Train Loss: 0.06021924317317712, Test Loss: 0.05488063163362746\n","Epoch: 7314, Train Loss: 0.05981088289783014, Test Loss: 0.056021762923405945\n","Epoch: 7315, Train Loss: 0.060211585464489854, Test Loss: 0.05487366296066027\n","Epoch: 7316, Train Loss: 0.05980346988935654, Test Loss: 0.056015161332724286\n","Epoch: 7317, Train Loss: 0.06020420469387567, Test Loss: 0.054866993219132756\n","Epoch: 7318, Train Loss: 0.05979635780083874, Test Loss: 0.05600890108508828\n","Epoch: 7319, Train Loss: 0.06019716240758672, Test Loss: 0.05486068021296448\n","Epoch: 7320, Train Loss: 0.05978960445681715, Test Loss: 0.05600303887266307\n","Epoch: 7321, Train Loss: 0.06019051501203906, Test Loss: 0.054854776494754724\n","Epoch: 7322, Train Loss: 0.059783262407567934, Test Loss: 0.05599762570756608\n","Epoch: 7323, Train Loss: 0.060184313275993585, Test Loss: 0.054849328912328056\n","Epoch: 7324, Train Loss: 0.05977737847614022, Test Loss: 0.05599270649473154\n","Epoch: 7325, Train Loss: 0.06017860190519495, Test Loss: 0.05484437822727069\n","Epoch: 7326, Train Loss: 0.05977199337760269, Test Loss: 0.05598831968030524\n","Epoch: 7327, Train Loss: 0.060173419192025915, Test Loss: 0.0548399588075709\n","Epoch: 7328, Train Loss: 0.059767141412602594, Test Loss: 0.05598449697728263\n","Epoch: 7329, Train Loss: 0.060168796741888124, Test Loss: 0.05483609839566624\n","Epoch: 7330, Train Loss: 0.05976285023652656, Test Loss: 0.05598126316928609\n","Epoch: 7331, Train Loss: 0.06016475927720784, Test Loss: 0.054832817952410066\n","Epoch: 7332, Train Loss: 0.05975914070476175, Test Loss: 0.05597863599256835\n","Epoch: 7333, Train Loss: 0.06016132451916237, Test Loss: 0.054830131576706645\n","Epoch: 7334, Train Loss: 0.05975602679379105, Test Loss: 0.055976626095574564\n","Epoch: 7335, Train Loss: 0.06015850314647275, Test Loss: 0.05482804649984066\n","Epoch: 7336, Train Loss: 0.0597535155971317, Test Loss: 0.055975237074650616\n","Epoch: 7337, Train Loss: 0.06015629882986833, Test Loss: 0.05482656315282294\n","Epoch: 7338, Train Loss: 0.059751607394420855, Test Loss: 0.05597446558379575\n","Epoch: 7339, Train Loss: 0.060154708340144636, Test Loss: 0.05482567530442322\n","Epoch: 7340, Train Loss: 0.05975029579130043, Test Loss: 0.05597430151570064\n","Epoch: 7341, Train Loss: 0.060153721727082214, Test Loss: 0.05482537026693693\n","Epoch: 7342, Train Loss: 0.05974956792712821, Test Loss: 0.05597472825069154\n","Epoch: 7343, Train Loss: 0.06015332256587797, Test Loss: 0.05482562916615187\n","Epoch: 7344, Train Loss: 0.05974940474696043, Test Loss: 0.055975722969627974\n","Epoch: 7345, Train Loss: 0.060153488267171096, Test Loss: 0.05482642727144506\n","Epoch: 7346, Train Loss: 0.05974978133371515, Test Loss: 0.055977257026270974\n","Epoch: 7347, Train Loss: 0.060154190446217795, Test Loss: 0.05482773438143911\n","Epoch: 7348, Train Loss: 0.05975066729592359, Test Loss: 0.05597929637415054\n","Epoch: 7349, Train Loss: 0.06015539534628391, Test Loss: 0.054829515260200024\n","Epoch: 7350, Train Loss: 0.05975202720602998, Test Loss: 0.05598180204252363\n","Epoch: 7351, Train Loss: 0.06015706431088919, Test Loss: 0.054831730118552795\n","Epoch: 7352, Train Loss: 0.05975382108379327, Test Loss: 0.055984730655623774\n","Epoch: 7353, Train Loss: 0.060159154299148296, Test Loss: 0.054834335134732966\n","Epoch: 7354, Train Loss: 0.05975600491898816, Test Loss: 0.05598803498906456\n","Epoch: 7355, Train Loss: 0.06016161843811679, Test Loss: 0.054837283008292416\n","Epoch: 7356, Train Loss: 0.059758531227300966, Test Loss: 0.055991664556977246\n","Epoch: 7357, Train Loss: 0.06016440660576835, Test Loss: 0.05484052354092173\n","Epoch: 7358, Train Loss: 0.05976134963306232, Test Loss: 0.055995566223233574\n","Epoch: 7359, Train Loss: 0.06016746603800137, Test Loss: 0.054844004237663904\n","Epoch: 7360, Train Loss: 0.05976440747227171, Test Loss: 0.055999684829945044\n","Epoch: 7361, Train Loss: 0.06017074195291277, Test Loss: 0.05484767092185636\n","Epoch: 7362, Train Loss: 0.05976765040923113, Test Loss: 0.05600396383632044\n","Epoch: 7363, Train Loss: 0.060174178185466956, Test Loss: 0.05485146835707274\n","Epoch: 7364, Train Loss: 0.059771023060042276, Test Loss: 0.056008345960930746\n","Epoch: 7365, Train Loss: 0.060177717825653614, Test Loss: 0.054855340869320784\n","Epoch: 7366, Train Loss: 0.05977446961620807, Test Loss: 0.056012773820461915\n","Epoch: 7367, Train Loss: 0.060181303853258566, Test Loss: 0.054859232962828866\n","Epoch: 7368, Train Loss: 0.05977793446165592, Test Loss: 0.05601719055813773\n","Epoch: 7369, Train Loss: 0.0601848797624713, Test Loss: 0.054863089922865836\n","Epoch: 7370, Train Loss: 0.05978136277661677, Test Loss: 0.056021540455164616\n","Epoch: 7371, Train Loss: 0.06018839016971935, Test Loss: 0.054866858399255244\n","Epoch: 7372, Train Loss: 0.059784701122009905, Test Loss: 0.05602576951880174\n","Epoch: 7373, Train Loss: 0.06019178139836938, Test Loss: 0.05487048696450022\n","Epoch: 7374, Train Loss: 0.05978789799824355, Test Loss: 0.056029826040963494\n","Epoch: 7375, Train Loss: 0.06019500203423502, Test Loss: 0.05487392664077627\n","Epoch: 7376, Train Loss: 0.05979090437268272, Test Loss: 0.05603366112165663\n","Epoch: 7377, Train Loss: 0.06019800344622195, Test Loss: 0.0548771313904525\n","Epoch: 7378, Train Loss: 0.05979367417044285, Test Loss: 0.05603722915199225\n","Epoch: 7379, Train Loss: 0.06020074026687715, Test Loss: 0.05488005856525819\n","Epoch: 7380, Train Loss: 0.05979616472362636, Test Loss: 0.05604048825203207\n","Epoch: 7381, Train Loss: 0.060203170828121724, Test Loss: 0.05488266930974002\n","Epoch: 7382, Train Loss: 0.059798337174649704, Test Loss: 0.05604340065928537\n","Epoch: 7383, Train Loss: 0.06020525754800177, Test Loss: 0.054884928915215736\n","Epoch: 7384, Train Loss: 0.059800156829871935, Test Loss: 0.05604593306429481\n","Epoch: 7385, Train Loss: 0.06020696726490722, Test Loss: 0.0548868071210563\n","Epoch: 7386, Train Loss: 0.059801593460365246, Test Loss: 0.05604805689040099\n","Epoch: 7387, Train Loss: 0.06020827151635701, Test Loss: 0.05488827836077126\n","Epoch: 7388, Train Loss: 0.059802621547310024, Test Loss: 0.05604974851546474\n","Epoch: 7389, Train Loss: 0.06020914676013312, Test Loss: 0.05488932195105718\n","Epoch: 7390, Train Loss: 0.05980322047018797, Test Loss: 0.05605098943403768\n","Epoch: 7391, Train Loss: 0.06020957453625283, Test Loss: 0.05488992222265993\n","Epoch: 7392, Train Loss: 0.05980337463663628, Test Loss: 0.05605176635920014\n","Epoch: 7393, Train Loss: 0.06020954156899276, Test Loss: 0.05489006859261354\n","Epoch: 7394, Train Loss: 0.059803073553541544, Test Loss: 0.05605207126400013\n","Epoch: 7395, Train Loss: 0.06020903980888985, Test Loss: 0.05488975557811372\n","Epoch: 7396, Train Loss: 0.059802311839647466, Test Loss: 0.056051901363164756\n","Epoch: 7397, Train Loss: 0.06020806641537476, Test Loss: 0.054888982752980375\n","Epoch: 7398, Train Loss: 0.05980108918064825, Test Loss: 0.056051259036438525\n","Epoch: 7399, Train Loss: 0.06020662368137713, Test Loss: 0.054887754648328575\n","Epoch: 7400, Train Loss: 0.059799410228405986, Test Loss: 0.056050151695585936\n","Epoch: 7401, Train Loss: 0.06020471890191712, Test Loss: 0.05488608059970313\n","Epoch: 7402, Train Loss: 0.05979728444656557, Test Loss: 0.05604859159772526\n","Epoch: 7403, Train Loss: 0.06020236418932635, Test Loss: 0.05488397454352992\n","Epoch: 7404, Train Loss: 0.059794725905438995, Test Loss: 0.05604659560825189\n","Epoch: 7405, Train Loss: 0.06019957623832977, Test Loss: 0.05488145476628283\n","Epoch: 7406, Train Loss: 0.05979175302957806, Test Loss: 0.056044184917137654\n","Epoch: 7407, Train Loss: 0.06019637604474466, Test Loss: 0.05487854361025257\n","Epoch: 7408, Train Loss: 0.05978838830193945, Test Loss: 0.056041384712869007\n","Epoch: 7409, Train Loss: 0.060192788582027756, Test Loss: 0.05487526714024137\n","Epoch: 7410, Train Loss: 0.05978465792898476, Test Loss: 0.05603822381869371\n","Epoch: 7411, Train Loss: 0.06018884244030757, Test Loss: 0.054871654775862026\n","Epoch: 7412, Train Loss: 0.059780591471412084, Test Loss: 0.056034734296177315\n","Epoch: 7413, Train Loss: 0.06018456943286714, Test Loss: 0.05486773889441919\n","Epoch: 7414, Train Loss: 0.05977622144551262, Test Loss: 0.05603095102133762\n","Epoch: 7415, Train Loss: 0.060180004175311046, Test Loss: 0.05486355440957283\n","Epoch: 7416, Train Loss: 0.059771582900368915, Test Loss: 0.056026911238813676\n","Epoch: 7417, Train Loss: 0.06017518364283674, Test Loss: 0.05485913833113248\n","Epoch: 7418, Train Loss: 0.059766712976257, Test Loss: 0.056022654099639954\n","Epoch: 7419, Train Loss: 0.060170146711145324, Test Loss: 0.05485452931141163\n","Epoch: 7420, Train Loss: 0.05976165044969527, Test Loss: 0.05601822018823751\n","Epoch: 7421, Train Loss: 0.06016493368656784, Test Loss: 0.054849767183580526\n","Epoch: 7422, Train Loss: 0.059756435270589894, Test Loss: 0.05601365104420779\n","Epoch: 7423, Train Loss: 0.06015958583095854, Test Loss: 0.054844892497399655\n","Epoch: 7424, Train Loss: 0.05975110809686956, Test Loss: 0.05600898868441779\n","Epoch: 7425, Train Loss: 0.060154144886810514, Test Loss: 0.05483994605759249\n","Epoch: 7426, Train Loss: 0.059745709831876465, Test Loss: 0.05600427513070726\n","Epoch: 7427, Train Loss: 0.06014865260789221, Test Loss: 0.05483496846994152\n","Epoch: 7428, Train Loss: 0.05974028116960544, Test Loss: 0.05599955194833379\n","Epoch: 7429, Train Loss: 0.06014315030049119, Test Loss: 0.054829999699953426\n","Epoch: 7430, Train Loss: 0.059734862152642336, Test Loss: 0.05599485980000101\n","Epoch: 7431, Train Loss: 0.060137678380082354, Test Loss: 0.054825078648664476\n","Epoch: 7432, Train Loss: 0.05972949174737732, Test Loss: 0.055990238020007625\n","Epoch: 7433, Train Loss: 0.06013227594793308, Test Loss: 0.05482024274982872\n","Epoch: 7434, Train Loss: 0.05972420744073837, Test Loss: 0.05598572421268874\n","Epoch: 7435, Train Loss: 0.06012698039179389, Test Loss: 0.054815527592376075\n","Epoch: 7436, Train Loss: 0.05971904486233322, Test Loss: 0.055981353878948106\n","Epoch: 7437, Train Loss: 0.060121827014453046, Test Loss: 0.05481096657163898\n","Epoch: 7438, Train Loss: 0.0597140374354991, Test Loss: 0.05597716007425266\n","Epoch: 7439, Train Loss: 0.060116848693510035, Test Loss: 0.05480659057243116\n","Epoch: 7440, Train Loss: 0.05970921606034133, Test Loss: 0.05597317310102904\n","Epoch: 7441, Train Loss: 0.06011207557529274, Test Loss: 0.05480242768663051\n","Epoch: 7442, Train Loss: 0.059704608831411383, Test Loss: 0.055969420237948715\n","Epoch: 7443, Train Loss: 0.060107534805394394, Test Loss: 0.054798502967479526\n","Epoch: 7444, Train Loss: 0.059700240792232816, Test Loss: 0.05596592550812671\n","Epoch: 7445, Train Loss: 0.06010325029784752, Test Loss: 0.054794838222359275\n","Epoch: 7446, Train Loss: 0.05969613372842694, Test Loss: 0.0559627094877888\n","Epoch: 7447, Train Loss: 0.060099242544485135, Test Loss: 0.05479145184534346\n","Epoch: 7448, Train Loss: 0.059692306000738, Test Loss: 0.05595978915649939\n","Epoch: 7449, Train Loss: 0.06009552846557925, Test Loss: 0.054788358690384426\n","Epoch: 7450, Train Loss: 0.05968877241880276, Test Loss: 0.05595717778957535\n","Epoch: 7451, Train Loss: 0.06009212130238412, Test Loss: 0.05478556998554225\n","Epoch: 7452, Train Loss: 0.05968554415606812, Test Loss: 0.055954884892859505\n","Epoch: 7453, Train Loss: 0.06008903055176235, Test Loss: 0.05478309328822689\n","Epoch: 7454, Train Loss: 0.05968262870581654, Test Loss: 0.05595291617958457\n","Epoch: 7455, Train Loss: 0.06008626194263135, Test Loss: 0.05478093248100428\n","Epoch: 7456, Train Loss: 0.059680029877841545, Test Loss: 0.055951273588629676\n","Epoch: 7457, Train Loss: 0.06008381745354294, Test Loss: 0.05477908780710858\n","Epoch: 7458, Train Loss: 0.05967774783490469, Test Loss: 0.05594995534306669\n","Epoch: 7459, Train Loss: 0.06008169537030581, Test Loss: 0.05477755594441942\n","Epoch: 7460, Train Loss: 0.05967577916772157, Test Loss: 0.05594895604750374\n","Epoch: 7461, Train Loss: 0.06007989038217295, Test Loss: 0.05477633011629337\n","Epoch: 7462, Train Loss: 0.059674117006854814, Test Loss: 0.055948266822369516\n","Epoch: 7463, Train Loss: 0.06007839371475499, Test Loss: 0.054775400237297094\n","Epoch: 7464, Train Loss: 0.05967275116954992, Test Loss: 0.055947875472944705\n","Epoch: 7465, Train Loss: 0.06007719329748498, Test Loss: 0.05477475309157833\n","Epoch: 7466, Train Loss: 0.05967166833923703, Test Loss: 0.055947766690636565\n","Epoch: 7467, Train Loss: 0.06007627396315105, Test Loss: 0.05477437254131691\n","Epoch: 7468, Train Loss: 0.05967085227512935, Test Loss: 0.05594792228371224\n","Epoch: 7469, Train Loss: 0.06007561767673539, Test Loss: 0.054774239762441786\n","Epoch: 7470, Train Loss: 0.05967028404909122, Test Loss: 0.055948321434455224\n","Epoch: 7471, Train Loss: 0.06007520379054705, Test Loss: 0.054774333504574384\n","Epoch: 7472, Train Loss: 0.05966994230672428, Test Loss: 0.055948940979497964\n","Epoch: 7473, Train Loss: 0.06007500932242621, Test Loss: 0.05477463037196323\n","Epoch: 7474, Train Loss: 0.05966980354942406, Test Loss: 0.05594975570989637\n","Epoch: 7475, Train Loss: 0.060075009253610576, Test Loss: 0.05477510512200955\n","Epoch: 7476, Train Loss: 0.05966984243399554, Test Loss: 0.055950738687371715\n","Epoch: 7477, Train Loss: 0.060075176842715364, Test Loss: 0.05477573097786883\n","Epoch: 7478, Train Loss: 0.059670032086300465, Test Loss: 0.055951861573031975\n","Epoch: 7479, Train Loss: 0.060075483952164076, Test Loss: 0.0547764799515118\n","Epoch: 7480, Train Loss: 0.05967034442531039, Test Loss: 0.05595309496481813\n","Epoch: 7481, Train Loss: 0.06007590138334109, Test Loss: 0.054777323173586974\n","Epoch: 7482, Train Loss: 0.05967075049389621, Test Loss: 0.05595440873988506\n","Epoch: 7483, Train Loss: 0.06007639921670154, Test Loss: 0.0547782312264079\n","Epoch: 7484, Train Loss: 0.059671220792669226, Test Loss: 0.05595577239813538\n","Epoch: 7485, Train Loss: 0.06007694715307977, Test Loss: 0.054779174476409384\n","Epoch: 7486, Train Loss: 0.05967172561320925, Test Loss: 0.05595715540316905\n","Epoch: 7487, Train Loss: 0.06007751485248432, Test Loss: 0.05478012340248275\n","Epoch: 7488, Train Loss: 0.05967223536708246, Test Loss: 0.055958527516995114\n","Epoch: 7489, Train Loss: 0.06007807226674621, Test Loss: 0.05478104891669824\n","Epoch: 7490, Train Loss: 0.0596727209071515, Test Loss: 0.05595985912497663\n","Epoch: 7491, Train Loss: 0.06007858996251446, Test Loss: 0.054781922674052116\n","Epoch: 7492, Train Loss: 0.05967315383780955, Test Loss: 0.0559611215476351\n","Epoch: 7493, Train Loss: 0.06007903943124222, Test Loss: 0.0547827173680519\n","Epoch: 7494, Train Loss: 0.059673506810949205, Test Loss: 0.05596228733613617\n","Epoch: 7495, Train Loss: 0.060079393383004616, Test Loss: 0.0547834070091528\n","Epoch: 7496, Train Loss: 0.0596737538046754, Test Loss: 0.0559633305485075\n","Epoch: 7497, Train Loss: 0.06007962602121326, Test Loss: 0.05478396718329376\n","Epoch: 7498, Train Loss: 0.05967387038201112, Test Loss: 0.055964227003893495\n","Epoch: 7499, Train Loss: 0.060079713295546205, Test Loss: 0.05478437528804248\n","Epoch: 7500, Train Loss: 0.05967383392710366, Test Loss: 0.05596495451244138\n","Epoch: 7501, Train Loss: 0.060079633130698924, Test Loss: 0.05478461074415049\n","Epoch: 7502, Train Loss: 0.05967362385673529, Test Loss: 0.055965493078719404\n","Epoch: 7503, Train Loss: 0.06007936562886533, Test Loss: 0.05478465518062597\n","Epoch: 7504, Train Loss: 0.0596732218052476, Test Loss: 0.05596582507690448\n","Epoch: 7505, Train Loss: 0.06007889324419341, Test Loss: 0.05478449259176192\n","Epoch: 7506, Train Loss: 0.05967261178132159, Test Loss: 0.055965935396313\n","Epoch: 7507, Train Loss: 0.06007820092779263, Test Loss: 0.05478410946490057\n","Epoch: 7508, Train Loss: 0.059671780295398276, Test Loss: 0.055965811556222295\n","Epoch: 7509, Train Loss: 0.06007727624224227, Test Loss: 0.054783494878065456\n","Epoch: 7510, Train Loss: 0.05967071645687825, Test Loss: 0.055965443789284065\n","Epoch: 7511, Train Loss: 0.06007610944490182, Test Loss: 0.05478264056695475\n","Epoch: 7512, Train Loss: 0.05966941204059927, Test Loss: 0.05596482509321222\n","Epoch: 7513, Train Loss: 0.06007469353970198, Test Loss: 0.05478154096114171\n","Epoch: 7514, Train Loss: 0.05966786152244686, Test Loss: 0.05596395125078639\n","Epoch: 7515, Train Loss: 0.060073024297453016, Test Loss: 0.05478019318968905\n","Epoch: 7516, Train Loss: 0.05966606208431104, Test Loss: 0.05596282081858112\n","Epoch: 7517, Train Loss: 0.06007110024507223, Test Loss: 0.05477859705672801\n","Epoch: 7518, Train Loss: 0.05966401358895081, Test Loss: 0.05596143508518014\n","Epoch: 7519, Train Loss: 0.060068922624481345, Test Loss: 0.05477675498788485\n","Epoch: 7520, Train Loss: 0.059661718525657166, Test Loss: 0.055959797999966907\n","Epoch: 7521, Train Loss: 0.0600664953222527, Test Loss: 0.05477467194876013\n","Epoch: 7522, Train Loss: 0.05965918192792995, Test Loss: 0.05595791607390325\n","Epoch: 7523, Train Loss: 0.06006382477140395, Test Loss: 0.05477235533695641\n","Epoch: 7524, Train Loss: 0.059656411264674206, Test Loss: 0.05595579825399787\n","Epoch: 7525, Train Loss: 0.06006091982702778, Test Loss: 0.0547698148494266\n","Epoch: 7526, Train Loss: 0.05965341630669792, Test Loss: 0.05595345577343333\n","Epoch: 7527, Train Loss: 0.060057791617709785, Test Loss: 0.05476706232715865\n","Epoch: 7528, Train Loss: 0.05965020897053712, Test Loss: 0.05595090197955531\n","Epoch: 7529, Train Loss: 0.06005445337492131, Test Loss: 0.054764111579428607\n","Epoch: 7530, Train Loss: 0.059646803141849274, Test Loss: 0.055948152142135846\n","Epoch: 7531, Train Loss: 0.06005092024278062, Test Loss: 0.0547609781900378\n","Epoch: 7532, Train Loss: 0.0596432144808013, Test Loss: 0.055945223244488994\n","Epoch: 7533, Train Loss: 0.06004720907074332, Test Loss: 0.05475767930809919\n","Epoch: 7534, Train Loss: 0.05963946021202514, Test Loss: 0.05594213376015559\n","Epoch: 7535, Train Loss: 0.060043338191918455, Test Loss: 0.05475423342605714\n","Epoch: 7536, Train Loss: 0.05963555890183476, Test Loss: 0.055938903417976873\n","Epoch: 7537, Train Loss: 0.06003932718981198, Test Loss: 0.05475066014770363\n","Epoch: 7538, Train Loss: 0.05963153022547473, Test Loss: 0.05593555295843429\n","Epoch: 7539, Train Loss: 0.060035196656355, Test Loss: 0.05474697994900122\n","Epoch: 7540, Train Loss: 0.05962739472721911, Test Loss: 0.05593210388416902\n","Epoch: 7541, Train Loss: 0.060030967944111766, Test Loss: 0.05474321393453901\n","Epoch: 7542, Train Loss: 0.05962317357615296, Test Loss: 0.05592857820758503\n","Epoch: 7543, Train Loss: 0.060026662915552566, Test Loss: 0.0547393835924206\n","Epoch: 7544, Train Loss: 0.05961888832044176, Test Loss: 0.055924998198399745\n","Epoch: 7545, Train Loss: 0.06002230369223745, Test Loss: 0.05473551055034031\n","Epoch: 7546, Train Loss: 0.05961456064285023, Test Loss: 0.05592138613393926\n","Epoch: 7547, Train Loss: 0.06001791240669096, Test Loss: 0.054731616335511456\n","Epoch: 7548, Train Loss: 0.05961021212017928, Test Loss: 0.055917764054868126\n","Epoch: 7549, Train Loss: 0.0600135109596416, Test Loss: 0.05472772214100786\n","Epoch: 7550, Train Loss: 0.05960586398918542, Test Loss: 0.05591415352892124\n","Epoch: 7551, Train Loss: 0.060009120785178297, Test Loss: 0.05472384860093972\n","Epoch: 7552, Train Loss: 0.05960153692140689, Test Loss: 0.055910575425046856\n","Epoch: 7553, Train Loss: 0.060004762626219704, Test Loss: 0.054720015576730036\n","Epoch: 7554, Train Loss: 0.05959725080916474, Test Loss: 0.05590704970019817\n","Epoch: 7555, Train Loss: 0.06000045632252065, Test Loss: 0.05471624195657019\n","Epoch: 7556, Train Loss: 0.059593024564819054, Test Loss: 0.05590359520081092\n","Epoch: 7557, Train Loss: 0.059996220613243084, Test Loss: 0.05471254546994802\n","Epoch: 7558, Train Loss: 0.05958887593517304, Test Loss: 0.05590022948079629\n","Epoch: 7559, Train Loss: 0.059992072955910726, Test Loss: 0.0547089425189184\n","Epoch: 7560, Train Loss: 0.05958482133269645, Test Loss: 0.055896968637652374\n","Epoch: 7561, Train Loss: 0.05998802936334374, Test Loss: 0.05470544802757158\n","Epoch: 7562, Train Loss: 0.05958087568502025, Test Loss: 0.05589382716805825\n","Epoch: 7563, Train Loss: 0.05998410425993064, Test Loss: 0.05470207531091426\n","Epoch: 7564, Train Loss: 0.05957705230391774, Test Loss: 0.05589081784407391\n","Epoch: 7565, Train Loss: 0.059980310358357276, Test Loss: 0.05469883596414611\n","Epoch: 7566, Train Loss: 0.05957336277475072, Test Loss: 0.05588795161081885\n","Epoch: 7567, Train Loss: 0.05997665855766218, Test Loss: 0.05469573977306619\n","Epoch: 7568, Train Loss: 0.05956981686711319, Test Loss: 0.055885237506252385\n","Epoch: 7569, Train Loss: 0.05997315786324129, Test Loss: 0.054692794646102785\n","Epoch: 7570, Train Loss: 0.059566422467161026, Test Loss: 0.05588268260342247\n","Epoch: 7571, Train Loss: 0.05996981532916883, Test Loss: 0.054690006568216515\n","Epoch: 7572, Train Loss: 0.05956318553187405, Test Loss: 0.05588029197531093\n","Epoch: 7573, Train Loss: 0.059966636022965335, Test Loss: 0.05468737957668966\n","Epoch: 7574, Train Loss: 0.05956011006525841, Test Loss: 0.05587806868215215\n","Epoch: 7575, Train Loss: 0.059963623012693236, Test Loss: 0.05468491575858034\n","Epoch: 7576, Train Loss: 0.05955719811626186, Test Loss: 0.05587601378086823\n","Epoch: 7577, Train Loss: 0.05996077737602878, Test Loss: 0.054682615269394705\n","Epoch: 7578, Train Loss: 0.059554449797949965, Test Loss: 0.05587412635604224\n","Epoch: 7579, Train Loss: 0.05995809823073881, Test Loss: 0.05468047637232126\n","Epoch: 7580, Train Loss: 0.05955186332728138, Test Loss: 0.055872403571630445\n","Epoch: 7581, Train Loss: 0.05995558278577125, Test Loss: 0.05467849549715969\n","Epoch: 7582, Train Loss: 0.05954943508460709, Test Loss: 0.05587084074241525\n","Epoch: 7583, Train Loss: 0.059953226411970234, Test Loss: 0.05467666731789752\n","Epoch: 7584, Train Loss: 0.05954715969184186, Test Loss: 0.05586943142401466\n","Epoch: 7585, Train Loss: 0.05995102273124249, Test Loss: 0.05467498484770123\n","Epoch: 7586, Train Loss: 0.05954503010806635, Test Loss: 0.0558681675200887\n","Epoch: 7587, Train Loss: 0.059948963722826334, Test Loss: 0.05467343954994367\n","Epoch: 7588, Train Loss: 0.0595430377411758, Test Loss: 0.05586703940523553\n","Epoch: 7589, Train Loss: 0.05994703984516848, Test Loss: 0.05467202146373466\n","Epoch: 7590, Train Loss: 0.05954117257403528, Test Loss: 0.055866036061927575\n","Epoch: 7591, Train Loss: 0.059945240171771164, Test Loss: 0.054670719342308965\n","Epoch: 7592, Train Loss: 0.05953942330348815, Test Loss: 0.055865145229728456\n","Epoch: 7593, Train Loss: 0.05994355253926425, Test Loss: 0.05466952080251816\n","Epoch: 7594, Train Loss: 0.059537777490458806, Test Loss: 0.055864353564930413\n","Epoch: 7595, Train Loss: 0.05994196370585526, Test Loss: 0.05466841248358699\n","Epoch: 7596, Train Loss: 0.059536221719303034, Test Loss: 0.05586364680868113\n","Epoch: 7597, Train Loss: 0.05994045951824016, Test Loss: 0.05466738021323605\n","Epoch: 7598, Train Loss: 0.05953474176450196, Test Loss: 0.05586300996161192\n","Epoch: 7599, Train Loss: 0.05993902508500042, Test Loss: 0.0546664091792249\n","Epoch: 7600, Train Loss: 0.05953332276274785, Test Loss: 0.05586242746294774\n","Epoch: 7601, Train Loss: 0.059937644954480854, Test Loss: 0.054665484104353596\n","Epoch: 7602, Train Loss: 0.059531949388454394, Test Loss: 0.055861883372072235\n","Epoch: 7603, Train Loss: 0.05993630329513489, Test Loss: 0.05466458942295386\n","Epoch: 7604, Train Loss: 0.05953060603071805, Test Loss: 0.05586136155052523\n","Epoch: 7605, Train Loss: 0.05993498407632746, Test Loss: 0.05466370945692534\n","Epoch: 7606, Train Loss: 0.05952927696978071, Test Loss: 0.055860845842451\n","Epoch: 7607, Train Loss: 0.059933671247626635, Test Loss: 0.0546628285894129\n","Epoch: 7608, Train Loss: 0.05952794655108653, Test Loss: 0.055860320251558876\n","Epoch: 7609, Train Loss: 0.05993234891465751, Test Loss: 0.054661931434275716\n","Epoch: 7610, Train Loss: 0.05952659935507998, Test Loss: 0.05585976911273953\n","Epoch: 7611, Train Loss: 0.05993100150967241, Test Loss: 0.054661002999586615\n","Epoch: 7612, Train Loss: 0.05952522036098085, Test Loss: 0.055859177256565765\n","Epoch: 7613, Train Loss: 0.059929613955076996, Test Loss: 0.05466002884348916\n","Epoch: 7614, Train Loss: 0.05952379510286221, Test Loss: 0.05585853016502012\n","Epoch: 7615, Train Loss: 0.05992817181826316, Test Loss: 0.0546589952208581\n","Epoch: 7616, Train Loss: 0.05952230981647449, Test Loss: 0.05585781411691413\n","Epoch: 7617, Train Loss: 0.05992666145622265, Test Loss: 0.05465788921933659\n","Epoch: 7618, Train Loss: 0.05952075157538973, Test Loss: 0.05585701632160896\n","Epoch: 7619, Train Loss: 0.05992507014855701, Test Loss: 0.05465669888346717\n","Epoch: 7620, Train Loss: 0.059519108415181717, Test Loss: 0.05585612503980078\n","Epoch: 7621, Train Loss: 0.05992338621765421, Test Loss: 0.054655413325787744\n","Epoch: 7622, Train Loss: 0.059517369444514384, Test Loss: 0.05585512969029566\n","Epoch: 7623, Train Loss: 0.05992159913496062, Test Loss: 0.05465402282392898\n","Epoch: 7624, Train Loss: 0.0595155249421765, Test Loss: 0.05585402094188183\n","Epoch: 7625, Train Loss: 0.05991969961245991, Test Loss: 0.054652518902923455\n","Epoch: 7626, Train Loss: 0.05951356643927364, Test Loss: 0.05585279078958095\n","Epoch: 7627, Train Loss: 0.05991767967864208, Test Loss: 0.05465089440211559\n","Epoch: 7628, Train Loss: 0.059511486785970215, Test Loss: 0.055851432614754476\n","Epoch: 7629, Train Loss: 0.059915532738440594, Test Loss: 0.05464914352624253\n","Epoch: 7630, Train Loss: 0.05950928020235361, Test Loss: 0.05584994122872492\n","Epoch: 7631, Train Loss: 0.05991325361679551, Test Loss: 0.05464726188044799\n","Epoch: 7632, Train Loss: 0.05950694231318632, Test Loss: 0.05584831289977084\n","Epoch: 7633, Train Loss: 0.05991083858570136, Test Loss: 0.05464524648916575\n","Epoch: 7634, Train Loss: 0.05950447016648663, Test Loss: 0.05584654536353341\n","Epoch: 7635, Train Loss: 0.059908285374773246, Test Loss: 0.054643095798998495\n","Epoch: 7636, Train Loss: 0.059501862236067654, Test Loss: 0.055844637817065175\n","Epoch: 7637, Train Loss: 0.05990559316555898, Test Loss: 0.05464080966589189\n","Epoch: 7638, Train Loss: 0.05949911840833886, Test Loss: 0.05584259089692568\n","Epoch: 7639, Train Loss: 0.05990276256999643, Test Loss: 0.05463838932707239\n","Epoch: 7640, Train Loss: 0.0594962399538441, Test Loss: 0.055840406641901576\n","Epoch: 7641, Train Loss: 0.05989979559358737, Test Loss: 0.054635837358381734\n","Epoch: 7642, Train Loss: 0.059493229484173384, Test Loss: 0.055838088441089014\n","Epoch: 7643, Train Loss: 0.05989669558401924, Test Loss: 0.054633157617788274\n","Epoch: 7644, Train Loss: 0.059490090895034084, Test Loss: 0.05583564096822177\n","Epoch: 7645, Train Loss: 0.05989346716611046, Test Loss: 0.05463035517599515\n","Epoch: 7646, Train Loss: 0.05948682929640664, Test Loss: 0.055833070103270446\n","Epoch: 7647, Train Loss: 0.05989011616409596, Test Loss: 0.054627436235194025\n","Epoch: 7648, Train Loss: 0.059483450930838615, Test Loss: 0.055830382842451565\n","Epoch: 7649, Train Loss: 0.059886649512382914, Test Loss: 0.054624408037112486\n","Epoch: 7650, Train Loss: 0.05947996308103081, Test Loss: 0.055827587197892554\n","Epoch: 7651, Train Loss: 0.05988307515601329, Test Loss: 0.05462127876161002\n","Epoch: 7652, Train Loss: 0.059476373967974824, Test Loss: 0.05582469208829037\n","Epoch: 7653, Train Loss: 0.059879401942160924, Test Loss: 0.05461805741715068\n","Epoch: 7654, Train Loss: 0.059472692640975384, Test Loss: 0.05582170722196773\n","Epoch: 7655, Train Loss: 0.059875639504057374, Test Loss: 0.0546147537245351\n","Epoch: 7656, Train Loss: 0.05946892886094496, Test Loss: 0.05581864297378415\n","Epoch: 7657, Train Loss: 0.059871798138793106, Test Loss: 0.05461137799532918\n","Epoch: 7658, Train Loss: 0.05946509297841217, Test Loss: 0.05581551025739784\n","Epoch: 7659, Train Loss: 0.059867888680480374, Test Loss: 0.05460794100644533\n","Epoch: 7660, Train Loss: 0.05946119580770399, Test Loss: 0.05581232039438953\n","Epoch: 7661, Train Loss: 0.059863922370278246, Test Loss: 0.05460445387234368\n","Epoch: 7662, Train Loss: 0.059457248498773395, Test Loss: 0.05580908498175667\n","Epoch: 7663, Train Loss: 0.05985991072477893, Test Loss: 0.05460092791631326\n","Epoch: 7664, Train Loss: 0.059453262408135456, Test Loss: 0.05580581575927515\n","Epoch: 7665, Train Loss: 0.05985586540424228, Test Loss: 0.054597374542268005\n","Epoch: 7666, Train Loss: 0.05944924897035001, Test Loss: 0.055802524478186136\n","Epoch: 7667, Train Loss: 0.059851798082127816, Test Loss: 0.05459380510845518\n","Epoch: 7668, Train Loss: 0.059445219571451276, Test Loss: 0.05579922277262187\n","Epoch: 7669, Train Loss: 0.0598477203173286, Test Loss: 0.054590230804417154\n","Epoch: 7670, Train Loss: 0.05944118542566722, Test Loss: 0.05579592203511568\n","Epoch: 7671, Train Loss: 0.05984364343044475, Test Loss: 0.054586662532483246\n","Epoch: 7672, Train Loss: 0.05943715745670773, Test Loss: 0.05579263329747015\n","Epoch: 7673, Train Loss: 0.059839578385362996, Test Loss: 0.05458311079498478\n","Epoch: 7674, Train Loss: 0.05943314618481546, Test Loss: 0.055789367118163326\n","Epoch: 7675, Train Loss: 0.059835535677315435, Test Loss: 0.0545795855882983\n","Epoch: 7676, Train Loss: 0.05942916162068574, Test Loss: 0.055786133477379256\n","Epoch: 7677, Train Loss: 0.05983152522849779, Test Loss: 0.054576096304721704\n","Epoch: 7678, Train Loss: 0.05942521316726037, Test Loss: 0.05578294168063298\n","Epoch: 7679, Train Loss: 0.05982755629221292, Test Loss: 0.05457265164307525\n","Epoch: 7680, Train Loss: 0.05942130953028758, Test Loss: 0.05577980027185196\n","Epoch: 7681, Train Loss: 0.059823637366396676, Test Loss: 0.05456925952880983\n","Epoch: 7682, Train Loss: 0.05941745863842975, Test Loss: 0.05577671695664745\n","Epoch: 7683, Train Loss: 0.05981977611725611, Test Loss: 0.05456592704427552\n","Epoch: 7684, Train Loss: 0.05941366757357129, Test Loss: 0.05577369853638205\n","Epoch: 7685, Train Loss: 0.059815979313625374, Test Loss: 0.05456266036968614\n","Epoch: 7686, Train Loss: 0.05940994251186142, Test Loss: 0.055770750853512276\n","Epoch: 7687, Train Loss: 0.059812252772514785, Test Loss: 0.054559464735181716\n","Epoch: 7688, Train Loss: 0.059406288675891504, Test Loss: 0.05576787874854849\n","Epoch: 7689, Train Loss: 0.059808601316196405, Test Loss: 0.05455634438426376\n","Epoch: 7690, Train Loss: 0.05940271029828046, Test Loss: 0.05576508602884238\n","Epoch: 7691, Train Loss: 0.059805028741035826, Test Loss: 0.05455330254875252\n","Epoch: 7692, Train Loss: 0.05939921059681456, Test Loss: 0.05576237544928457\n","Epoch: 7693, Train Loss: 0.059801537798153996, Test Loss: 0.05455034143528008\n","Epoch: 7694, Train Loss: 0.0593957917611536, Test Loss: 0.05575974870485439\n","Epoch: 7695, Train Loss: 0.05979813018586383, Test Loss: 0.05454746222321835\n","Epoch: 7696, Train Loss: 0.059392454950998846, Test Loss: 0.05575720643485003\n","Epoch: 7697, Train Loss: 0.05979480655371201, Test Loss: 0.05454466507381354\n","Epoch: 7698, Train Loss: 0.05938920030549194, Test Loss: 0.05575474823849812\n","Epoch: 7699, Train Loss: 0.05979156651782885, Test Loss: 0.05454194915018646\n","Epoch: 7700, Train Loss: 0.05938602696350013, Test Loss: 0.05575237270152782\n","Epoch: 7701, Train Loss: 0.05978840868717571, Test Loss: 0.05453931264774856\n","Epoch: 7702, Train Loss: 0.05938293309433503, Test Loss: 0.05575007743318811\n","Epoch: 7703, Train Loss: 0.059785330700173604, Test Loss: 0.054536752834481714\n","Epoch: 7704, Train Loss: 0.05937991593834859, Test Loss: 0.05574785911308236\n","Epoch: 7705, Train Loss: 0.05978232927109238, Test Loss: 0.05453426610043471\n","Epoch: 7706, Train Loss: 0.05937697185675627, Test Loss: 0.05574571354710687\n","Epoch: 7707, Train Loss: 0.05977940024549373, Test Loss: 0.054531848015705896\n","Epoch: 7708, Train Loss: 0.059374096389952565, Test Loss: 0.05574363573169471\n","Epoch: 7709, Train Loss: 0.059776538663934786, Test Loss: 0.054529493396106264\n","Epoch: 7710, Train Loss: 0.0593712843235101, Test Loss: 0.05574161992549441\n","Epoch: 7711, Train Loss: 0.059773738833069814, Test Loss: 0.054527196375628174\n","Epoch: 7712, Train Loss: 0.059368529760983085, Test Loss: 0.05573965972755024\n","Epoch: 7713, Train Loss: 0.05977099440322303, Test Loss: 0.05452495048479535\n","Epoch: 7714, Train Loss: 0.05936582620258788, Test Loss: 0.05573774816100465\n","Epoch: 7715, Train Loss: 0.05976829845146057, Test Loss: 0.05452274873391982\n","Epoch: 7716, Train Loss: 0.059363166628782466, Test Loss: 0.055735877761296584\n","Epoch: 7717, Train Loss: 0.05976564356914258, Test Loss: 0.05452058370026421\n","Epoch: 7718, Train Loss: 0.059360543587739914, Test Loss: 0.05573404066781085\n","Epoch: 7719, Train Loss: 0.05976302195291835, Test Loss: 0.05451844761808021\n","Epoch: 7720, Train Loss: 0.05935794928568364, Test Loss: 0.05573222871790913\n","Epoch: 7721, Train Loss: 0.05976042549810138, Test Loss: 0.05451633247049126\n","Epoch: 7722, Train Loss: 0.0593553756790497, Test Loss: 0.05573043354227515\n","Epoch: 7723, Train Loss: 0.05975784589336571, Test Loss: 0.05451423008217688\n","Epoch: 7724, Train Loss: 0.059352814567430516, Test Loss: 0.055728646660509626\n","Epoch: 7725, Train Loss: 0.059755274715704285, Test Loss: 0.05451213221184328\n","Epoch: 7726, Train Loss: 0.059350257686282305, Test Loss: 0.05572685957593383\n","Epoch: 7727, Train Loss: 0.05975270352461581, Test Loss: 0.05451003064347347\n","Epoch: 7728, Train Loss: 0.0593476967983877, Test Loss: 0.055725063868583495\n","Epoch: 7729, Train Loss: 0.05975012395450848, Test Loss: 0.05450791727539136\n","Epoch: 7730, Train Loss: 0.05934512378310566, Test Loss: 0.05572325128542019\n","Epoch: 7731, Train Loss: 0.05974752780435253, Test Loss: 0.05450578420621453\n","Epoch: 7732, Train Loss: 0.05934253072248225, Test Loss: 0.05572141382683445\n","Epoch: 7733, Train Loss: 0.0597449071236623, Test Loss: 0.054503623816824553\n","Epoch: 7734, Train Loss: 0.05933990998334986, Test Loss: 0.055719543828574344\n","Epoch: 7735, Train Loss: 0.059742254293945765, Test Loss: 0.05450142884754074\n","Epoch: 7736, Train Loss: 0.059337254294599816, Test Loss: 0.05571763403829892\n","Epoch: 7737, Train Loss: 0.059739562104825776, Test Loss: 0.054499192469756194\n","Epoch: 7738, Train Loss: 0.059334556818886906, Test Loss: 0.0557156776860321\n","Epoch: 7739, Train Loss: 0.05973682382411195, Test Loss: 0.054496908351365496\n","Epoch: 7740, Train Loss: 0.05933181121809527, Test Loss: 0.055713668547874635\n","Epoch: 7741, Train Loss: 0.05973403326118441, Test Loss: 0.054494570715401924\n","Epoch: 7742, Train Loss: 0.059329011711982974, Test Loss: 0.055711601002415595\n","Epoch: 7743, Train Loss: 0.05973118482313256, Test Loss: 0.05449217439137801\n","Epoch: 7744, Train Loss: 0.059326153129500424, Test Loss: 0.0557094700793782\n","Epoch: 7745, Train Loss: 0.059728273563186475, Test Loss: 0.054489714858924405\n","Epoch: 7746, Train Loss: 0.059323230952378264, Test Loss: 0.055707271500129614\n","Epoch: 7747, Train Loss: 0.059725295221070965, Test Loss: 0.054487188283406617\n","Epoch: 7748, Train Loss: 0.05932024135066484, Test Loss: 0.05570500170978159\n","Epoch: 7749, Train Loss: 0.05972224625500964, Test Loss: 0.054484591543299196\n","Epoch: 7750, Train Loss: 0.05931718120999535, Test Loss: 0.0557026579007042\n","Epoch: 7751, Train Loss: 0.05971912386520182, Test Loss: 0.054481922249187825\n","Epoch: 7752, Train Loss: 0.05931404815046368, Test Loss: 0.05570023802737637\n","Epoch: 7753, Train Loss: 0.05971592600869407, Test Loss: 0.054479178754370625\n","Epoch: 7754, Train Loss: 0.05931084053707107, Test Loss: 0.05569774081259521\n","Epoch: 7755, Train Loss: 0.059712651405667665, Test Loss: 0.05447636015711571\n","Epoch: 7756, Train Loss: 0.059307557481810276, Test Loss: 0.055695165745154825\n","Epoch: 7757, Train Loss: 0.059709299537250536, Test Loss: 0.054473466294731335\n","Epoch: 7758, Train Loss: 0.0593041988375441, Test Loss: 0.05569251306920395\n","Epoch: 7759, Train Loss: 0.059705870635060096, Test Loss: 0.05447049772968429\n","Epoch: 7760, Train Loss: 0.0593007651839163, Test Loss: 0.05568978376557549\n","Epoch: 7761, Train Loss: 0.05970236566276696, Test Loss: 0.054467455728089434\n","Epoch: 7762, Train Loss: 0.05929725780562047, Test Loss: 0.05568697952546463\n","Epoch: 7763, Train Loss: 0.05969878629005325, Test Loss: 0.0544643422309719\n","Epoch: 7764, Train Loss: 0.05929367866343101, Test Loss: 0.05568410271690948\n","Epoch: 7765, Train Loss: 0.05969513485941441, Test Loss: 0.054461159818769304\n","Epoch: 7766, Train Loss: 0.059290030358466514, Test Loss: 0.05568115634459327\n","Epoch: 7767, Train Loss: 0.05969141434632009, Test Loss: 0.054457911669610934\n","Epoch: 7768, Train Loss: 0.05928631609022463, Test Loss: 0.05567814400355843\n","Epoch: 7769, Train Loss: 0.05968762831331905, Test Loss: 0.05445460151196669\n","Epoch: 7770, Train Loss: 0.05928253960898488, Test Loss: 0.055675069827468254\n","Epoch: 7771, Train Loss: 0.05968378085871944, Test Loss: 0.05445123357230429\n","Epoch: 7772, Train Loss: 0.05927870516321975, Test Loss: 0.05567193843210202\n","Epoch: 7773, Train Loss: 0.059679876560524384, Test Loss: 0.05444781251843962\n","Epoch: 7774, Train Loss: 0.0592748174427024, Test Loss: 0.055668754854808024\n","Epoch: 7775, Train Loss: 0.05967592041634308, Test Loss: 0.0544443433992938\n","Epoch: 7776, Train Loss: 0.05927088151802626, Test Loss: 0.055665524490664885\n","Epoch: 7777, Train Loss: 0.05967191778002139, Test Loss: 0.0544408315817946\n","Epoch: 7778, Train Loss: 0.05926690277727745, Test Loss: 0.05566225302612149\n","Epoch: 7779, Train Loss: 0.05966787429575767, Test Loss: 0.054437282685675895\n","Epoch: 7780, Train Loss: 0.05926288686061533, Test Loss: 0.055658946370897626\n","Epoch: 7781, Train Loss: 0.0596637958304802, Test Loss: 0.05443370251693381\n","Epoch: 7782, Train Loss: 0.05925883959352218, Test Loss: 0.0556556105889283\n","Epoch: 7783, Train Loss: 0.05965968840526393, Test Loss: 0.054430097000696565\n","Epoch: 7784, Train Loss: 0.059254766919481346, Test Loss: 0.05565225182912487\n","Epoch: 7785, Train Loss: 0.05965555812655519, Test Loss: 0.05442647211425528\n","Epoch: 7786, Train Loss: 0.059250674832832244, Test Loss: 0.0556488762567181\n","Epoch: 7787, Train Loss: 0.05965141111796366, Test Loss: 0.05442283382098013\n","Epoch: 7788, Train Loss: 0.05924656931252828, Test Loss: 0.055645489985910256\n","Epoch: 7789, Train Loss: 0.05964725345334521, Test Loss: 0.05441918800582569\n","Epoch: 7790, Train Loss: 0.05924245625750347, Test Loss: 0.05564209901455394\n","Epoch: 7791, Train Loss: 0.05964309109188811, Test Loss: 0.05441554041309315\n","Epoch: 7792, Train Loss: 0.05923834142431579, Test Loss: 0.05563870916151193\n","Epoch: 7793, Train Loss: 0.05963892981585345, Test Loss: 0.05441189658707543\n","Epoch: 7794, Train Loss: 0.05923423036769423, Test Loss: 0.05563532600733071\n","Epoch: 7795, Train Loss: 0.05963477517159893, Test Loss: 0.054408261816169465\n","Epoch: 7796, Train Loss: 0.059230128384574796, Test Loss: 0.055631954838793296\n","Epoch: 7797, Train Loss: 0.05963063241444806, Test Loss: 0.05440464108098555\n","Epoch: 7798, Train Loss: 0.059226040462155495, Test Loss: 0.05562860059787204\n","Epoch: 7799, Train Loss: 0.059626506457923056, Test Loss: 0.05440103900693033\n","Epoch: 7800, Train Loss: 0.05922197123044702, Test Loss: 0.055625267835536965\n","Epoch: 7801, Train Loss: 0.05962240182779496, Test Loss: 0.054397459821679386\n","Epoch: 7802, Train Loss: 0.05921792491973477, Test Loss: 0.055621960670816505\n","Epoch: 7803, Train Loss: 0.05961832262134504, Test Loss: 0.0543939073178926\n","Epoch: 7804, Train Loss: 0.059213905323305366, Test Loss: 0.05561868275543474\n","Epoch: 7805, Train Loss: 0.05961427247216175, Test Loss: 0.05439038482146219\n","Epoch: 7806, Train Loss: 0.059209915765727095, Test Loss: 0.055615437244290696\n","Epoch: 7807, Train Loss: 0.059610254520736865, Test Loss: 0.054386895165516334\n","Epoch: 7808, Train Loss: 0.05920595907690576, Test Loss: 0.05561222677197004\n","Epoch: 7809, Train Loss: 0.05960627139105066, Test Loss: 0.0543834406703323\n","Epoch: 7810, Train Loss: 0.059202037572069544, Test Loss: 0.05560905343540772\n","Epoch: 7811, Train Loss: 0.059602325173265865, Test Loss: 0.054380023129248285\n","Epoch: 7812, Train Loss: 0.05919815303777068, Test Loss: 0.05560591878276\n","Epoch: 7813, Train Loss: 0.05959841741258863, Test Loss: 0.054376643800595384\n","Epoch: 7814, Train Loss: 0.05919430672392402, Test Loss: 0.055602823808467185\n","Epoch: 7815, Train Loss: 0.059594549104278555, Test Loss: 0.05437330340560446\n","Epoch: 7816, Train Loss: 0.059190499341835706, Test Loss: 0.055599768954426726\n","Epoch: 7817, Train Loss: 0.059590720694729614, Test Loss: 0.05437000213218139\n","Epoch: 7818, Train Loss: 0.05918673106811405, Test Loss: 0.05559675411713092\n","Epoch: 7819, Train Loss: 0.0595869320884779, Test Loss: 0.05436673964438344\n","Epoch: 7820, Train Loss: 0.05918300155429343, Test Loss: 0.055593778660562025\n","Epoch: 7821, Train Loss: 0.0595831826609309, Test Loss: 0.0543635150973665\n","Epoch: 7822, Train Loss: 0.05917930994193917, Test Loss: 0.05559084143458129\n","Epoch: 7823, Train Loss: 0.059579471276557244, Test Loss: 0.05436032715752882\n","Epoch: 7824, Train Loss: 0.05917565488295724, Test Loss: 0.055587940798494745\n","Epoch: 7825, Train Loss: 0.059575796312223435, Test Loss: 0.05435717402751666\n","Epoch: 7826, Train Loss: 0.05917203456477209, Test Loss: 0.05558507464942712\n","Epoch: 7827, Train Loss: 0.059572155685311205, Test Loss: 0.054354053475718304\n","Epoch: 7828, Train Loss: 0.05916844673999719, Test Loss: 0.05558224045509594\n","Epoch: 7829, Train Loss: 0.05956854688621129, Test Loss: 0.05435096286983191\n","Epoch: 7830, Train Loss: 0.05916488876018197, Test Loss: 0.055579435290535883\n","Epoch: 7831, Train Loss: 0.05956496701474724, Test Loss: 0.0543478992140547\n","Epoch: 7832, Train Loss: 0.05916135761318023, Test Loss: 0.05557665587829171\n","Epoch: 7833, Train Loss: 0.05956141282005106, Test Loss: 0.05434485918941642\n","Epoch: 7834, Train Loss: 0.059157849963661226, Test Loss: 0.05557389863157007\n","Epoch: 7835, Train Loss: 0.05955788074338524, Test Loss: 0.05434183919675007\n","Epoch: 7836, Train Loss: 0.059154362196255125, Test Loss: 0.05557115969982161\n","Epoch: 7837, Train Loss: 0.05955436696338573, Test Loss: 0.054338835401782325\n","Epoch: 7838, Train Loss: 0.05915089046081232, Test Loss: 0.0555684350162065\n","Epoch: 7839, Train Loss: 0.05955086744318368, Test Loss: 0.054335843781806614\n","Epoch: 7840, Train Loss: 0.059147430719238704, Test Loss: 0.055565720346390915\n","Epoch: 7841, Train Loss: 0.05954737797885635, Test Loss: 0.054332860173403955\n","Epoch: 7842, Train Loss: 0.05914397879337027, Test Loss: 0.05556301133811736\n","Epoch: 7843, Train Loss: 0.059543894248655006, Test Loss: 0.054329880320669065\n","Epoch: 7844, Train Loss: 0.059140530413343, Test Loss: 0.05556030357099445\n","Epoch: 7845, Train Loss: 0.05954041186245795, Test Loss: 0.05432689992341006\n","Epoch: 7846, Train Loss: 0.05913708126592445, Test Loss: 0.05555759260596446\n","Epoch: 7847, Train Loss: 0.059536926410911205, Test Loss: 0.0543239146848011\n","Epoch: 7848, Train Loss: 0.05913362704228648, Test Loss: 0.05555487403391633\n","Epoch: 7849, Train Loss: 0.05953343351372773, Test Loss: 0.05432092035797905\n","Epoch: 7850, Train Loss: 0.059130163484707926, Test Loss: 0.05555214352293704\n","Epoch: 7851, Train Loss: 0.05952992886664113, Test Loss: 0.05431791279110714\n","Epoch: 7852, Train Loss: 0.05912668643172978, Test Loss: 0.05554939686371834\n","Epoch: 7853, Train Loss: 0.05952640828653385, Test Loss: 0.05431488797044396\n","Epoch: 7854, Train Loss: 0.05912319186130075, Test Loss: 0.05554663001266294\n","Epoch: 7855, Train Loss: 0.05952286775428634, Test Loss: 0.05431184206099566\n","Epoch: 7856, Train Loss: 0.05911967593149032, Test Loss: 0.055543839132273023\n","Epoch: 7857, Train Loss: 0.05951930345493281, Test Loss: 0.054308771444361054\n","Epoch: 7858, Train Loss: 0.059116135018378445, Test Loss: 0.05554102062844152\n","Epoch: 7859, Train Loss: 0.05951571181474532, Test Loss: 0.05430567275341718\n","Epoch: 7860, Train Loss: 0.059112565750770014, Test Loss: 0.055538171184301814\n","Epoch: 7861, Train Loss: 0.0595120895349047, Test Loss: 0.05430254290353667\n","Epoch: 7862, Train Loss: 0.05910896504142421, Test Loss: 0.05553528779034785\n","Epoch: 7863, Train Loss: 0.05950843362147029, Test Loss: 0.05429937912007238\n","Epoch: 7864, Train Loss: 0.059105330114536114, Test Loss: 0.05553236777057377\n","Epoch: 7865, Train Loss: 0.05950474141139961, Test Loss: 0.054296178961887695\n","Epoch: 7866, Train Loss: 0.059101658529247675, Test Loss: 0.05552940880443127\n","Epoch: 7867, Train Loss: 0.0595010105944166, Test Loss: 0.05429294034076426\n","Epoch: 7868, Train Loss: 0.05909794819902119, Test Loss: 0.055526408944461474\n","Epoch: 7869, Train Loss: 0.05949723923058501, Test Loss: 0.05428966153656134\n","Epoch: 7870, Train Loss: 0.05909419740675037, Test Loss: 0.05552336662949571\n","Epoch: 7871, Train Loss: 0.059493425763482474, Test Loss: 0.05428634120805494\n","Epoch: 7872, Train Loss: 0.05909040481553742, Test Loss: 0.055520280693381566\n","Epoch: 7873, Train Loss: 0.059489569028929594, Test Loss: 0.05428297839943039\n","Epoch: 7874, Train Loss: 0.059086569475110616, Test Loss: 0.0555171503692326\n","Epoch: 7875, Train Loss: 0.0594856682592735, Test Loss: 0.05427957254245278\n","Epoch: 7876, Train Loss: 0.05908269082390903, Test Loss: 0.05551397528925238\n","Epoch: 7877, Train Loss: 0.05948172308327409, Test Loss: 0.05427612345438166\n","Epoch: 7878, Train Loss: 0.05907876868690022, Test Loss: 0.055510755480230704\n","Epoch: 7879, Train Loss: 0.059477733521690695, Test Loss: 0.054272631331747964\n","Epoch: 7880, Train Loss: 0.05907480326925178, Test Loss: 0.055507491354854444\n","Epoch: 7881, Train Loss: 0.05947369997870882, Test Loss: 0.05426909674014811\n","Epoch: 7882, Train Loss: 0.05907079514601187, Test Loss: 0.05550418369901711\n","Epoch: 7883, Train Loss: 0.05946962322939069, Test Loss: 0.054265520600252797\n","Epoch: 7884, Train Loss: 0.05906674524799838, Test Loss: 0.055500833655352855\n","Epoch: 7885, Train Loss: 0.05946550440337161, Test Loss: 0.05426190417026632\n","Epoch: 7886, Train Loss: 0.059062654844133165, Test Loss: 0.05549744270325591\n","Epoch: 7887, Train Loss: 0.05946134496506253, Test Loss: 0.05425824902510135\n","Epoch: 7888, Train Loss: 0.059058525520489014, Test Loss: 0.05549401263567926\n","Epoch: 7889, Train Loss: 0.05945714669064893, Test Loss: 0.05425455703257171\n","Epoch: 7890, Train Loss: 0.059054359156351564, Test Loss: 0.05549054553303357\n","Epoch: 7891, Train Loss: 0.059452911642205224, Test Loss: 0.05425083032691995\n","Epoch: 7892, Train Loss: 0.059050157897616286, Test Loss: 0.05548704373453559\n","Epoch: 7893, Train Loss: 0.059448642139271056, Test Loss: 0.054247071280034975\n","Epoch: 7894, Train Loss: 0.05904592412787587, Test Loss: 0.05548350980737258\n","Epoch: 7895, Train Loss: 0.059444340728253095, Test Loss: 0.054243282470714575\n","Epoch: 7896, Train Loss: 0.05904166043755467, Test Loss: 0.05547994651406333\n","Epoch: 7897, Train Loss: 0.05944001015003049, Test Loss: 0.05423946665235311\n","Epoch: 7898, Train Loss: 0.05903736959147195, Test Loss: 0.055476356778412894\n","Epoch: 7899, Train Loss: 0.059435653306157965, Test Loss: 0.054235626719441626\n","Epoch: 7900, Train Loss: 0.059033054495222476, Test Loss: 0.05547274365046076\n","Epoch: 7901, Train Loss: 0.059431273224063635, Test Loss: 0.05423176567326637\n","Epoch: 7902, Train Loss: 0.059028718160761, Test Loss: 0.05546911027082182\n","Epoch: 7903, Train Loss: 0.05942687302163845, Test Loss: 0.05422788658719661\n","Epoch: 7904, Train Loss: 0.0590243636715836, Test Loss: 0.05546545983482571\n","Epoch: 7905, Train Loss: 0.059422455871619595, Test Loss: 0.05422399257195069\n","Epoch: 7906, Train Loss: 0.05901999414789481, Test Loss: 0.055461795556842934\n","Epoch: 7907, Train Loss: 0.0594180249661541, Test Loss: 0.05422008674121126\n","Epoch: 7908, Train Loss: 0.059015612712133354, Test Loss: 0.05545812063518369\n","Epoch: 7909, Train Loss: 0.05941358348192627, Test Loss: 0.05421617217795997\n","Epoch: 7910, Train Loss: 0.059011222455226435, Test Loss: 0.055454438217934034\n","Epoch: 7911, Train Loss: 0.05940913454621101, Test Loss: 0.05421225190187724\n","Epoch: 7912, Train Loss: 0.05900682640391992, Test Loss: 0.0554507513700822\n","Epoch: 7913, Train Loss: 0.05940468120420359, Test Loss: 0.05420832883813834\n","Epoch: 7914, Train Loss: 0.059002427489515466, Test Loss: 0.05544706304225888\n","Epoch: 7915, Train Loss: 0.059400226387948556, Test Loss: 0.05420440578791146\n","Epoch: 7916, Train Loss: 0.05899802851832111, Test Loss: 0.05544337604139685\n","Epoch: 7917, Train Loss: 0.05939577288717035, Test Loss: 0.05420048540084013\n","Epoch: 7918, Train Loss: 0.058993632144097245, Test Loss: 0.05543969300358397\n","Epoch: 7919, Train Loss: 0.05939132332227912, Test Loss: 0.05419657014976401\n","Epoch: 7920, Train Loss: 0.05898924084275325, Test Loss: 0.05543601636935351\n","Epoch: 7921, Train Loss: 0.05938688011979394, Test Loss: 0.054192662307900506\n","Epoch: 7922, Train Loss: 0.05898485688951624, Test Loss: 0.05543234836162713\n","Epoch: 7923, Train Loss: 0.05938244549039796, Test Loss: 0.054188763928682125\n","Epoch: 7924, Train Loss: 0.05898048233876732, Test Loss: 0.055428690966489697\n","Epoch: 7925, Train Loss: 0.059378021409803576, Test Loss: 0.05418487682840453\n","Epoch: 7926, Train Loss: 0.058976119006699856, Test Loss: 0.05542504591693889\n","Epoch: 7927, Train Loss: 0.059373609602570675, Test Loss: 0.05418100257181704\n","Epoch: 7928, Train Loss: 0.058971768456930945, Test Loss: 0.055421414679724584\n","Epoch: 7929, Train Loss: 0.05936921152899187, Test Loss: 0.05417714246073988\n","Epoch: 7930, Train Loss: 0.05896743198915076, Test Loss: 0.055417798445346\n","Epoch: 7931, Train Loss: 0.05936482837511378, Test Loss: 0.05417329752576902\n","Epoch: 7932, Train Loss: 0.0589631106308689, Test Loss: 0.055414198121250695\n","Epoch: 7933, Train Loss: 0.0593604610459372, Test Loss: 0.05416946852108795\n","Epoch: 7934, Train Loss: 0.05895880513227731, Test Loss: 0.055410614328234346\n","Epoch: 7935, Train Loss: 0.05935611016179702, Test Loss: 0.05416565592237492\n","Epoch: 7936, Train Loss: 0.05895451596421725, Test Loss: 0.0554070474000121\n","Epoch: 7937, Train Loss: 0.05935177605789267, Test Loss: 0.05416185992776162\n","Epoch: 7938, Train Loss: 0.0589502433192054, Test Loss: 0.05540349738589642\n","Epoch: 7939, Train Loss: 0.059347458786904464, Test Loss: 0.054158080461762556\n","Epoch: 7940, Train Loss: 0.05894598711543766, Test Loss: 0.055399964056484355\n","Epoch: 7941, Train Loss: 0.059343158124600906, Test Loss: 0.054154317182072684\n","Epoch: 7942, Train Loss: 0.05894174700366736, Test Loss: 0.05539644691222647\n","Epoch: 7943, Train Loss: 0.059338873578309685, Test Loss: 0.05415056948909264\n","Epoch: 7944, Train Loss: 0.05893752237681541, Test Loss: 0.0553929451947232\n","Epoch: 7945, Train Loss: 0.05933460439809994, Test Loss: 0.05414683653802373\n","Epoch: 7946, Train Loss: 0.05893331238215517, Test Loss: 0.05538945790056812\n","Epoch: 7947, Train Loss: 0.05933034959049633, Test Loss: 0.054143117253342844\n","Epoch: 7948, Train Loss: 0.05892911593587935, Test Loss: 0.055385983797530854\n","Epoch: 7949, Train Loss: 0.05932610793452042, Test Loss: 0.05413941034545078\n","Epoch: 7950, Train Loss: 0.058924931739842694, Test Loss: 0.055382521442856866\n","Epoch: 7951, Train Loss: 0.05932187799983692, Test Loss: 0.05413571432926955\n","Epoch: 7952, Train Loss: 0.05892075830025418, Test Loss: 0.055379069203439726\n","Epoch: 7953, Train Loss: 0.05931765816676398, Test Loss: 0.05413202754454052\n","Epoch: 7954, Train Loss: 0.0589165939480705, Test Loss: 0.05537562527760919\n","Epoch: 7955, Train Loss: 0.059313446647891135, Test Loss: 0.054128348177575655\n","Epoch: 7956, Train Loss: 0.058912436860840384, Test Loss: 0.055372187718263216\n","Epoch: 7957, Train Loss: 0.05930924151103642, Test Loss: 0.054124674284186935\n","Epoch: 7958, Train Loss: 0.058908285085726236, Test Loss: 0.0553687544570674\n","Epoch: 7959, Train Loss: 0.059305040703267094, Test Loss: 0.05412100381352978\n","Epoch: 7960, Train Loss: 0.05890413656343557, Test Loss: 0.05536532332943703\n","Epoch: 7961, Train Loss: 0.05930084207570204, Test Loss: 0.05411733463257749\n","Epoch: 7962, Train Loss: 0.05889998915278019, Test Loss: 0.0553618921000159\n","Epoch: 7963, Train Loss: 0.05929664340881108, Test Loss: 0.0541136645509538\n","Epoch: 7964, Train Loss: 0.0588958406555884, Test Loss: 0.05535845848836717\n","Epoch: 7965, Train Loss: 0.05929244243792927, Test Loss: 0.05410999134584414\n","Epoch: 7966, Train Loss: 0.05889168884169044, Test Loss: 0.05535502019459312\n","Epoch: 7967, Train Loss: 0.059288236878704124, Test Loss: 0.05410631278672027\n","Epoch: 7968, Train Loss: 0.05888753147371094, Test Loss: 0.055351574924611506\n","Epoch: 7969, Train Loss: 0.059284024452205875, Test Loss: 0.054102626659609986\n","Epoch: 7970, Train Loss: 0.058883366331399956, Test Loss: 0.05534812041482164\n","Epoch: 7971, Train Loss: 0.05927980290943498, Test Loss: 0.05409893079066587\n","Epoch: 7972, Train Loss: 0.05887919123525529, Test Loss: 0.055344654455908776\n","Epoch: 7973, Train Loss: 0.059275570054977376, Test Loss: 0.0540952230687904\n","Epoch: 7974, Train Loss: 0.05887500406919385, Test Loss: 0.055341174915547714\n","Epoch: 7975, Train Loss: 0.05927132376956988, Test Loss: 0.05409150146709526\n","Epoch: 7976, Train Loss: 0.05887080280204847, Test Loss: 0.055337679759785346\n","Epoch: 7977, Train Loss: 0.059267062031356135, Test Loss: 0.054087764062988306\n","Epoch: 7978, Train Loss: 0.05886658550768492, Test Loss: 0.05533416707290028\n","Epoch: 7979, Train Loss: 0.059262782935633126, Test Loss: 0.0540840090567023\n","Epoch: 7980, Train Loss: 0.05886235038355153, Test Loss: 0.055330635075558424\n","Epoch: 7981, Train Loss: 0.059258484712907504, Test Loss: 0.054080234788097814\n","Epoch: 7982, Train Loss: 0.05885809576749478, Test Loss: 0.05532708214110478\n","Epoch: 7983, Train Loss: 0.05925416574510291, Test Loss: 0.05407643975159717\n","Epoch: 7984, Train Loss: 0.05885382015269713, Test Loss: 0.05532350680985606\n","Epoch: 7985, Train Loss: 0.05924982457978372, Test Loss: 0.054072622609130046\n","Epoch: 7986, Train Loss: 0.058849522200618724, Test Loss: 0.055319907801286865\n","Epoch: 7987, Train Loss: 0.05924545994228785, Test Loss: 0.054068782200992004\n","Epoch: 7988, Train Loss: 0.058845200751843366, Test Loss: 0.05531628402401689\n","Epoch: 7989, Train Loss: 0.05924107074567692, Test Loss: 0.05406491755454577\n","Epoch: 7990, Train Loss: 0.05884085483475955, Test Loss: 0.05531263458354651\n","Epoch: 7991, Train Loss: 0.059236656098450774, Test Loss: 0.05406102789071872\n","Epoch: 7992, Train Loss: 0.058836483672030496, Test Loss: 0.05530895878770236\n","Epoch: 7993, Train Loss: 0.05923221530998792, Test Loss: 0.05405711262827199\n","Epoch: 7994, Train Loss: 0.05883208668482848, Test Loss: 0.05530525614978232\n","Epoch: 7995, Train Loss: 0.05922774789370129, Test Loss: 0.05405317138584717\n","Epoch: 7996, Train Loss: 0.058827663494840435, Test Loss: 0.055301526389422526\n","Epoch: 7997, Train Loss: 0.059223253567930845, Test Loss: 0.05404920398181414\n","Epoch: 7998, Train Loss: 0.05882321392406888, Test Loss: 0.05529776943121962\n","Epoch: 7999, Train Loss: 0.05921873225460652, Test Loss: 0.05404521043197069\n","Epoch: 8000, Train Loss: 0.05881873799247916, Test Loss: 0.055293985401176\n","Epoch: 8001, Train Loss: 0.05921418407574772, Test Loss: 0.05404119094516655\n","Epoch: 8002, Train Loss: 0.05881423591356656, Test Loss: 0.05529017462105554\n","Epoch: 8003, Train Loss: 0.059209609347886445, Test Loss: 0.05403714591694358\n","Epoch: 8004, Train Loss: 0.05880970808793573, Test Loss: 0.0552863376007503\n","Epoch: 8005, Train Loss: 0.05920500857451317, Test Loss: 0.05403307592130308\n","Epoch: 8006, Train Loss: 0.058805155095004155, Test Loss: 0.055282475028792424\n","Epoch: 8007, Train Loss: 0.05920038243667901, Test Loss: 0.05402898170073334\n","Epoch: 8008, Train Loss: 0.058800577682963026, Test Loss: 0.055278587761145984\n","Epoch: 8009, Train Loss: 0.059195731781887556, Test Loss: 0.054024864154638565\n","Epoch: 8010, Train Loss: 0.05879597675713832, Test Loss: 0.05527467680844351\n","Epoch: 8011, Train Loss: 0.05919105761143938, Test Loss: 0.054020724326333786\n","Epoch: 8012, Train Loss: 0.058791353366916765, Test Loss: 0.055270743321837026\n","Epoch: 8013, Train Loss: 0.05918636106639852, Test Loss: 0.054016563388774086\n","Epoch: 8014, Train Loss: 0.05878670869140586, Test Loss: 0.055266788577644374\n","Epoch: 8015, Train Loss: 0.05918164341235956, Test Loss: 0.05401238262920092\n","Epoch: 8016, Train Loss: 0.05878204402401172, Test Loss: 0.05526281396098613\n","Epoch: 8017, Train Loss: 0.059176906023209776, Test Loss: 0.054008183432894985\n","Epoch: 8018, Train Loss: 0.05877736075612469, Test Loss: 0.05525882094860945\n","Epoch: 8019, Train Loss: 0.05917215036408086, Test Loss: 0.05400396726622894\n","Epoch: 8020, Train Loss: 0.058772660360107004, Test Loss: 0.05525481109110276\n","Epoch: 8021, Train Loss: 0.059167377973692765, Test Loss: 0.05399973565922167\n","Epoch: 8022, Train Loss: 0.05876794437178424, Test Loss: 0.055250785994705266\n","Epoch: 8023, Train Loss: 0.059162590446292435, Test Loss: 0.053995490187786724\n","Epoch: 8024, Train Loss: 0.05876321437263456, Test Loss: 0.05524674730291706\n","Epoch: 8025, Train Loss: 0.05915778941339172, Test Loss: 0.05399123245587895\n","Epoch: 8026, Train Loss: 0.05875847197188003, Test Loss: 0.0552426966781115\n","Epoch: 8027, Train Loss: 0.05915297652550444, Test Loss: 0.05398696407772974\n","Epoch: 8028, Train Loss: 0.05875371878867021, Test Loss: 0.0552386357833473\n","Epoch: 8029, Train Loss: 0.05914815343407949, Test Loss: 0.05398268666036055\n","Epoch: 8030, Train Loss: 0.0587489564345498, Test Loss: 0.05523456626457182\n","Epoch: 8031, Train Loss: 0.059143321773819625, Test Loss: 0.05397840178655838\n","Epoch: 8032, Train Loss: 0.05874418649639257, Test Loss: 0.05523048973339932\n","Epoch: 8033, Train Loss: 0.05913848314556873, Test Loss: 0.05397411099848451\n","Epoch: 8034, Train Loss: 0.05873941051997452, Test Loss: 0.05522640775063396\n","Epoch: 8035, Train Loss: 0.059133639099936686, Test Loss: 0.053969815782077386\n","Epoch: 8036, Train Loss: 0.05873462999434663, Test Loss: 0.055222321810700435\n","Epoch: 8037, Train Loss: 0.05912879112182306, Test Loss: 0.05396551755240216\n","Epoch: 8038, Train Loss: 0.05872984633716054, Test Loss: 0.05521823332712759\n","Epoch: 8039, Train Loss: 0.0591239406159851, Test Loss: 0.053961217640082694\n","Epoch: 8040, Train Loss: 0.05872506088108256, Test Loss: 0.05521414361922036\n","Epoch: 8041, Train Loss: 0.0591190888937836, Test Loss: 0.05395691727893567\n","Epoch: 8042, Train Loss: 0.05872027486141598, Test Loss: 0.05521005390003217\n","Epoch: 8043, Train Loss: 0.05911423716121931, Test Loss: 0.05395261759491509\n","Epoch: 8044, Train Loss: 0.05871548940503999, Test Loss: 0.055205965265743305\n","Epoch: 8045, Train Loss: 0.05910938650836383, Test Loss: 0.053948319596456656\n","Epoch: 8046, Train Loss: 0.058710705520754365, Test Loss: 0.0552018786865239\n","Epoch: 8047, Train Loss: 0.05910453790026443, Test Loss: 0.053944024166292785\n","Epoch: 8048, Train Loss: 0.05870592409110118, Test Loss: 0.05519779499894936\n","Epoch: 8049, Train Loss: 0.0590996921693892, Test Loss: 0.053939732054796206\n","Epoch: 8050, Train Loss: 0.05870114586571995, Test Loss: 0.055193714900013464\n","Epoch: 8051, Train Loss: 0.05909485000965903, Test Loss: 0.053935443874886795\n","Epoch: 8052, Train Loss: 0.05869637145627281, Test Loss: 0.05518963894276908\n","Epoch: 8053, Train Loss: 0.059090011972095256, Test Loss: 0.053931160098525124\n","Epoch: 8054, Train Loss: 0.058691601332960266, Test Loss: 0.05518556753360807\n","Epoch: 8055, Train Loss: 0.05908517846209529, Test Loss: 0.0539268810547905\n","Epoch: 8056, Train Loss: 0.05868683582262737, Test Loss: 0.055181500931169546\n","Epoch: 8057, Train Loss: 0.05908034973832559, Test Loss: 0.05392260692953567\n","Epoch: 8058, Train Loss: 0.05868207510845024, Test Loss: 0.055177439246857744\n","Epoch: 8059, Train Loss: 0.059075525913213967, Test Loss: 0.05391833776657904\n","Epoch: 8060, Train Loss: 0.05867731923116462, Test Loss: 0.055173382446923526\n","Epoch: 8061, Train Loss: 0.05907070695499464, Test Loss: 0.053914073470395535\n","Epoch: 8062, Train Loss: 0.05867256809179638, Test Loss: 0.05516933035605549\n","Epoch: 8063, Train Loss: 0.059065892691254306, Test Loss: 0.053909813810239106\n","Epoch: 8064, Train Loss: 0.0586678214558267, Test Loss: 0.05516528266240916\n","Epoch: 8065, Train Loss: 0.05906108281390681, Test Loss: 0.05390555842562302\n","Epoch: 8066, Train Loss: 0.058663078958717904, Test Loss: 0.05516123892398607\n","Epoch: 8067, Train Loss: 0.05905627688551015, Test Loss: 0.05390130683306907\n","Epoch: 8068, Train Loss: 0.05865834011270974, Test Loss: 0.05515719857626808\n","Epoch: 8069, Train Loss: 0.059051474346831, Test Loss: 0.0538970584340255\n","Epoch: 8070, Train Loss: 0.05865360431478614, Test Loss: 0.055153160940994136\n","Epoch: 8071, Train Loss: 0.059046674525546086, Test Loss: 0.0538928125238441\n","Epoch: 8072, Train Loss: 0.05864887085570218, Test Loss: 0.055149125235965074\n","Epoch: 8073, Train Loss: 0.05904187664596551, Test Loss: 0.053888568301695215\n","Epoch: 8074, Train Loss: 0.05864413892994899, Test Loss: 0.055145090585742976\n","Epoch: 8075, Train Loss: 0.05903707983964688, Test Loss: 0.05388432488129523\n","Epoch: 8076, Train Loss: 0.05863940764653116, Test Loss: 0.05514105603311607\n","Epoch: 8077, Train Loss: 0.0590322831567713, Test Loss: 0.05388008130231437\n","Epoch: 8078, Train Loss: 0.058634676040423835, Test Loss: 0.05513702055118737\n","Epoch: 8079, Train Loss: 0.05902748557814088, Test Loss: 0.05387583654232597\n","Epoch: 8080, Train Loss: 0.05862994308456999, Test Loss: 0.055132983055943184\n","Epoch: 8081, Train Loss: 0.05902268602765488, Test Loss: 0.05387158952916003\n","Epoch: 8082, Train Loss: 0.05862520770228023, Test Loss: 0.05512894241915963\n","Epoch: 8083, Train Loss: 0.05901788338512338, Test Loss: 0.0538673391535166\n","Epoch: 8084, Train Loss: 0.05862046877989059, Test Loss: 0.05512489748149746\n","Epoch: 8085, Train Loss: 0.05901307649927013, Test Loss: 0.053863084281703145\n","Epoch: 8086, Train Loss: 0.05861572517954126, Test Loss: 0.05512084706564503\n","Epoch: 8087, Train Loss: 0.05900826420078539, Test Loss: 0.05385882376835055\n","Epoch: 8088, Train Loss: 0.058610975751931534, Test Loss: 0.05511678998936444\n","Epoch: 8089, Train Loss: 0.059003445315284106, Test Loss: 0.0538545564689799\n","Epoch: 8090, Train Loss: 0.05860621934892177, Test Loss: 0.05511272507830611\n","Epoch: 8091, Train Loss: 0.05899861867603653, Test Loss: 0.05385028125228068\n","Epoch: 8092, Train Loss: 0.058601454835843614, Test Loss: 0.055108651178458055\n","Epoch: 8093, Train Loss: 0.05899378313633736, Test Loss: 0.05384599701198281\n","Epoch: 8094, Train Loss: 0.0585966811033997, Test Loss: 0.055104567168104385\n","Epoch: 8095, Train Loss: 0.05898893758138967, Test Loss: 0.05384170267819868\n","Epoch: 8096, Train Loss: 0.05859189707902981, Test Loss: 0.05510047196917853\n","Epoch: 8097, Train Loss: 0.058984080939588894, Test Loss: 0.05383739722812981\n","Epoch: 8098, Train Loss: 0.05858710173763683, Test Loss: 0.05509636455790006\n","Epoch: 8099, Train Loss: 0.05897921219309731, Test Loss: 0.053833079696034764\n","Epoch: 8100, Train Loss: 0.058582294111570184, Test Loss: 0.0550922439745993\n","Epoch: 8101, Train Loss: 0.05897433038761241, Test Loss: 0.05382874918236902\n","Epoch: 8102, Train Loss: 0.05857747329977632, Test Loss: 0.05508810933263975\n","Epoch: 8103, Train Loss: 0.058969434641241505, Test Loss: 0.053824404862017304\n","Epoch: 8104, Train Loss: 0.058572638476037456, Test Loss: 0.05508395982636685\n","Epoch: 8105, Train Loss: 0.05896452415240936, Test Loss: 0.05382004599155014\n","Epoch: 8106, Train Loss: 0.058567788896229996, Test Loss: 0.05507979473801552\n","Epoch: 8107, Train Loss: 0.05895959820673323, Test Loss: 0.05381567191544898\n","Epoch: 8108, Train Loss: 0.05856292390454722, Test Loss: 0.05507561344352827\n","Epoch: 8109, Train Loss: 0.058954656182816464, Test Loss: 0.05381128207125426\n","Epoch: 8110, Train Loss: 0.05855804293864077, Test Loss: 0.05507141541724337\n","Epoch: 8111, Train Loss: 0.058949697556920214, Test Loss: 0.053806875993608214\n","Epoch: 8112, Train Loss: 0.05855314553365244, Test Loss: 0.05506720023543103\n","Epoch: 8113, Train Loss: 0.05894472190649176, Test Loss: 0.05380245331717048\n","Epoch: 8114, Train Loss: 0.058548231325115475, Test Loss: 0.05506296757866082\n","Epoch: 8115, Train Loss: 0.05893972891253177, Test Loss: 0.053798013778401904\n","Epoch: 8116, Train Loss: 0.058543300050719664, Test Loss: 0.0550587172330057\n","Epoch: 8117, Train Loss: 0.05893471836080641, Test Loss: 0.05379355721622172\n","Epoch: 8118, Train Loss: 0.058538351550947394, Test Loss: 0.05505444909009029\n","Epoch: 8119, Train Loss: 0.05892969014191153, Test Loss: 0.05378908357155674\n","Epoch: 8120, Train Loss: 0.058533385768597526, Test Loss: 0.0550501631460138\n","Epoch: 8121, Train Loss: 0.058924644250218275, Test Loss: 0.05378459288581133\n","Epoch: 8122, Train Loss: 0.05852840274722844, Test Loss: 0.05504585949918\n","Epoch: 8123, Train Loss: 0.05891958078173351, Test Loss: 0.053780085298297566\n","Epoch: 8124, Train Loss: 0.05852340262855775, Test Loss: 0.05504153834708196\n","Epoch: 8125, Train Loss: 0.05891449993092077, Test Loss: 0.053775561042678\n","Epoch: 8126, Train Loss: 0.058518385648873746, Test Loss: 0.05503719998210189\n","Epoch: 8127, Train Loss: 0.05890940198654273, Test Loss: 0.05377102044247928\n","Epoch: 8128, Train Loss: 0.05851335213451501, Test Loss: 0.05503284478638996\n","Epoch: 8129, Train Loss: 0.058904287326587784, Test Loss: 0.053766463905745375\n","Epoch: 8130, Train Loss: 0.058508302496489036, Test Loss: 0.055028473225896886\n","Epoch: 8131, Train Loss: 0.05889915641235524, Test Loss: 0.053761891918906596\n","Epoch: 8132, Train Loss: 0.05850323722430554, Test Loss: 0.05502408584364567\n","Epoch: 8133, Train Loss: 0.05889400978178367, Test Loss: 0.05375730503994831\n","Epoch: 8134, Train Loss: 0.05849815687910896, Test Loss: 0.05501968325232947\n","Epoch: 8135, Train Loss: 0.058888848042108775, Test Loss: 0.05375270389096697\n","Epoch: 8136, Train Loss: 0.05849306208619835, Test Loss: 0.0550152661263279\n","Epoch: 8137, Train Loss: 0.058883671861942426, Test Loss: 0.05374808915020477\n","Epoch: 8138, Train Loss: 0.058487953527026064, Test Loss: 0.055010835193242874\n","Epoch: 8139, Train Loss: 0.058878481962873166, Test Loss: 0.05374346154366357\n","Epoch: 8140, Train Loss: 0.05848283193077628, Test Loss: 0.055006391225053325\n","Epoch: 8141, Train Loss: 0.05887327911068668, Test Loss: 0.053738821836392864\n","Epoch: 8142, Train Loss: 0.05847769806561909, Test Loss: 0.0550019350289918\n","Epoch: 8143, Train Loss: 0.05886806410630874, Test Loss: 0.05373417082355457\n","Epoch: 8144, Train Loss: 0.05847255272974213, Test Loss: 0.054997467438247225\n","Epoch: 8145, Train Loss: 0.05886283777657406, Test Loss: 0.053729509321363694\n","Epoch: 8146, Train Loss: 0.058467396742260175, Test Loss: 0.05499298930259479\n","Epoch: 8147, Train Loss: 0.058857600964921264, Test Loss: 0.053724838158004204\n","Epoch: 8148, Train Loss: 0.058462230934102064, Test Loss: 0.05498850147905704\n","Epoch: 8149, Train Loss: 0.05885235452211757, Test Loss: 0.053720158164618524\n","Epoch: 8150, Train Loss: 0.05845705613897282, Test Loss: 0.05498400482269337\n","Epoch: 8151, Train Loss: 0.05884709929710955, Test Loss: 0.053715470166463344\n","Epoch: 8152, Train Loss: 0.058451873184485574, Test Loss: 0.05497950017761369\n","Epoch: 8153, Train Loss: 0.05884183612809537, Test Loss: 0.053710774974324395\n","Epoch: 8154, Train Loss: 0.058446682883554035, Test Loss: 0.054974988368307456\n","Epoch: 8155, Train Loss: 0.05883656583390841, Test Loss: 0.05370607337627334\n","Epoch: 8156, Train Loss: 0.05844148602613099, Test Loss: 0.054970470191371516\n","Epoch: 8157, Train Loss: 0.05883128920579671, Test Loss: 0.05370136612984975\n","Epoch: 8158, Train Loss: 0.05843628337137412, Test Loss: 0.05496594640771815\n","Epoch: 8159, Train Loss: 0.05882600699967744, Test Loss: 0.053696653954738875\n","Epoch: 8160, Train Loss: 0.05843107564031091, Test Loss: 0.054961417735335964\n","Epoch: 8161, Train Loss: 0.05882071992893971, Test Loss: 0.05369193752601558\n","Epoch: 8162, Train Loss: 0.05842586350907321, Test Loss: 0.05495688484266457\n","Epoch: 8163, Train Loss: 0.058815428657856286, Test Loss: 0.053687217468009966\n","Epoch: 8164, Train Loss: 0.05842064760275633, Test Loss: 0.05495234834264467\n","Epoch: 8165, Train Loss: 0.058810133795664916, Test Loss: 0.053682494348849516\n","Epoch: 8166, Train Loss: 0.05841542848995798, Test Loss: 0.054947808787488235\n","Epoch: 8167, Train Loss: 0.05880483589136431, Test Loss: 0.053677768675718225\n","Epoch: 8168, Train Loss: 0.05841020667803727, Test Loss: 0.05494326666421004\n","Epoch: 8169, Train Loss: 0.058799535429265734, Test Loss: 0.05367304089086813\n","Epoch: 8170, Train Loss: 0.05840498260912961, Test Loss: 0.05493872239095098\n","Epoch: 8171, Train Loss: 0.058794232825330674, Test Loss: 0.053668311368409716\n","Epoch: 8172, Train Loss: 0.05839975665694286, Test Loss: 0.054934176314113896\n","Epoch: 8173, Train Loss: 0.058788928424314636, Test Loss: 0.05366358041189693\n","Epoch: 8174, Train Loss: 0.058394529124351405, Test Loss: 0.054929628706325825\n","Epoch: 8175, Train Loss: 0.05878362249773226, Test Loss: 0.05365884825271401\n","Epoch: 8176, Train Loss: 0.05838930024179481, Test Loss: 0.05492507976522674\n","Epoch: 8177, Train Loss: 0.05877831524264255, Test Loss: 0.053654115049264955\n","Epoch: 8178, Train Loss: 0.05838407016648139, Test Loss: 0.05492052961308318\n","Epoch: 8179, Train Loss: 0.05877300678125374, Test Loss: 0.05364938088695728\n","Epoch: 8180, Train Loss: 0.058378838982388416, Test Loss: 0.05491597829721059\n","Epoch: 8181, Train Loss: 0.05876769716133159, Test Loss: 0.053644645778958215\n","Epoch: 8182, Train Loss: 0.05837360670103728, Test Loss: 0.05491142579118082\n","Epoch: 8183, Train Loss: 0.05876238635738754, Test Loss: 0.05363990966770396\n","Epoch: 8184, Train Loss: 0.05836837326302309, Test Loss: 0.054906871996786746\n","Epoch: 8185, Train Loss: 0.05875707427261927, Test Loss: 0.05363517242712388\n","Epoch: 8186, Train Loss: 0.05836313854026109, Test Loss: 0.05490231674672446\n","Epoch: 8187, Train Loss: 0.05875176074156455, Test Loss: 0.05363043386554396\n","Epoch: 8188, Train Loss: 0.05835790233891369, Test Loss: 0.05489775980794948\n","Epoch: 8189, Train Loss: 0.058746445533425055, Test Loss: 0.0536256937292219\n","Epoch: 8190, Train Loss: 0.05835266440295012, Test Loss: 0.05489320088565692\n","Epoch: 8191, Train Loss: 0.058741128356010364, Test Loss: 0.0536209517064619\n","Epoch: 8192, Train Loss: 0.058347424418286775, Test Loss: 0.0548886396278282\n","Epoch: 8193, Train Loss: 0.05873580886024576, Test Loss: 0.053616207432254606\n","Epoch: 8194, Train Loss: 0.05834218201745289, Test Loss: 0.05488407563028453\n","Epoch: 8195, Train Loss: 0.05873048664518376, Test Loss: 0.0536114604933784\n","Epoch: 8196, Train Loss: 0.05833693678471796, Test Loss: 0.05487950844218065\n","Epoch: 8197, Train Loss: 0.0587251612634541, Test Loss: 0.05360671043389973\n","Epoch: 8198, Train Loss: 0.05833168826161846, Test Loss: 0.05487493757187394\n","Epoch: 8199, Train Loss: 0.0587198322270873, Test Loss: 0.05360195676100587\n","Epoch: 8200, Train Loss: 0.058326435952816086, Test Loss: 0.054870362493095355\n","Epoch: 8201, Train Loss: 0.05871449901363906, Test Loss: 0.05359719895109843\n","Epoch: 8202, Train Loss: 0.05832117933221666, Test Loss: 0.05486578265135037\n","Epoch: 8203, Train Loss: 0.05870916107254384, Test Loss: 0.053592436456078495\n","Epoch: 8204, Train Loss: 0.05831591784927943, Test Loss: 0.054861197470479156\n","Epoch: 8205, Train Loss: 0.05870381783162797, Test Loss: 0.05358766870975399\n","Epoch: 8206, Train Loss: 0.058310650935447995, Test Loss: 0.054856606359300224\n","Epoch: 8207, Train Loss: 0.05869846870370644, Test Loss: 0.053582895134296135\n","Epoch: 8208, Train Loss: 0.0583053780106285, Test Loss: 0.054852008718267724\n","Epoch: 8209, Train Loss: 0.058693113093194256, Test Loss: 0.053578115146677\n","Epoch: 8210, Train Loss: 0.0583000984896477, Test Loss: 0.05484740394606834\n","Epoch: 8211, Train Loss: 0.058687750402658764, Test Loss: 0.05357332816501811\n","Epoch: 8212, Train Loss: 0.05829481178862062, Test Loss: 0.05484279144609238\n","Epoch: 8213, Train Loss: 0.05868238003924819, Test Loss: 0.05356853361479064\n","Epoch: 8214, Train Loss: 0.058289517331167315, Test Loss: 0.0548381706327131\n","Epoch: 8215, Train Loss: 0.058677001420930394, Test Loss: 0.0535637309347964\n","Epoch: 8216, Train Loss: 0.05828421455440922, Test Loss: 0.0548335409373086\n","Epoch: 8217, Train Loss: 0.05867161398247704, Test Loss: 0.05355891958287848\n","Epoch: 8218, Train Loss: 0.05827890291469229, Test Loss: 0.05482890181397207\n","Epoch: 8219, Train Loss: 0.05866621718113883, Test Loss: 0.05355409904130203\n","Epoch: 8220, Train Loss: 0.0582735818929788, Test Loss: 0.0548242527448549\n","Epoch: 8221, Train Loss: 0.05866081050195713, Test Loss: 0.053549268821759206\n","Epoch: 8222, Train Loss: 0.058268250999860514, Test Loss: 0.0548195932450953\n","Epoch: 8223, Train Loss: 0.058655393462664504, Test Loss: 0.05354442846995167\n","Epoch: 8224, Train Loss: 0.05826290978014733, Test Loss: 0.05481492286728822\n","Epoch: 8225, Train Loss: 0.05864996561813021, Test Loss: 0.0535395775697107\n","Epoch: 8226, Train Loss: 0.058257557816991296, Test Loss: 0.05481024120545918\n","Epoch: 8227, Train Loss: 0.05864452656431362, Test Loss: 0.05353471574662335\n","Epoch: 8228, Train Loss: 0.05825219473551415, Test Loss: 0.05480554789851407\n","Epoch: 8229, Train Loss: 0.05863907594169778, Test Loss: 0.05352984267113797\n","Epoch: 8230, Train Loss: 0.05824682020591231, Test Loss: 0.05480084263313593\n","Epoch: 8231, Train Loss: 0.05863361343817381, Test Loss: 0.05352495806112394\n","Epoch: 8232, Train Loss: 0.05824143394601333, Test Loss: 0.05479612514611263\n","Epoch: 8233, Train Loss: 0.058628138791360515, Test Loss: 0.05352006168387491\n","Epoch: 8234, Train Loss: 0.05823603572327431, Test Loss: 0.054791395226083686\n","Epoch: 8235, Train Loss: 0.05862265179034751, Test Loss: 0.05351515335754559\n","Epoch: 8236, Train Loss: 0.058230625356211416, Test Loss: 0.054786652714700776\n","Epoch: 8237, Train Loss: 0.05861715227685558, Test Loss: 0.05351023295201699\n","Epoch: 8238, Train Loss: 0.05822520271525602, Test Loss: 0.05478189750719987\n","Epoch: 8239, Train Loss: 0.058611640145813444, Test Loss: 0.05350530038919673\n","Epoch: 8240, Train Loss: 0.0582197677230442, Test Loss: 0.05477712955239563\n","Epoch: 8241, Train Loss: 0.05860611534536, Test Loss: 0.053500355642763085\n","Epoch: 8242, Train Loss: 0.05821432035414845, Test Loss: 0.05477234885211079\n","Epoch: 8243, Train Loss: 0.058600577876285614, Test Loss: 0.05349539873736788\n","Epoch: 8244, Train Loss: 0.05820886063426637, Test Loss: 0.05476755546005656\n","Epoch: 8245, Train Loss: 0.05859502779092786, Test Loss: 0.053490429747316985\n","Epoch: 8246, Train Loss: 0.058203388638886126, Test Loss: 0.054762749480188805\n","Epoch: 8247, Train Loss: 0.058589465191546355, Test Loss: 0.05348544879475663\n","Epoch: 8248, Train Loss: 0.05819790449145631, Test Loss: 0.05475793106457303\n","Epoch: 8249, Train Loss: 0.05858389022820913, Test Loss: 0.053480456047394466\n","Epoch: 8250, Train Loss: 0.05819240836108991, Test Loss: 0.05475310041078496\n","Epoch: 8251, Train Loss: 0.05857830309621783, Test Loss: 0.05347545171579004\n","Epoch: 8252, Train Loss: 0.05818690045983659, Test Loss: 0.05474825775889143\n","Epoch: 8253, Train Loss: 0.05857270403311512, Test Loss: 0.05347043605025238\n","Epoch: 8254, Train Loss: 0.05818138103956211, Test Loss: 0.05474340338804699\n","Epoch: 8255, Train Loss: 0.05856709331530998, Test Loss: 0.0534654093373881\n","Epoch: 8256, Train Loss: 0.05817585038847737, Test Loss: 0.05473853761275538\n","Epoch: 8257, Train Loss: 0.05856147125436971, Test Loss: 0.05346037189634061\n","Epoch: 8258, Train Loss: 0.05817030882735889, Test Loss: 0.0547336607788399\n","Epoch: 8259, Train Loss: 0.05855583819302205, Test Loss: 0.05345532407477244\n","Epoch: 8260, Train Loss: 0.05816475670551246, Test Loss: 0.054728773259173545\n","Epoch: 8261, Train Loss: 0.058550194500918154, Test Loss: 0.05345026624463363\n","Epoch: 8262, Train Loss: 0.05815919439652306, Test Loss: 0.05472387544921735\n","Epoch: 8263, Train Loss: 0.05854454057020414, Test Loss: 0.05344519879776859\n","Epoch: 8264, Train Loss: 0.05815362229384338, Test Loss: 0.05471896776242076\n","Epoch: 8265, Train Loss: 0.0585388768109553, Test Loss: 0.05344012214141067\n","Epoch: 8266, Train Loss: 0.058148040806271555, Test Loss: 0.054714050625534194\n","Epoch: 8267, Train Loss: 0.05853320364652204, Test Loss: 0.05343503669361535\n","Epoch: 8268, Train Loss: 0.05814245035336749, Test Loss: 0.0547091244738854\n","Epoch: 8269, Train Loss: 0.05852752150883926, Test Loss: 0.053429942878679075\n","Epoch: 8270, Train Loss: 0.05813685136085626, Test Loss: 0.05470418974666975\n","Epoch: 8271, Train Loss: 0.05852183083374895, Test Loss: 0.053424841122595206\n","Epoch: 8272, Train Loss: 0.05813124425606979, Test Loss: 0.054699246882305284\n","Epoch: 8273, Train Loss: 0.05851613205638639, Test Loss: 0.053419731848593464\n","Epoch: 8274, Train Loss: 0.05812562946347287, Test Loss: 0.05469429631389988\n","Epoch: 8275, Train Loss: 0.058510425606676804, Test Loss: 0.053414615472806994\n","Epoch: 8276, Train Loss: 0.058120007400318696, Test Loss: 0.05468933846487263\n","Epoch: 8277, Train Loss: 0.05850471190498495, Test Loss: 0.05340949240010971\n","Epoch: 8278, Train Loss: 0.05811437847247557, Test Loss: 0.054684373744776686\n","Epoch: 8279, Train Loss: 0.05849899135796396, Test Loss: 0.053404363020166615\n","Epoch: 8280, Train Loss: 0.05810874307046863, Test Loss: 0.054679402545360765\n","Epoch: 8281, Train Loss: 0.05849326435464042, Test Loss: 0.053399227703730276\n","Epoch: 8282, Train Loss: 0.058103101565768915, Test Loss: 0.054674425236904675\n","Epoch: 8283, Train Loss: 0.0584875312627716, Test Loss: 0.05339408679921953\n","Epoch: 8284, Train Loss: 0.05809745430736663, Test Loss: 0.05466944216486292\n","Epoch: 8285, Train Loss: 0.05848179242550775, Test Loss: 0.053388940629609465\n","Epoch: 8286, Train Loss: 0.05809180161865691, Test Loss: 0.05466445364684442\n","Epoch: 8287, Train Loss: 0.058476048158388014, Test Loss: 0.053383789489658294\n","Epoch: 8288, Train Loss: 0.0580861437946647, Test Loss: 0.054659459969952\n","Epoch: 8289, Train Loss: 0.05847029874669327, Test Loss: 0.05337863364349107\n","Epoch: 8290, Train Loss: 0.05808048109962762, Test Loss: 0.0546544613885017\n","Epoch: 8291, Train Loss: 0.05846454444317577, Test Loss: 0.05337347332256274\n","Epoch: 8292, Train Loss: 0.05807481376495986, Test Loss: 0.05464945812213801\n","Epoch: 8293, Train Loss: 0.05845878546618196, Test Loss: 0.05336830872400615\n","Epoch: 8294, Train Loss: 0.05806914198760326, Test Loss: 0.0546444503543557\n","Epoch: 8295, Train Loss: 0.05845302199817877, Test Loss: 0.05336314000938067\n","Epoch: 8296, Train Loss: 0.05806346592877921, Test Loss: 0.054639438231433614\n","Epoch: 8297, Train Loss: 0.05844725418468901, Test Loss: 0.05335796730382088\n","Epoch: 8298, Train Loss: 0.058057785713143524, Test Loss: 0.054634421861784875\n","Epoch: 8299, Train Loss: 0.05844148213364034, Test Loss: 0.0533527906955873\n","Epoch: 8300, Train Loss: 0.05805210142834367, Test Loss: 0.05462940131571777\n","Epoch: 8301, Train Loss: 0.05843570591512192, Test Loss: 0.05334761023601507\n","Epoch: 8302, Train Loss: 0.05804641312497557, Test Loss: 0.05462437662560312\n","Epoch: 8303, Train Loss: 0.05842992556154506, Test Loss: 0.05334242593985115\n","Epoch: 8304, Train Loss: 0.05804072081693009, Test Loss: 0.05461934778643643\n","Epoch: 8305, Train Loss: 0.05842414106819604, Test Loss: 0.05333723778596801\n","Epoch: 8306, Train Loss: 0.05803502448211714, Test Loss: 0.05461431475677967\n","Epoch: 8307, Train Loss: 0.058418352394166026, Test Loss: 0.053332045718438977\n","Epoch: 8308, Train Loss: 0.05802932406355185, Test Loss: 0.05460927746006469\n","Epoch: 8309, Train Loss: 0.05841255946364022, Test Loss: 0.053326849647954096\n","Epoch: 8310, Train Loss: 0.05802361947078267, Test Loss: 0.05460423578623677\n","Epoch: 8311, Train Loss: 0.05840676216752581, Test Loss: 0.053321649453555865\n","Epoch: 8312, Train Loss: 0.05801791058163943, Test Loss: 0.05459918959371276\n","Epoch: 8313, Train Loss: 0.05840096036539176, Test Loss: 0.05331644498466719\n","Epoch: 8314, Train Loss: 0.05801219724427439, Test Loss: 0.05459413871162589\n","Epoch: 8315, Train Loss: 0.0583951538876944, Test Loss: 0.05331123606338605\n","Epoch: 8316, Train Loss: 0.05800647927947024, Test Loss: 0.05458908294232881\n","Epoch: 8317, Train Loss: 0.05838934253825947, Test Loss: 0.053306022487015596\n","Epoch: 8318, Train Loss: 0.05800075648318375, Test Loss: 0.05458402206412122\n","Epoch: 8319, Train Loss: 0.05838352609698775, Test Loss: 0.05330080403079639\n","Epoch: 8320, Train Loss: 0.057995028629291484, Test Loss: 0.05457895583416902\n","Epoch: 8321, Train Loss: 0.0583777043227514, Test Loss: 0.05329558045081222\n","Epoch: 8322, Train Loss: 0.057989295472509056, Test Loss: 0.05457388399158006\n","Epoch: 8323, Train Loss: 0.05837187695644646, Test Loss: 0.053290351487028854\n","Epoch: 8324, Train Loss: 0.05798355675144335, Test Loss: 0.05456880626060126\n","Epoch: 8325, Train Loss: 0.05836604372416591, Test Loss: 0.053285116866436784\n","Epoch: 8326, Train Loss: 0.05797781219174776, Test Loss: 0.05456372235389946\n","Epoch: 8327, Train Loss: 0.05836020434045699, Test Loss: 0.05327987630625722\n","Epoch: 8328, Train Loss: 0.05797206150934082, Test Loss: 0.05455863197589037\n","Epoch: 8329, Train Loss: 0.05835435851162614, Test Loss: 0.05327462951718171\n","Epoch: 8330, Train Loss: 0.05796630441365744, Test Loss: 0.054553534826081225\n","Epoch: 8331, Train Loss: 0.05834850593905885, Test Loss: 0.053269376206606586\n","Epoch: 8332, Train Loss: 0.057960540610894284, Test Loss: 0.05454843060238908\n","Epoch: 8333, Train Loss: 0.05834264632251492, Test Loss: 0.053264116081829196\n","Epoch: 8334, Train Loss: 0.057954769807216464, Test Loss: 0.05454331900440177\n","Epoch: 8335, Train Loss: 0.05833677936336794, Test Loss: 0.05325884885317573\n","Epoch: 8336, Train Loss: 0.057948991711894236, Test Loss: 0.05453819973654959\n","Epoch: 8337, Train Loss: 0.05833090476775624, Test Loss: 0.05325357423702633\n","Epoch: 8338, Train Loss: 0.05794320604033685, Test Loss: 0.054533072511154516\n","Epoch: 8339, Train Loss: 0.05832502224961284, Test Loss: 0.053248291958709185\n","Epoch: 8340, Train Loss: 0.057937412516993615, Test Loss: 0.05452793705132974\n","Epoch: 8341, Train Loss: 0.05831913153354686, Test Loss: 0.05324300175523763\n","Epoch: 8342, Train Loss: 0.0579316108780971, Test Loss: 0.05452279309370102\n","Epoch: 8343, Train Loss: 0.05831323235754877, Test Loss: 0.05323770337786319\n","Epoch: 8344, Train Loss: 0.05792580087422138, Test Loss: 0.05451764039092627\n","Epoch: 8345, Train Loss: 0.05830732447549522, Test Loss: 0.05323239659442204\n","Epoch: 8346, Train Loss: 0.0579199822726321, Test Loss: 0.05451247871399081\n","Epoch: 8347, Train Loss: 0.05830140765943172, Test Loss: 0.05322708119145708\n","Epoch: 8348, Train Loss: 0.05791415485941147, Test Loss: 0.05450730785425977\n","Epoch: 8349, Train Loss: 0.058295481701614034, Test Loss: 0.053221756976095505\n","Epoch: 8350, Train Loss: 0.05790831844133681, Test Loss: 0.05450212762527201\n","Epoch: 8351, Train Loss: 0.05828954641629357, Test Loss: 0.0532164237776704\n","Epoch: 8352, Train Loss: 0.05790247284750295, Test Loss: 0.05449693786426148\n","Epoch: 8353, Train Loss: 0.05828360164123171, Test Loss: 0.05321108144907267\n","Epoch: 8354, Train Loss: 0.05789661793067289, Test Loss: 0.0544917384333966\n","Epoch: 8355, Train Loss: 0.05827764723893503, Test Loss: 0.053205729867827824\n","Epoch: 8356, Train Loss: 0.05789075356835271, Test Loss: 0.05448652922073316\n","Epoch: 8357, Train Loss: 0.05827168309760502, Test Loss: 0.05320036893689018\n","Epoch: 8358, Train Loss: 0.05788487966358259, Test Loss: 0.05448131014087196\n","Epoch: 8359, Train Loss: 0.05826570913179581, Test Loss: 0.053194998585153856\n","Epoch: 8360, Train Loss: 0.057878996145444034, Test Loss: 0.05447608113532837\n","Epoch: 8361, Train Loss: 0.058259725282784926, Test Loss: 0.05318961876768192\n","Epoch: 8362, Train Loss: 0.057873102969283406, Test Loss: 0.054470842172609224\n","Epoch: 8363, Train Loss: 0.05825373151865385, Test Loss: 0.05318422946565569\n","Epoch: 8364, Train Loss: 0.05786720011665565, Test Loss: 0.054465593248007926\n","Epoch: 8365, Train Loss: 0.05824772783408832, Test Loss: 0.05317883068605394\n","Epoch: 8366, Train Loss: 0.05786128759499671, Test Loss: 0.05446033438312296\n","Epoch: 8367, Train Loss: 0.05824171424990428, Test Loss: 0.05317342246106834\n","Epoch: 8368, Train Loss: 0.05785536543703134, Test Loss: 0.05445506562511405\n","Epoch: 8369, Train Loss: 0.0582356908123126, Test Loss: 0.05316800484726964\n","Epoch: 8370, Train Loss: 0.05784943369993128, Test Loss: 0.05444978704570625\n","Epoch: 8371, Train Loss: 0.05822965759193374, Test Loss: 0.053162577924537595\n","Epoch: 8372, Train Loss: 0.05784349246423663, Test Loss: 0.05444449873996164\n","Epoch: 8373, Train Loss: 0.05822361468258109, Test Loss: 0.05315714179477228\n","Epoch: 8374, Train Loss: 0.05783754183255797, Test Loss: 0.05443920082483399\n","Epoch: 8375, Train Loss: 0.058217562199828675, Test Loss: 0.05315169658040285\n","Epoch: 8376, Train Loss: 0.057831581928075804, Test Loss: 0.054433893437526755\n","Epoch: 8377, Train Loss: 0.058211500279383356, Test Loss: 0.053146242422716865\n","Epoch: 8378, Train Loss: 0.057825612892860236, Test Loss: 0.05442857673367851\n","Epoch: 8379, Train Loss: 0.058205429075285045, Test Loss: 0.053140779480030126\n","Epoch: 8380, Train Loss: 0.05781963488603119, Test Loss: 0.05442325088539419\n","Epoch: 8381, Train Loss: 0.05819934875795383, Test Loss: 0.05313530792571873\n","Epoch: 8382, Train Loss: 0.057813648081780654, Test Loss: 0.05441791607915087\n","Epoch: 8383, Train Loss: 0.05819325951211203, Test Loss: 0.053129827946139706\n","Epoch: 8384, Train Loss: 0.05780765266728372, Test Loss: 0.0544125725135991\n","Epoch: 8385, Train Loss: 0.05818716153460203, Test Loss: 0.05312433973846134\n","Epoch: 8386, Train Loss: 0.05780164884051932, Test Loss: 0.05440722039728645\n","Epoch: 8387, Train Loss: 0.058181055032126894, Test Loss: 0.05311884350843052\n","Epoch: 8388, Train Loss: 0.05779563680802862, Test Loss: 0.05440185994633147\n","Epoch: 8389, Train Loss: 0.05817494021894092, Test Loss: 0.05311333946810044\n","Epoch: 8390, Train Loss: 0.057789616782633675, Test Loss: 0.054396491382066474\n","Epoch: 8391, Train Loss: 0.05816881731450934, Test Loss: 0.053107827833542315\n","Epoch: 8392, Train Loss: 0.05778358898114095, Test Loss: 0.054391114928682364\n","Epoch: 8393, Train Loss: 0.0581626865411692, Test Loss: 0.05310230882256814\n","Epoch: 8394, Train Loss: 0.057777553622056145, Test Loss: 0.054385730810895\n","Epoch: 8395, Train Loss: 0.05815654812181183, Test Loss: 0.05309678265248486\n","Epoch: 8396, Train Loss: 0.05777151092333108, Test Loss: 0.054380339251658395\n","Epoch: 8397, Train Loss: 0.05815040227761098, Test Loss: 0.05309124953790591\n","Epoch: 8398, Train Loss: 0.057765461100168794, Test Loss: 0.05437494046994767\n","Epoch: 8399, Train Loss: 0.05814424922582037, Test Loss: 0.05308570968863669\n","Epoch: 8400, Train Loss: 0.05775940436290307, Test Loss: 0.05436953467863274\n","Epoch: 8401, Train Loss: 0.05813808917766064, Test Loss: 0.05308016330766129\n","Epoch: 8402, Train Loss: 0.05775334091497999, Test Loss: 0.05436412208246355\n","Epoch: 8403, Train Loss: 0.05813192233631705, Test Loss: 0.05307461058924173\n","Epoch: 8404, Train Loss: 0.057747270951052886, Test Loss: 0.05435870287618408\n","Epoch: 8405, Train Loss: 0.058125748895064874, Test Loss: 0.05306905171715286\n","Epoch: 8406, Train Loss: 0.05774119465521383, Test Loss: 0.05435327724279433\n","Epoch: 8407, Train Loss: 0.058119569035541045, Test Loss: 0.05306348686306483\n","Epoch: 8408, Train Loss: 0.05773511219937329, Test Loss: 0.05434784535197116\n","Epoch: 8409, Train Loss: 0.058113382926174066, Test Loss: 0.053057916185087144\n","Epoch: 8410, Train Loss: 0.057729023741802464, Test Loss: 0.05434240735866493\n","Epoch: 8411, Train Loss: 0.05810719072078768, Test Loss: 0.05305233982648832\n","Epoch: 8412, Train Loss: 0.05772292942585195, Test Loss: 0.05433696340187879\n","Epoch: 8413, Train Loss: 0.05810099255738621, Test Loss: 0.053046757914595316\n","Epoch: 8414, Train Loss: 0.057716829378850996, Test Loss: 0.054331513603643036\n","Epoch: 8415, Train Loss: 0.05809478855713307, Test Loss: 0.053041170559886455\n","Epoch: 8416, Train Loss: 0.05771072371120124, Test Loss: 0.054326058068187424\n","Epoch: 8417, Train Loss: 0.05808857882352636, Test Loss: 0.05303557785527746\n","Epoch: 8418, Train Loss: 0.05770461251566415, Test Loss: 0.054320596881316095\n","Epoch: 8419, Train Loss: 0.058082363441775156, Test Loss: 0.053029979875606266\n","Epoch: 8420, Train Loss: 0.05769849586684807, Test Loss: 0.054315130109991235\n","Epoch: 8421, Train Loss: 0.05807614247838287, Test Loss: 0.05302437667731862\n","Epoch: 8422, Train Loss: 0.057692373820896974, Test Loss: 0.05430965780211698\n","Epoch: 8423, Train Loss: 0.058069915980930434, Test Loss: 0.0530187682983487\n","Epoch: 8424, Train Loss: 0.05768624641537457, Test Loss: 0.05430417998652923\n","Epoch: 8425, Train Loss: 0.058063683978062754, Test Loss: 0.05301315475819744\n","Epoch: 8426, Train Loss: 0.057680113669346944, Test Loss: 0.0542986966731829\n","Epoch: 8427, Train Loss: 0.058057446479672245, Test Loss: 0.05300753605819825\n","Epoch: 8428, Train Loss: 0.057673975583653406, Test Loss: 0.05429320785353021\n","Epoch: 8429, Train Loss: 0.058051203477271766, Test Loss: 0.053001912181967106\n","Epoch: 8430, Train Loss: 0.057667832141361584, Test Loss: 0.054287713501083225\n","Epoch: 8431, Train Loss: 0.05804495494455055, Test Loss: 0.05299628309602435\n","Epoch: 8432, Train Loss: 0.0576616833083953, Test Loss: 0.05428221357214838\n","Epoch: 8433, Train Loss: 0.05803870083810141, Test Loss: 0.052990648750580925\n","Epoch: 8434, Train Loss: 0.05765552903432697, Test Loss: 0.05427670800672447\n","Epoch: 8435, Train Loss: 0.0580324410983101, Test Loss: 0.052985009080475626\n","Epoch: 8436, Train Loss: 0.05764936925332124, Test Loss: 0.05427119672954872\n","Epoch: 8437, Train Loss: 0.05802617565039219, Test Loss: 0.0529793640062513\n","Epoch: 8438, Train Loss: 0.057643203885218014, Test Loss: 0.054265679651277605\n","Epoch: 8439, Train Loss: 0.058019904405563726, Test Loss: 0.05297371343535364\n","Epoch: 8440, Train Loss: 0.05763703283673813, Test Loss: 0.054260156669787465\n","Epoch: 8441, Train Loss: 0.05801362726233121, Test Loss: 0.05296805726344103\n","Epoch: 8442, Train Loss: 0.05763085600280013, Test Loss: 0.05425462767157749\n","Epoch: 8443, Train Loss: 0.05800734410788337, Test Loss: 0.052962395375784045\n","Epoch: 8444, Train Loss: 0.05762467326792666, Test Loss: 0.05424909253325949\n","Epoch: 8445, Train Loss: 0.05800105481956943, Test Loss: 0.05295672764874406\n","Epoch: 8446, Train Loss: 0.05761848450773023, Test Loss: 0.05424355112311756\n","Epoch: 8447, Train Loss: 0.05799475926644705, Test Loss: 0.05295105395130924\n","Epoch: 8448, Train Loss: 0.057612289590455605, Test Loss: 0.054238003302716574\n","Epoch: 8449, Train Loss: 0.057988457310878885, Test Loss: 0.05294537414667184\n","Epoch: 8450, Train Loss: 0.057606088378564, Test Loss: 0.05423244892854611\n","Epoch: 8451, Train Loss: 0.057982148810164655, Test Loss: 0.052939688093832246\n","Epoch: 8452, Train Loss: 0.057599880730343285, Test Loss: 0.05422688785368019\n","Epoch: 8453, Train Loss: 0.05797583361818921, Test Loss: 0.05293399564920884\n","Epoch: 8454, Train Loss: 0.057593666501523894, Test Loss: 0.05422131992943329\n","Epoch: 8455, Train Loss: 0.05796951158706696, Test Loss: 0.052928296668237375\n","Epoch: 8456, Train Loss: 0.05758744554688371, Test Loss: 0.054215745006997974\n","Epoch: 8457, Train Loss: 0.057963182568768645, Test Loss: 0.052922591006947496\n","Epoch: 8458, Train Loss: 0.05758121772182985, Test Loss: 0.05421016293904844\n","Epoch: 8459, Train Loss: 0.05795684641671402, Test Loss: 0.05291687852349513\n","Epoch: 8460, Train Loss: 0.05757498288393581, Test Loss: 0.05420457358129146\n","Epoch: 8461, Train Loss: 0.0579505029873129, Test Loss: 0.05291115907963967\n","Epoch: 8462, Train Loss: 0.0575687408944227, Test Loss: 0.05419897679395021\n","Epoch: 8463, Train Loss: 0.057944152141439574, Test Loss: 0.05290543254215034\n","Epoch: 8464, Train Loss: 0.05756249161956955, Test Loss: 0.05419337244317061\n","Epoch: 8465, Train Loss: 0.05793779374583049, Test Loss: 0.052899698784129666\n","Epoch: 8466, Train Loss: 0.057556234932039485, Test Loss: 0.05418776040233209\n","Epoch: 8467, Train Loss: 0.05793142767438729, Test Loss: 0.05289395768624053\n","Epoch: 8468, Train Loss: 0.05754997071210967, Test Loss: 0.05418214055325632\n","Epoch: 8469, Train Loss: 0.05792505380937866, Test Loss: 0.05288820913782961\n","Epoch: 8470, Train Loss: 0.05754369884879651, Test Loss: 0.05417651278730079\n","Epoch: 8471, Train Loss: 0.05791867204252763, Test Loss: 0.052882453037933645\n","Epoch: 8472, Train Loss: 0.057537419240863155, Test Loss: 0.05417087700632859\n","Epoch: 8473, Train Loss: 0.05791228227597671, Test Loss: 0.05287668929616452\n","Epoch: 8474, Train Loss: 0.0575311317977054, Test Loss: 0.05416523312354895\n","Epoch: 8475, Train Loss: 0.05790588442312421, Test Loss: 0.05287091783346517\n","Epoch: 8476, Train Loss: 0.057524836440107745, Test Loss: 0.05415958106422188\n","Epoch: 8477, Train Loss: 0.057899478409325994, Test Loss: 0.05286513858273067\n","Epoch: 8478, Train Loss: 0.057518533100863724, Test Loss: 0.05415392076622151\n","Epoch: 8479, Train Loss: 0.05789306417245673, Test Loss: 0.052859351489292244\n","Epoch: 8480, Train Loss: 0.057512221725259285, Test Loss: 0.054148252180457206\n","Epoch: 8481, Train Loss: 0.057886641663330185, Test Loss: 0.05285355651126377\n","Epoch: 8482, Train Loss: 0.057505902271417254, Test Loss: 0.054142575271151495\n","Epoch: 8483, Train Loss: 0.05788021084597729, Test Loss: 0.052847753619745336\n","Epoch: 8484, Train Loss: 0.05749957471049917, Test Loss: 0.054136890015972626\n","Epoch: 8485, Train Loss: 0.057873771697779644, Test Loss: 0.05284194279889162\n","Epoch: 8486, Train Loss: 0.057493239026770636, Test Loss: 0.05413119640602782\n","Epoch: 8487, Train Loss: 0.057867324209464825, Test Loss: 0.05283612404584277\n","Epoch: 8488, Train Loss: 0.057486895217529595, Test Loss: 0.054125494445716196\n","Epoch: 8489, Train Loss: 0.057860868384962315, Test Loss: 0.05283029737052236\n","Epoch: 8490, Train Loss: 0.05748054329290042, Test Loss: 0.054119784152449335\n","Epoch: 8491, Train Loss: 0.05785440424112786, Test Loss: 0.052824462795308766\n","Epoch: 8492, Train Loss: 0.05747418327550182, Test Loss: 0.05411406555624237\n","Epoch: 8493, Train Loss: 0.05784793180733918, Test Loss: 0.05281862035458616\n","Epoch: 8494, Train Loss: 0.057467815199993375, Test Loss: 0.054108338699186714\n","Epoch: 8495, Train Loss: 0.057841451124973985, Test Loss: 0.05281277009418107\n","Epoch: 8496, Train Loss: 0.05746143911250776, Test Loss: 0.05410260363480877\n","Epoch: 8497, Train Loss: 0.057834962246774806, Test Loss: 0.05280691207069414\n","Epoch: 8498, Train Loss: 0.057455055069978105, Test Loss: 0.05409686042732568\n","Epoch: 8499, Train Loss: 0.05782846523611122, Test Loss: 0.05280104635073613\n","Epoch: 8500, Train Loss: 0.057448663139368764, Test Loss: 0.054091109150807434\n","Epoch: 8501, Train Loss: 0.05782196016614923, Test Loss: 0.05279517301007742\n","Epoch: 8502, Train Loss: 0.057442263396820445, Test Loss: 0.054085349888257604\n","Epoch: 8503, Train Loss: 0.05781544711893939, Test Loss: 0.05278929213272453\n","Epoch: 8504, Train Loss: 0.05743585592672113, Test Loss: 0.05407958273062159\n","Epoch: 8505, Train Loss: 0.057808926184432986, Test Loss: 0.052783403809928964\n","Epoch: 8506, Train Loss: 0.057429440820710574, Test Loss: 0.05407380777573581\n","Epoch: 8507, Train Loss: 0.057802397459439436, Test Loss: 0.05277750813914741\n","Epoch: 8508, Train Loss: 0.05742301817663505, Test Loss: 0.05406802512723058\n","Epoch: 8509, Train Loss: 0.0577958610465373, Test Loss: 0.05277160522295998\n","Epoch: 8510, Train Loss: 0.05741658809746109, Test Loss: 0.054062234893396614\n","Epoch: 8511, Train Loss: 0.05778931705294923, Test Loss: 0.052765695167959856\n","Epoch: 8512, Train Loss: 0.05741015069016014, Test Loss: 0.054056437186030676\n","Epoch: 8513, Train Loss: 0.05778276558939583, Test Loss: 0.052759778083627286\n","Epoch: 8514, Train Loss: 0.05740370606457824, Test Loss: 0.05405063211927007\n","Epoch: 8515, Train Loss: 0.05777620676893864, Test Loss: 0.05275385408120023\n","Epoch: 8516, Train Loss: 0.05739725433230234, Test Loss: 0.054044819808430566\n","Epoch: 8517, Train Loss: 0.05776964070582578, Test Loss: 0.052747923272549596\n","Epoch: 8518, Train Loss: 0.057390795605531376, Test Loss: 0.0540390003688565\n","Epoch: 8519, Train Loss: 0.05776306751435003, Test Loss: 0.052741985769076474\n","Epoch: 8520, Train Loss: 0.057384329995969936, Test Loss: 0.05403317391479914\n","Epoch: 8521, Train Loss: 0.057756487307734604, Test Loss: 0.05273604168063719\n","Epoch: 8522, Train Loss: 0.05737785761374971, Test Loss: 0.05402734055832871\n","Epoch: 8523, Train Loss: 0.05774990019705216, Test Loss: 0.05273009111450849\n","Epoch: 8524, Train Loss: 0.0573713785663919, Test Loss: 0.0540215004082957\n","Epoch: 8525, Train Loss: 0.05774330629019273, Test Loss: 0.05272413417440396\n","Epoch: 8526, Train Loss: 0.0573648929578206, Test Loss: 0.05401565356934764\n","Epoch: 8527, Train Loss: 0.05773670569088651, Test Loss: 0.052718170959547755\n","Epoch: 8528, Train Loss: 0.05735840088743466, Test Loss: 0.054009800141010846\n","Epoch: 8529, Train Loss: 0.05773009849779109, Test Loss: 0.052712201563815266\n","Epoch: 8530, Train Loss: 0.05735190244924631, Test Loss: 0.054003940216845694\n","Epoch: 8531, Train Loss: 0.05772348480365153, Test Loss: 0.052706226074949104\n","Epoch: 8532, Train Loss: 0.057345397731095656, Test Loss: 0.05399807388368306\n","Epoch: 8533, Train Loss: 0.05771686469454102, Test Loss: 0.05270024457385486\n","Epoch: 8534, Train Loss: 0.05733888681394562, Test Loss: 0.0539922012209451\n","Epoch: 8535, Train Loss: 0.05771023824918521, Test Loss: 0.05269425713398312\n","Epoch: 8536, Train Loss: 0.05733236977126328, Test Loss: 0.053986322300059536\n","Epoch: 8537, Train Loss: 0.057703605538379145, Test Loss: 0.05268826382080149\n","Epoch: 8538, Train Loss: 0.0573258466684917, Test Loss: 0.05398043718396697\n","Epoch: 8539, Train Loss: 0.057696966624496866, Test Loss: 0.05268226469136133\n","Epoch: 8540, Train Loss: 0.0573193175626172, Test Loss: 0.05397454592672719\n","Epoch: 8541, Train Loss: 0.05769032156109897, Test Loss: 0.052676259793958013\n","Epoch: 8542, Train Loss: 0.05731278250183007, Test Loss: 0.05396864857322609\n","Epoch: 8543, Train Loss: 0.0576836703926402, Test Loss: 0.05267024916789238\n","Epoch: 8544, Train Loss: 0.05730624152528717, Test Loss: 0.05396274515898192\n","Epoch: 8545, Train Loss: 0.05767701315427561, Test Loss: 0.05266423284332618\n","Epoch: 8546, Train Loss: 0.0572996946629689, Test Loss: 0.05395683571005217\n","Epoch: 8547, Train Loss: 0.05767034987176676, Test Loss: 0.05265821084123626\n","Epoch: 8548, Train Loss: 0.057293141935635145, Test Loss: 0.053950920243040025\n","Epoch: 8549, Train Loss: 0.057663680561486685, Test Loss: 0.05265218317346249\n","Epoch: 8550, Train Loss: 0.05728658335487534, Test Loss: 0.05394499876519615\n","Epoch: 8551, Train Loss: 0.057657005230519656, Test Loss: 0.05264614984285002\n","Epoch: 8552, Train Loss: 0.05728001892325311, Test Loss: 0.05393907127461536\n","Epoch: 8553, Train Loss: 0.05765032387685516, Test Loss: 0.05264011084347814\n","Epoch: 8554, Train Loss: 0.057273448634537906, Test Loss: 0.05393313776052134\n","Epoch: 8555, Train Loss: 0.05764363648966942, Test Loss: 0.052634066160974505\n","Epoch: 8556, Train Loss: 0.05726687247402216, Test Loss: 0.05392719820363593\n","Epoch: 8557, Train Loss: 0.05763694304969095, Test Loss: 0.05262801577290981\n","Epoch: 8558, Train Loss: 0.057260290418918995, Test Loss: 0.05392125257662761\n","Epoch: 8559, Train Loss: 0.057630243529644756, Test Loss: 0.052621959649263715\n","Epoch: 8560, Train Loss: 0.057253702438831364, Test Loss: 0.05391530084463097\n","Epoch: 8561, Train Loss: 0.057623537894767256, Test Loss: 0.05261589775295985\n","Epoch: 8562, Train Loss: 0.05724710849629083, Test Loss: 0.053909342965832385\n","Epoch: 8563, Train Loss: 0.057616826103387, Test Loss: 0.05260983004045891\n","Epoch: 8564, Train Loss: 0.057240508547353644, Test Loss: 0.05390337889211324\n","Epoch: 8565, Train Loss: 0.05761010810756252, Test Loss: 0.05260375646240645\n","Epoch: 8566, Train Loss: 0.05723390254225239, Test Loss: 0.05389740856974283\n","Epoch: 8567, Train Loss: 0.05760338385376981, Test Loss: 0.052597676964323804\n","Epoch: 8568, Train Loss: 0.057227290426090476, Test Loss: 0.053891431940112824\n","Epoch: 8569, Train Loss: 0.05759665328363104, Test Loss: 0.052591591487336165\n","Epoch: 8570, Train Loss: 0.057220672139573624, Test Loss: 0.05388544894050516\n","Epoch: 8571, Train Loss: 0.0575899163346767, Test Loss: 0.052585499968929535\n","Epoch: 8572, Train Loss: 0.057214047619770295, Test Loss: 0.05387945950488466\n","Epoch: 8573, Train Loss: 0.05758317294113238, Test Loss: 0.05257940234372699\n","Epoch: 8574, Train Loss: 0.057207416800891406, Test Loss: 0.053873463564706923\n","Epoch: 8575, Train Loss: 0.05757642303472075, Test Loss: 0.0525732985442771\n","Epoch: 8576, Train Loss: 0.05720077961508201, Test Loss: 0.053867461049733764\n","Epoch: 8577, Train Loss: 0.05756966654547118, Test Loss: 0.05256718850184523\n","Epoch: 8578, Train Loss: 0.057194135993215676, Test Loss: 0.05386145188884868\n","Epoch: 8579, Train Loss: 0.05756290340252981, Test Loss: 0.05256107214720099\n","Epoch: 8580, Train Loss: 0.05718748586568476, Test Loss: 0.053855436010861076\n","Epoch: 8581, Train Loss: 0.057556133534958066, Test Loss: 0.05255494941139208\n","Epoch: 8582, Train Loss: 0.057180829163177176, Test Loss: 0.05384941334529385\n","Epoch: 8583, Train Loss: 0.05754935687251509, Test Loss: 0.052548820226497074\n","Epoch: 8584, Train Loss: 0.05717416581743141, Test Loss: 0.053843383823145456\n","Epoch: 8585, Train Loss: 0.05754257334641494, Test Loss: 0.052542684526352246\n","Epoch: 8586, Train Loss: 0.05716749576196579, Test Loss: 0.053837347377621046\n","Epoch: 8587, Train Loss: 0.05753578289005283, Test Loss: 0.05253654224724417\n","Epoch: 8588, Train Loss: 0.05716081893277294, Test Loss: 0.05383130394482298\n","Epoch: 8589, Train Loss: 0.057528985439691836, Test Loss: 0.05253039332855925\n","Epoch: 8590, Train Loss: 0.05715413526897098, Test Loss: 0.053825253464396594\n","Epoch: 8591, Train Loss: 0.05752218093510434, Test Loss: 0.052524237713389856\n","Epoch: 8592, Train Loss: 0.05714744471341117, Test Loss: 0.0538191958801264\n","Epoch: 8593, Train Loss: 0.05751536932016515, Test Loss: 0.05251807534908746\n","Epoch: 8594, Train Loss: 0.057140747213231834, Test Loss: 0.05381313114047492\n","Epoch: 8595, Train Loss: 0.05750855054338709, Test Loss: 0.05251190618776029\n","Epoch: 8596, Train Loss: 0.05713404272035727, Test Loss: 0.053807059199063534\n","Epoch: 8597, Train Loss: 0.05750172455839949, Test Loss: 0.05250573018671086\n","Epoch: 8598, Train Loss: 0.057127331191935325, Test Loss: 0.053800980015087076\n","Epoch: 8599, Train Loss: 0.057494891324360836, Test Loss: 0.05249954730881117\n","Epoch: 8600, Train Loss: 0.057120612590712896, Test Loss: 0.05379489355366703\n","Epoch: 8601, Train Loss: 0.057488050806310254, Test Loss: 0.052493357522814095\n","Epoch: 8602, Train Loss: 0.05711388688534726, Test Loss: 0.053788799786131894\n","Epoch: 8603, Train Loss: 0.057481202975447, Test Loss: 0.05248716080359475\n","Epoch: 8604, Train Loss: 0.05710715405064708, Test Loss: 0.05378269869023016\n","Epoch: 8605, Train Loss: 0.05747434780934292, Test Loss: 0.05248095713232684\n","Epoch: 8606, Train Loss: 0.05710041406774773, Test Loss: 0.05377659025027306\n","Epoch: 8607, Train Loss: 0.05746748529208516, Test Loss: 0.05247474649659033\n","Epoch: 8608, Train Loss: 0.05709366692421844, Test Loss: 0.05377047445720806\n","Epoch: 8609, Train Loss: 0.057460615414350116, Test Loss: 0.0524685288904112\n","Epoch: 8610, Train Loss: 0.0570869126141005, Test Loss: 0.05376435130862118\n","Epoch: 8611, Train Loss: 0.05745373817340675, Test Loss: 0.0524623043142349\n","Epoch: 8612, Train Loss: 0.057080151137879315, Test Loss: 0.05375822080867432\n","Epoch: 8613, Train Loss: 0.05744685357305522, Test Loss: 0.05245607277483553\n","Epoch: 8614, Train Loss: 0.05707338250239198, Test Loss: 0.053752082967975574\n","Epoch: 8615, Train Loss: 0.057439961623499, Test Loss: 0.05244983428516265\n","Epoch: 8616, Train Loss: 0.057066606720672305, Test Loss: 0.05374593780338762\n","Epoch: 8617, Train Loss: 0.057433062341155645, Test Loss: 0.0524435888641287\n","Epoch: 8618, Train Loss: 0.05705982381173593, Test Loss: 0.05373978533777722\n","Epoch: 8619, Train Loss: 0.05742615574840865, Test Loss: 0.052437336536341514\n","Epoch: 8620, Train Loss: 0.05705303380031087, Test Loss: 0.05373362559970893\n","Epoch: 8621, Train Loss: 0.05741924187330407, Test Loss: 0.05243107733178351\n","Epoch: 8622, Train Loss: 0.05704623671651443, Test Loss: 0.053727458623088066\n","Epoch: 8623, Train Loss: 0.057412320749196936, Test Loss: 0.052424811285445384\n","Epoch: 8624, Train Loss: 0.05703943259548435, Test Loss: 0.053721284446758326\n","Epoch: 8625, Train Loss: 0.05740539241435201, Test Loss: 0.05241853843691659\n","Epoch: 8626, Train Loss: 0.05703262147696681, Test Loss: 0.05371510311405762\n","Epoch: 8627, Train Loss: 0.057398456911503346, Test Loss: 0.052412258829938045\n","Epoch: 8628, Train Loss: 0.05702580340486689, Test Loss: 0.05370891467233849\n","Epoch: 8629, Train Loss: 0.057391514287378764, Test Loss: 0.05240597251192424\n","Epoch: 8630, Train Loss: 0.05701897842676776, Test Loss: 0.053702719172459654\n","Epoch: 8631, Train Loss: 0.057384564592195104, Test Loss: 0.052399679533456874\n","Epoch: 8632, Train Loss: 0.05701214659342205, Test Loss: 0.053696516668252404\n","Epoch: 8633, Train Loss: 0.057377607879128587, Test Loss: 0.05239337994776018\n","Epoch: 8634, Train Loss: 0.057005307958224664, Test Loss: 0.05369030721596933\n","Epoch: 8635, Train Loss: 0.05737064420376806, Test Loss: 0.05238707381015955\n","Epoch: 8636, Train Loss: 0.05699846257666891, Test Loss: 0.05368409087372174\n","Epoch: 8637, Train Loss: 0.05736367362355604, Test Loss: 0.052380761177532764\n","Epoch: 8638, Train Loss: 0.05699161050579538, Test Loss: 0.053677867700910105\n","Epoch: 8639, Train Loss: 0.0573566961972234, Test Loss: 0.052374442107756124\n","Epoch: 8640, Train Loss: 0.05698475180363594, Test Loss: 0.05367163775765493\n","Epoch: 8641, Train Loss: 0.05734971198422424, Test Loss: 0.0523681166591551\n","Epoch: 8642, Train Loss: 0.056977886528662154, Test Loss: 0.05366540110423328\n","Epoch: 8643, Train Loss: 0.057342721044176155, Test Loss: 0.05236178488996133\n","Epoch: 8644, Train Loss: 0.05697101473924023, Test Loss: 0.05365915780052572\n","Epoch: 8645, Train Loss: 0.057335723436310895, Test Loss: 0.05235544685778315\n","Epoch: 8646, Train Loss: 0.05696413649310003, Test Loss: 0.053652907905481474\n","Epoch: 8647, Train Loss: 0.057328719218943114, Test Loss: 0.05234910261909582\n","Epoch: 8648, Train Loss: 0.05695725184682326, Test Loss: 0.053646651476603725\n","Epoch: 8649, Train Loss: 0.05732170844895884, Test Loss: 0.05234275222875339\n","Epoch: 8650, Train Loss: 0.05695036085535417, Test Loss: 0.05364038856946131\n","Epoch: 8651, Train Loss: 0.05731469118133016, Test Loss: 0.05233639573952676\n","Epoch: 8652, Train Loss: 0.05694346357153615, Test Loss: 0.053634119237231534\n","Epoch: 8653, Train Loss: 0.057307667468660915, Test Loss: 0.052330033201677416\n","Epoch: 8654, Train Loss: 0.05693656004568465, Test Loss: 0.05362784353027751\n","Epoch: 8655, Train Loss: 0.05730063736076643, Test Loss: 0.05232366466256168\n","Epoch: 8656, Train Loss: 0.05692965032519029, Test Loss: 0.05362156149576386\n","Epoch: 8657, Train Loss: 0.05729360090429139, Test Loss: 0.052317290166276724\n","Epoch: 8658, Train Loss: 0.056922734454164355, Test Loss: 0.053615273177312114\n","Epoch: 8659, Train Loss: 0.05728655814236699, Test Loss: 0.05231090975334487\n","Epoch: 8660, Train Loss: 0.05691581247312285, Test Loss: 0.053608978614701215\n","Epoch: 8661, Train Loss: 0.05727950911431301, Test Loss: 0.052304523460443446\n","Epoch: 8662, Train Loss: 0.05690888441871589, Test Loss: 0.05360267784361291\n","Epoch: 8663, Train Loss: 0.05727245385538434, Test Loss: 0.05229813132017758\n","Epoch: 8664, Train Loss: 0.056901950323500594, Test Loss: 0.05359637089542408\n","Epoch: 8665, Train Loss: 0.05726539239656392, Test Loss: 0.052291733360900705\n","Epoch: 8666, Train Loss: 0.05689501021576196, Test Loss: 0.05359005779704759\n","Epoch: 8667, Train Loss: 0.05725832476440439, Test Loss: 0.05228532960658158\n","Epoch: 8668, Train Loss: 0.056888064119380036, Test Loss: 0.05358373857082196\n","Epoch: 8669, Train Loss: 0.05725125098091742, Test Loss: 0.05227892007671778\n","Epoch: 8670, Train Loss: 0.056881112053744326, Test Loss: 0.05357741323444909\n","Epoch: 8671, Train Loss: 0.05724417106351148, Test Loss: 0.05227250478629807\n","Epoch: 8672, Train Loss: 0.05687415403371673, Test Loss: 0.05357108180098071\n","Epoch: 8673, Train Loss: 0.057237085024977694, Test Loss: 0.05226608374580863\n","Epoch: 8674, Train Loss: 0.056867190069638834, Test Loss: 0.05356474427885107\n","Epoch: 8675, Train Loss: 0.057229992873521526, Test Loss: 0.05225965696128653\n","Epoch: 8676, Train Loss: 0.056860220167386545, Test Loss: 0.053558400671956885\n","Epoch: 8677, Train Loss: 0.057222894612841475, Test Loss: 0.052253224434416344\n","Epoch: 8678, Train Loss: 0.056853244328467854, Test Loss: 0.05355205097978\n","Epoch: 8679, Train Loss: 0.057215790242250604, Test Loss: 0.05224678616266715\n","Epoch: 8680, Train Loss: 0.05684626255016141, Test Loss: 0.05354569519755162\n","Epoch: 8681, Train Loss: 0.05720867975683872, Test Loss: 0.05224034213947068\n","Epoch: 8682, Train Loss: 0.05683927482569604, Test Loss: 0.0535393333164567\n","Epoch: 8683, Train Loss: 0.05720156314767487, Test Loss: 0.052233892354434416\n","Epoch: 8684, Train Loss: 0.05683228114446556, Test Loss: 0.05353296532387314\n","Epoch: 8685, Train Loss: 0.05719444040204445, Test Loss: 0.05222743679359018\n","Epoch: 8686, Train Loss: 0.0568252814922789, Test Loss: 0.053526591203644724\n","Epoch: 8687, Train Loss: 0.05718731150371982, Test Loss: 0.052220975439670705\n","Epoch: 8688, Train Loss: 0.05681827585163856, Test Loss: 0.05352021093638324\n","Epoch: 8689, Train Loss: 0.05718017643325971, Test Loss: 0.052214508272415625\n","Epoch: 8690, Train Loss: 0.05681126420204819, Test Loss: 0.053513824499796074\n","Epoch: 8691, Train Loss: 0.05717303516833431, Test Loss: 0.05220803526889821\n","Epoch: 8692, Train Loss: 0.0568042465203414, Test Loss: 0.053507431869035296\n","Epoch: 8693, Train Loss: 0.05716588768407171, Test Loss: 0.05220155640387376\n","Epoch: 8694, Train Loss: 0.056797222781031644, Test Loss: 0.05350103301706497\n","Epoch: 8695, Train Loss: 0.05715873395342229, Test Loss: 0.05219507165014092\n","Epoch: 8696, Train Loss: 0.05679019295667562, Test Loss: 0.053494627915042\n","Epoch: 8697, Train Loss: 0.05715157394753669, Test Loss: 0.05218858097891684\n","Epoch: 8698, Train Loss: 0.056783157018249736, Test Loss: 0.05348821653270624\n","Epoch: 8699, Train Loss: 0.05714440763615312, Test Loss: 0.052182084360217755\n","Epoch: 8700, Train Loss: 0.05677611493553293, Test Loss: 0.053481798838775726\n","Epoch: 8701, Train Loss: 0.057137234987989886, Test Loss: 0.05217558176324428\n","Epoch: 8702, Train Loss: 0.0567690666774932, Test Loss: 0.05347537480134432\n","Epoch: 8703, Train Loss: 0.05713005597114003, Test Loss: 0.052169073156765874\n","Epoch: 8704, Train Loss: 0.05676201221267374, Test Loss: 0.05346894438827538\n","Epoch: 8705, Train Loss: 0.057122870553462504, Test Loss: 0.05216255850949964\n","Epoch: 8706, Train Loss: 0.05675495150957313, Test Loss: 0.05346250756758903\n","Epoch: 8707, Train Loss: 0.057115678702966695, Test Loss: 0.05215603779048349\n","Epoch: 8708, Train Loss: 0.056747884537020024, Test Loss: 0.05345606430783967\n","Epoch: 8709, Train Loss: 0.05710848038818746, Test Loss: 0.05214951096943464\n","Epoch: 8710, Train Loss: 0.0567408112645327, Test Loss: 0.05344961457847856\n","Epoch: 8711, Train Loss: 0.057101275578545496, Test Loss: 0.05214297801709597\n","Epoch: 8712, Train Loss: 0.05673373166266659, Test Loss: 0.0534431583502003\n","Epoch: 8713, Train Loss: 0.057094064244691524, Test Loss: 0.05213643890556202\n","Epoch: 8714, Train Loss: 0.056726645703341176, Test Loss: 0.05343669559526787\n","Epoch: 8715, Train Loss: 0.05708684635882946, Test Loss: 0.05212989360858556\n","Epoch: 8716, Train Loss: 0.05671955336014754, Test Loss: 0.05343022628781533\n","Epoch: 8717, Train Loss: 0.05707962189501717, Test Loss: 0.05212334210185972\n","Epoch: 8718, Train Loss: 0.05671245460863099, Test Loss: 0.053423750404124795\n","Epoch: 8719, Train Loss: 0.057072390829442134, Test Loss: 0.05211678436327412\n","Epoch: 8720, Train Loss: 0.05670534942654777, Test Loss: 0.05341726792287462\n","Epoch: 8721, Train Loss: 0.057065153140667964, Test Loss: 0.05211022037314355\n","Epoch: 8722, Train Loss: 0.056698237794094226, Test Loss: 0.05341077882535991\n","Epoch: 8723, Train Loss: 0.057057908809854056, Test Loss: 0.05210365011440867\n","Epoch: 8724, Train Loss: 0.05669111969410761, Test Loss: 0.0534042830956799\n","Epoch: 8725, Train Loss: 0.05705065782094198, Test Loss: 0.052097073572802056\n","Epoch: 8726, Train Loss: 0.05668399511223213, Test Loss: 0.05339778072089425\n","Epoch: 8727, Train Loss: 0.05704340016081126, Test Loss: 0.052090490736986746\n","Epoch: 8728, Train Loss: 0.05667686403705759, Test Loss: 0.05339127169114478\n","Epoch: 8729, Train Loss: 0.057036135819400424, Test Loss: 0.052083901598659006\n","Epoch: 8730, Train Loss: 0.05666972646022182, Test Loss: 0.053384755999746264\n","Epoch: 8731, Train Loss: 0.057028864789798024, Test Loss: 0.05207730615262117\n","Epoch: 8732, Train Loss: 0.05666258237648319, Test Loss: 0.05337823364323997\n","Epoch: 8733, Train Loss: 0.05702158706829599, Test Loss: 0.052070704396819704\n","Epoch: 8734, Train Loss: 0.05665543178375809, Test Loss: 0.05337170462141671\n","Epoch: 8735, Train Loss: 0.057014302654413165, Test Loss: 0.052064096332351435\n","Epoch: 8736, Train Loss: 0.05664827468312663, Test Loss: 0.05336516893730457\n","Epoch: 8737, Train Loss: 0.057007011550883405, Test Loss: 0.05205748196343972\n","Epoch: 8738, Train Loss: 0.05664111107880808, Test Loss: 0.05335862659712794\n","Epoch: 8739, Train Loss: 0.05699971376361553, Test Loss: 0.052050861297377606\n","Epoch: 8740, Train Loss: 0.056633940978103044, Test Loss: 0.05335207761023016\n","Epoch: 8741, Train Loss: 0.05699240930161679, Test Loss: 0.0520442343444428\n","Epoch: 8742, Train Loss: 0.05662676439130767, Test Loss: 0.05334552198897254\n","Epoch: 8743, Train Loss: 0.056985098176893074, Test Loss: 0.052037601117786\n","Epoch: 8744, Train Loss: 0.05661958133160077, Test Loss: 0.05333895974860238\n","Epoch: 8745, Train Loss: 0.0569777804043182, Test Loss: 0.052030961633290185\n","Epoch: 8746, Train Loss: 0.0566123918149022, Test Loss: 0.05333239090709526\n","Epoch: 8747, Train Loss: 0.05697045600147768, Test Loss: 0.052024315909408515\n","Epoch: 8748, Train Loss: 0.056605195859709356, Test Loss: 0.053325815484974864\n","Epoch: 8749, Train Loss: 0.056963124988490134, Test Loss: 0.05201766396697913\n","Epoch: 8750, Train Loss: 0.05659799348691101, Test Loss: 0.05331923350511103\n","Epoch: 8751, Train Loss: 0.056955787387807134, Test Loss: 0.05201100582902062\n","Epoch: 8752, Train Loss: 0.056590784719581406, Test Loss: 0.05331264499249875\n","Epoch: 8753, Train Loss: 0.05694844322399369, Test Loss: 0.05200434152051055\n","Epoch: 8754, Train Loss: 0.05658356958275754, Test Loss: 0.053306049974021094\n","Epoch: 8755, Train Loss: 0.05694109252349347, Test Loss: 0.051997671068148885\n","Epoch: 8756, Train Loss: 0.05657634810320128, Test Loss: 0.053299448478199535\n","Epoch: 8757, Train Loss: 0.0569337353143807, Test Loss: 0.05199099450011045\n","Epoch: 8758, Train Loss: 0.056569120309150664, Test Loss: 0.0532928405349327\n","Epoch: 8759, Train Loss: 0.0569263716261011, Test Loss: 0.051984311845788805\n","Epoch: 8760, Train Loss: 0.0565618862300624, Test Loss: 0.053286226175227415\n","Epoch: 8761, Train Loss: 0.05691900148920485, Test Loss: 0.051977623135531656\n","Epoch: 8762, Train Loss: 0.056554645896346116, Test Loss: 0.0532796054309259\n","Epoch: 8763, Train Loss: 0.05691162493507568, Test Loss: 0.05197092840037582\n","Epoch: 8764, Train Loss: 0.05654739933909825, Test Loss: 0.05327297833442959\n","Epoch: 8765, Train Loss: 0.056904241995656746, Test Loss: 0.05196422767177842\n","Epoch: 8766, Train Loss: 0.056540146589831766, Test Loss: 0.05326634491842442\n","Epoch: 8767, Train Loss: 0.05689685270317779, Test Loss: 0.05195752098135274\n","Epoch: 8768, Train Loss: 0.05653288768021138, Test Loss: 0.05325970521561017\n","Epoch: 8769, Train Loss: 0.05688945708988621, Test Loss: 0.0519508083606066\n","Epoch: 8770, Train Loss: 0.05652562264179069, Test Loss: 0.05325305925843383\n","Epoch: 8771, Train Loss: 0.05688205518778235, Test Loss: 0.05194408984068884\n","Epoch: 8772, Train Loss: 0.056518351505757666, Test Loss: 0.053246407078834386\n","Epoch: 8773, Train Loss: 0.05687464702836597, Test Loss: 0.05193736545214455\n","Epoch: 8774, Train Loss: 0.05651107430268933, Test Loss: 0.053239748707995624\n","Epoch: 8775, Train Loss: 0.05686723264239033, Test Loss: 0.051930635224681866\n","Epoch: 8776, Train Loss: 0.05650379106231749, Test Loss: 0.053233084176114345\n","Epoch: 8777, Train Loss: 0.05685981205963223, Test Loss: 0.05192389918695368\n","Epoch: 8778, Train Loss: 0.05649650181331005, Test Loss: 0.05322641351218272\n","Epoch: 8779, Train Loss: 0.05685238530867539, Test Loss: 0.05191715736635287\n","Epoch: 8780, Train Loss: 0.056489206583065506, Test Loss: 0.0532197367437875\n","Epoch: 8781, Train Loss: 0.056844952416710906, Test Loss: 0.051910409788826634\n","Epoch: 8782, Train Loss: 0.05648190539752709, Test Loss: 0.05321305389692843\n","Epoch: 8783, Train Loss: 0.05683751340935667, Test Loss: 0.051903656478707816\n","Epoch: 8784, Train Loss: 0.0564745982810133, Test Loss: 0.053206364995855404\n","Epoch: 8785, Train Loss: 0.05683006831049546, Test Loss: 0.05189689745856686\n","Epoch: 8786, Train Loss: 0.05646728525607034, Test Loss: 0.05319967006292824\n","Epoch: 8787, Train Loss: 0.05682261714213523, Test Loss: 0.05189013274908529\n","Epoch: 8788, Train Loss: 0.05645996634334481, Test Loss: 0.053192969118497316\n","Epoch: 8789, Train Loss: 0.056815159924290405, Test Loss: 0.05188336236894983\n","Epoch: 8790, Train Loss: 0.05645264156147816, Test Loss: 0.05318626218080715\n","Epoch: 8791, Train Loss: 0.05680769667488576, Test Loss: 0.05187658633476866\n","Epoch: 8792, Train Loss: 0.056445310927022894, Test Loss: 0.05317954926592363\n","Epoch: 8793, Train Loss: 0.056800227409683914, Test Loss: 0.051869804661011376\n","Epoch: 8794, Train Loss: 0.05643797445438302, Test Loss: 0.05317283038768326\n","Epoch: 8795, Train Loss: 0.05679275214223442, Test Loss: 0.05186301735996994\n","Epoch: 8796, Train Loss: 0.056430632155774846, Test Loss: 0.053166105557666474\n","Epoch: 8797, Train Loss: 0.05678527088384719, Test Loss: 0.05185622444174361\n","Epoch: 8798, Train Loss: 0.056423284041212544, Test Loss: 0.053159374785194904\n","Epoch: 8799, Train Loss: 0.0567777836435893, Test Loss: 0.05184942591424514\n","Epoch: 8800, Train Loss: 0.05641593011851495, Test Loss: 0.053152638077349185\n","Epoch: 8801, Train Loss: 0.056770290428302346, Test Loss: 0.05184262178322835\n","Epoch: 8802, Train Loss: 0.05640857039333339, Test Loss: 0.05314589543900969\n","Epoch: 8803, Train Loss: 0.056762791242642635, Test Loss: 0.051835812052337366\n","Epoch: 8804, Train Loss: 0.056401204869201776, Test Loss: 0.05313914687291785\n","Epoch: 8805, Train Loss: 0.05675528608914193, Test Loss: 0.05182899672317416\n","Epoch: 8806, Train Loss: 0.05639383354760466, Test Loss: 0.05313239237975715\n","Epoch: 8807, Train Loss: 0.05674777496828732, Test Loss: 0.05182217579538596\n","Epoch: 8808, Train Loss: 0.05638645642806556, Test Loss: 0.05312563195825297\n","Epoch: 8809, Train Loss: 0.05674025787862028, Test Loss: 0.05181534926677026\n","Epoch: 8810, Train Loss: 0.05637907350825261, Test Loss: 0.053118865605289445\n","Epoch: 8811, Train Loss: 0.0567327348168526, Test Loss: 0.051808517133394044\n","Epoch: 8812, Train Loss: 0.056371684784098736, Test Loss: 0.05311209331604169\n","Epoch: 8813, Train Loss: 0.05672520577799716, Test Loss: 0.051801679389729606\n","Epoch: 8814, Train Loss: 0.05636429024993825, Test Loss: 0.05310531508412148\n","Epoch: 8815, Train Loss: 0.05671767075551267, Test Loss: 0.051794836028800736\n","Epoch: 8816, Train Loss: 0.056356889898653964, Test Loss: 0.05309853090173618\n","Epoch: 8817, Train Loss: 0.05671012974146121, Test Loss: 0.051787987042341964\n","Epoch: 8818, Train Loss: 0.05634948372183733, Test Loss: 0.053091740759857134\n","Epoch: 8819, Train Loss: 0.05670258272667532, Test Loss: 0.0517811324209661\n","Epoch: 8820, Train Loss: 0.05634207170995687, Test Loss: 0.05308494464839638\n","Epoch: 8821, Train Loss: 0.056695029700933375, Test Loss: 0.051774272154337844\n","Epoch: 8822, Train Loss: 0.05633465385253265, Test Loss: 0.05307814255639002\n","Epoch: 8823, Train Loss: 0.05668747065314164, Test Loss: 0.05176740623135514\n","Epoch: 8824, Train Loss: 0.05632723013831862, Test Loss: 0.0530713344721865\n","Epoch: 8825, Train Loss: 0.05667990557152105, Test Loss: 0.05176053464033214\n","Epoch: 8826, Train Loss: 0.0563198005554863, Test Loss: 0.0530645203836363\n","Epoch: 8827, Train Loss: 0.056672334443795686, Test Loss: 0.05175365736918438\n","Epoch: 8828, Train Loss: 0.05631236509181083, Test Loss: 0.053057700278282945\n","Epoch: 8829, Train Loss: 0.056664757257382306, Test Loss: 0.0517467744056131\n","Epoch: 8830, Train Loss: 0.05630492373485615, Test Loss: 0.05305087414355275\n","Epoch: 8831, Train Loss: 0.056657173999578994, Test Loss: 0.051739885737289035\n","Epoch: 8832, Train Loss: 0.056297476472159436, Test Loss: 0.05304404196694104\n","Epoch: 8833, Train Loss: 0.05664958465774987, Test Loss: 0.051732991352030466\n","Epoch: 8834, Train Loss: 0.056290023291409884, Test Loss: 0.053037203736193646\n","Epoch: 8835, Train Loss: 0.056641989219505626, Test Loss: 0.051726091237976786\n","Epoch: 8836, Train Loss: 0.056282564180622885, Test Loss: 0.05303035943948225\n","Epoch: 8837, Train Loss: 0.05663438767287757, Test Loss: 0.05171918538375496\n","Epoch: 8838, Train Loss: 0.05627509912830716, Test Loss: 0.05302350906556986\n","Epoch: 8839, Train Loss: 0.05662678000648219, Test Loss: 0.05171227377863627\n","Epoch: 8840, Train Loss: 0.05626762812362188, Test Loss: 0.05301665260396907\n","Epoch: 8841, Train Loss: 0.05661916620967833, Test Loss: 0.05170535641268536\n","Epoch: 8842, Train Loss: 0.05626015115652623, Test Loss: 0.05300979004508732\n","Epoch: 8843, Train Loss: 0.05661154627271169, Test Loss: 0.051698433276895564\n","Epoch: 8844, Train Loss: 0.056252668217915046, Test Loss: 0.053002921380361494\n","Epoch: 8845, Train Loss: 0.05660392018684864, Test Loss: 0.05169150436331358\n","Epoch: 8846, Train Loss: 0.05624517929974387, Test Loss: 0.05299604660237789\n","Epoch: 8847, Train Loss: 0.05659628794449547, Test Loss: 0.05168456966515031\n","Epoch: 8848, Train Loss: 0.05623768439513993, Test Loss: 0.052989165704979586\n","Epoch: 8849, Train Loss: 0.05658864953930528, Test Loss: 0.05167762917687882\n","Epoch: 8850, Train Loss: 0.05623018349850026, Test Loss: 0.052982278683358655\n","Epoch: 8851, Train Loss: 0.05658100496626973, Test Loss: 0.05167068289431593\n","Epoch: 8852, Train Loss: 0.056222676605573396, Test Loss: 0.05297538553413138\n","Epoch: 8853, Train Loss: 0.056573354221793944, Test Loss: 0.051663730814689224\n","Epoch: 8854, Train Loss: 0.05621516371352629, Test Loss: 0.05296848625540073\n","Epoch: 8855, Train Loss: 0.05656569730375875, Test Loss: 0.05165677293669064\n","Epoch: 8856, Train Loss: 0.05620764482099793, Test Loss: 0.05296158084680019\n","Epoch: 8857, Train Loss: 0.0565580342115645, Test Loss: 0.051649809260511506\n","Epoch: 8858, Train Loss: 0.05620011992813413, Test Loss: 0.05295466930952332\n","Epoch: 8859, Train Loss: 0.05655036494616063, Test Loss: 0.05164283978786408\n","Epoch: 8860, Train Loss: 0.05619258903660887, Test Loss: 0.05294775164633558\n","Epoch: 8861, Train Loss: 0.05654268951005743, Test Loss: 0.05163586452198652\n","Epoch: 8862, Train Loss: 0.056185052149629114, Test Loss: 0.05294082786157272\n","Epoch: 8863, Train Loss: 0.05653500790732499, Test Loss: 0.05162888346763425\n","Epoch: 8864, Train Loss: 0.05617750927192552, Test Loss: 0.0529338979611219\n","Epoch: 8865, Train Loss: 0.05652732014357437, Test Loss: 0.05162189663105469\n","Epoch: 8866, Train Loss: 0.056169960409727125, Test Loss: 0.05292696195238852\n","Epoch: 8867, Train Loss: 0.05651962622592523, Test Loss: 0.05161490401994991\n","Epoch: 8868, Train Loss: 0.05616240557072341, Test Loss: 0.05292001984425043\n","Epoch: 8869, Train Loss: 0.056511926162960024, Test Loss: 0.05160790564342432\n","Epoch: 8870, Train Loss: 0.05615484476401135, Test Loss: 0.05291307164699602\n","Epoch: 8871, Train Loss: 0.05650421996466315, Test Loss: 0.05160090151192104\n","Epoch: 8872, Train Loss: 0.05614727800003153, Test Loss: 0.05290611737225255\n","Epoch: 8873, Train Loss: 0.05649650764234991, Test Loss: 0.051593891637145575\n","Epoch: 8874, Train Loss: 0.056139705290491004, Test Loss: 0.05289915703290112\n","Epoch: 8875, Train Loss: 0.05648878920858199, Test Loss: 0.05158687603197975\n","Epoch: 8876, Train Loss: 0.056132126648276784, Test Loss: 0.05289219064298274\n","Epoch: 8877, Train Loss: 0.05648106467707444, Test Loss: 0.05157985471038569\n","Epoch: 8878, Train Loss: 0.05612454208735924, Test Loss: 0.05288521821759474\n","Epoch: 8879, Train Loss: 0.056473334062593056, Test Loss: 0.05157282768730148\n","Epoch: 8880, Train Loss: 0.05611695162268697, Test Loss: 0.05287823977277807\n","Epoch: 8881, Train Loss: 0.056465597380842454, Test Loss: 0.051565794978529894\n","Epoch: 8882, Train Loss: 0.056109355270075045, Test Loss: 0.052871255325400206\n","Epoch: 8883, Train Loss: 0.056457854648349896, Test Loss: 0.051558756600620724\n","Epoch: 8884, Train Loss: 0.056101753046086646, Test Loss: 0.05286426489303054\n","Epoch: 8885, Train Loss: 0.056450105882341645, Test Loss: 0.05155171257074936\n","Epoch: 8886, Train Loss: 0.05609414496791103, Test Loss: 0.0528572684938136\n","Epoch: 8887, Train Loss: 0.05644235110061707, Test Loss: 0.05154466290659112\n","Epoch: 8888, Train Loss: 0.05608653105323723, Test Loss: 0.052850266146338074\n","Epoch: 8889, Train Loss: 0.05643459032141878, Test Loss: 0.05153760762619452\n","Epoch: 8890, Train Loss: 0.056078911320126705, Test Loss: 0.0528432578695052\n","Epoch: 8891, Train Loss: 0.05642682356330166, Test Loss: 0.05153054674785259\n","Epoch: 8892, Train Loss: 0.05607128578688415, Test Loss: 0.05283624368239741\n","Epoch: 8893, Train Loss: 0.05641905084500264, Test Loss: 0.05152348028997636\n","Epoch: 8894, Train Loss: 0.05606365447193034, Test Loss: 0.052829223604147754\n","Epoch: 8895, Train Loss: 0.056411272185311105, Test Loss: 0.05151640827096933\n","Epoch: 8896, Train Loss: 0.056056017393676126, Test Loss: 0.05282219765381201\n","Epoch: 8897, Train Loss: 0.056403487602941636, Test Loss: 0.05150933070910389\n","Epoch: 8898, Train Loss: 0.056048374570398414, Test Loss: 0.052815165850244615\n","Epoch: 8899, Train Loss: 0.05639569711641077, Test Loss: 0.05150224762240381\n","Epoch: 8900, Train Loss: 0.05604072602012208, Test Loss: 0.05280812821197896\n","Epoch: 8901, Train Loss: 0.0563879007439182, Test Loss: 0.051495159028530524\n","Epoch: 8902, Train Loss: 0.056033071760506, Test Loss: 0.05280108475711445\n","Epoch: 8903, Train Loss: 0.0563800985032345, Test Loss: 0.051488064944675664\n","Epoch: 8904, Train Loss: 0.05602541180873514, Test Loss: 0.05279403550320925\n","Epoch: 8905, Train Loss: 0.05637229041159445, Test Loss: 0.05148096538746199\n","Epoch: 8906, Train Loss: 0.05601774618142125, Test Loss: 0.05278698046718285\n","Epoch: 8907, Train Loss: 0.05636447648560018, Test Loss: 0.05147386037285022\n","Epoch: 8908, Train Loss: 0.056010074894509314, Test Loss: 0.052779919665223746\n","Epoch: 8909, Train Loss: 0.056356656741129496, Test Loss: 0.05146674991605546\n","Epoch: 8910, Train Loss: 0.056002397963193964, Test Loss: 0.052772853112709954\n","Epoch: 8911, Train Loss: 0.05634883119325669, Test Loss: 0.051459634031473196\n","Epoch: 8912, Train Loss: 0.05599471540184514, Test Loss: 0.052765780824136695\n","Epoch: 8913, Train Loss: 0.05634099985618064, Test Loss: 0.05145251273261439\n","Epoch: 8914, Train Loss: 0.05598702722394321, Test Loss: 0.052758702813056134\n","Epoch: 8915, Train Loss: 0.05633316274316475, Test Loss: 0.05144538603205056\n","Epoch: 8916, Train Loss: 0.055979333442023986, Test Loss: 0.05275161909202623\n","Epoch: 8917, Train Loss: 0.05632531986648625, Test Loss: 0.051438253941370145\n","Epoch: 8918, Train Loss: 0.05597163406763506, Test Loss: 0.0527445296725716\n","Epoch: 8919, Train Loss: 0.056317471237396825, Test Loss: 0.05143111647114475\n","Epoch: 8920, Train Loss: 0.05596392911130208, Test Loss: 0.05273743456515444\n","Epoch: 8921, Train Loss: 0.05630961686609385, Test Loss: 0.0514239736309052\n","Epoch: 8922, Train Loss: 0.055956218582505116, Test Loss: 0.05273033377915675\n","Epoch: 8923, Train Loss: 0.05630175676170256, Test Loss: 0.05141682542913062\n","Epoch: 8924, Train Loss: 0.05594850248966759, Test Loss: 0.05272322732287466\n","Epoch: 8925, Train Loss: 0.05629389093227008, Test Loss: 0.051409671873247076\n","Epoch: 8926, Train Loss: 0.055940780840155274, Test Loss: 0.052716115203521575\n","Epoch: 8927, Train Loss: 0.05628601938476875, Test Loss: 0.05140251296963386\n","Epoch: 8928, Train Loss: 0.05593305364028296, Test Loss: 0.05270899742724251\n","Epoch: 8929, Train Loss: 0.05627814212510973, Test Loss: 0.051395348723644416\n","Epoch: 8930, Train Loss: 0.055925320895335404, Test Loss: 0.05270187399913945\n","Epoch: 8931, Train Loss: 0.05627025915816853, Test Loss: 0.05138817913963309\n","Epoch: 8932, Train Loss: 0.05591758260959451, Test Loss: 0.052694744923304636\n","Epoch: 8933, Train Loss: 0.05626237048781756, Test Loss: 0.051381004220993\n","Epoch: 8934, Train Loss: 0.055909838786377675, Test Loss: 0.052687610202864506\n","Epoch: 8935, Train Loss: 0.056254476116969925, Test Loss: 0.051373823970201314\n","Epoch: 8936, Train Loss: 0.05590208942808315, Test Loss: 0.0526804698400306\n","Epoch: 8937, Train Loss: 0.05624657604762962, Test Loss: 0.05136663838887305\n","Epoch: 8938, Train Loss: 0.055894334536244546, Test Loss: 0.052673323836159025\n","Epoch: 8939, Train Loss: 0.05623867028095064, Test Loss: 0.051359447477821424\n","Epoch: 8940, Train Loss: 0.05588657411159134, Test Loss: 0.05266617219181695\n","Epoch: 8941, Train Loss: 0.05623075881730283, Test Loss: 0.05135225123712516\n","Epoch: 8942, Train Loss: 0.05587880815411685, Test Loss: 0.052659014906854616\n","Epoch: 8943, Train Loss: 0.05622284165634334, Test Loss: 0.05134504966620116\n","Epoch: 8944, Train Loss: 0.05587103666315112, Test Loss: 0.052651851980483035\n","Epoch: 8945, Train Loss: 0.056214918797093776, Test Loss: 0.05133784276388165\n","Epoch: 8946, Train Loss: 0.05586325963743867, Test Loss: 0.05264468341135642\n","Epoch: 8947, Train Loss: 0.05620699023802179, Test Loss: 0.05133063052849633\n","Epoch: 8948, Train Loss: 0.05585547707522106, Test Loss: 0.05263750919765758\n","Epoch: 8949, Train Loss: 0.05619905597712611, Test Loss: 0.05132341295795569\n","Epoch: 8950, Train Loss: 0.05584768897432055, Test Loss: 0.05263032933718612\n","Epoch: 8951, Train Loss: 0.056191116012023895, Test Loss: 0.05131619004983805\n","Epoch: 8952, Train Loss: 0.05583989533222774, Test Loss: 0.05262314382744856\n","Epoch: 8953, Train Loss: 0.056183170340040174, Test Loss: 0.051308961801477516\n","Epoch: 8954, Train Loss: 0.05583209614618982, Test Loss: 0.052615952665748984\n","Epoch: 8955, Train Loss: 0.056175218958298176, Test Loss: 0.05130172821005135\n","Epoch: 8956, Train Loss: 0.05582429141329843, Test Loss: 0.052608755849279536\n","Epoch: 8957, Train Loss: 0.05616726186380866, Test Loss: 0.05129448927266848\n","Epoch: 8958, Train Loss: 0.055816481130578424, Test Loss: 0.05260155337521006\n","Epoch: 8959, Train Loss: 0.056159299053559465, Test Loss: 0.05128724498645476\n","Epoch: 8960, Train Loss: 0.05580866529507372, Test Loss: 0.05259434524077622\n","Epoch: 8961, Train Loss: 0.056151330524602826, Test Loss: 0.05127999534863843\n","Epoch: 8962, Train Loss: 0.05580084390393292, Test Loss: 0.052587131443364674\n","Epoch: 8963, Train Loss: 0.05614335627413978, Test Loss: 0.05127274035662981\n","Epoch: 8964, Train Loss: 0.055793016954489315, Test Loss: 0.0525799119805942\n","Epoch: 8965, Train Loss: 0.056135376299601275, Test Loss: 0.051265480008100495\n","Epoch: 8966, Train Loss: 0.0557851844443405, Test Loss: 0.05257268685039415\n","Epoch: 8967, Train Loss: 0.056127390598725624, Test Loss: 0.05125821430105524\n","Epoch: 8968, Train Loss: 0.05577734637142041, Test Loss: 0.05256545605107707\n","Epoch: 8969, Train Loss: 0.056119399169631067, Test Loss: 0.051250943233901815\n","Epoch: 8970, Train Loss: 0.05576950273406945, Test Loss: 0.052558219581406135\n","Epoch: 8971, Train Loss: 0.05611140201088258, Test Loss: 0.05124366680551261\n","Epoch: 8972, Train Loss: 0.05576165353109613, Test Loss: 0.05255097744065716\n","Epoch: 8973, Train Loss: 0.056103399121553627, Test Loss: 0.051236385015282944\n","Epoch: 8974, Train Loss: 0.05575379876183577, Test Loss: 0.05254372962867356\n","Epoch: 8975, Train Loss: 0.05609539050128083, Test Loss: 0.05122909786318137\n","Epoch: 8976, Train Loss: 0.05574593842620073, Test Loss: 0.052536476145915216\n","Epoch: 8977, Train Loss: 0.05608737615031251, Test Loss: 0.05122180534979345\n","Epoch: 8978, Train Loss: 0.05573807252472426, Test Loss: 0.05252921699349977\n","Epoch: 8979, Train Loss: 0.056079356069549974, Test Loss: 0.051214507476359264\n","Epoch: 8980, Train Loss: 0.055730201058598154, Test Loss: 0.0525219521732375\n","Epoch: 8981, Train Loss: 0.05607133026058202, Test Loss: 0.05120720424480352\n","Epoch: 8982, Train Loss: 0.055722324029702716, Test Loss: 0.05251468168765746\n","Epoch: 8983, Train Loss: 0.05606329872571119, Test Loss: 0.05119989565775817\n","Epoch: 8984, Train Loss: 0.05571444144062954, Test Loss: 0.05250740554002743\n","Epoch: 8985, Train Loss: 0.056055261467973565, Test Loss: 0.05119258171857799\n","Epoch: 8986, Train Loss: 0.05570655329469666, Test Loss: 0.052500123734365294\n","Epoch: 8987, Train Loss: 0.056047218491150234, Test Loss: 0.05118526243134756\n","Epoch: 8988, Train Loss: 0.05569865959595594, Test Loss: 0.052492836275443426\n","Epoch: 8989, Train Loss: 0.05603916979977162, Test Loss: 0.051177937800884206\n","Epoch: 8990, Train Loss: 0.05569076034919537, Test Loss: 0.05248554316878605\n","Epoch: 8991, Train Loss: 0.05603111539911527, Test Loss: 0.05117060783273074\n","Epoch: 8992, Train Loss: 0.05568285555993198, Test Loss: 0.05247824442065892\n","Epoch: 8993, Train Loss: 0.05602305529519539, Test Loss: 0.05116327253314259\n","Epoch: 8994, Train Loss: 0.055674945234398615, Test Loss: 0.05247094003805207\n","Epoch: 8995, Train Loss: 0.05601498949474592, Test Loss: 0.0511559319090683\n","Epoch: 8996, Train Loss: 0.05566702937952424, Test Loss: 0.052463630028656554\n","Epoch: 8997, Train Loss: 0.056006918005197526, Test Loss: 0.0511485859681246\n","Epoch: 8998, Train Loss: 0.05565910800290881, Test Loss: 0.0524563144008344\n","Epoch: 8999, Train Loss: 0.055998840834647935, Test Loss: 0.051141234718563786\n","Epoch: 9000, Train Loss: 0.05565118111279039, Test Loss: 0.05244899316358363\n","Epoch: 9001, Train Loss: 0.05599075799182717, Test Loss: 0.05113387816923773\n","Epoch: 9002, Train Loss: 0.05564324871800888, Test Loss: 0.052441666326497424\n","Epoch: 9003, Train Loss: 0.05598266948605701, Test Loss: 0.05112651632955665\n","Epoch: 9004, Train Loss: 0.055635310827964406, Test Loss: 0.052434333899717786\n","Epoch: 9005, Train Loss: 0.05597457532720522, Test Loss: 0.05111914920944133\n","Epoch: 9006, Train Loss: 0.05562736745256939, Test Loss: 0.052426995893887135\n","Epoch: 9007, Train Loss: 0.05596647552563752, Test Loss: 0.05111177681927529\n","Epoch: 9008, Train Loss: 0.05561941860220021, Test Loss: 0.05241965232009418\n","Epoch: 9009, Train Loss: 0.05595837009216366, Test Loss: 0.05110439916984998\n","Epoch: 9010, Train Loss: 0.055611464287642465, Test Loss: 0.05241230318981768\n","Epoch: 9011, Train Loss: 0.05595025903798188, Test Loss: 0.0510970162723102\n","Epoch: 9012, Train Loss: 0.055603504520035554, Test Loss: 0.05240494851486828\n","Epoch: 9013, Train Loss: 0.05594214237462107, Test Loss: 0.05108962813809536\n","Epoch: 9014, Train Loss: 0.05559553931081406, Test Loss: 0.05239758830732704\n","Epoch: 9015, Train Loss: 0.0559340201138797, Test Loss: 0.05108223477888057\n","Epoch: 9016, Train Loss: 0.05558756867164834, Test Loss: 0.052390222579484995\n","Epoch: 9017, Train Loss: 0.0559258922677659, Test Loss: 0.05107483620651652\n","Epoch: 9018, Train Loss: 0.0555795926143841, Test Loss: 0.052382851343778694\n","Epoch: 9019, Train Loss: 0.05591775884843348, Test Loss: 0.051067432432967824\n","Epoch: 9020, Train Loss: 0.05557161115098059, Test Loss: 0.052375474612730254\n","Epoch: 9021, Train Loss: 0.055909619868122304, Test Loss: 0.0510600234702546\n","Epoch: 9022, Train Loss: 0.05556362429345162, Test Loss: 0.052368092398884576\n","Epoch: 9023, Train Loss: 0.05590147533909595, Test Loss: 0.05105260933039239\n","Epoch: 9024, Train Loss: 0.05555563205380564, Test Loss: 0.0523607047147495\n","Epoch: 9025, Train Loss: 0.055893325273582393, Test Loss: 0.05104519002533374\n","Epoch: 9026, Train Loss: 0.055547634443986686, Test Loss: 0.05235331157273653\n","Epoch: 9027, Train Loss: 0.055885169683714875, Test Loss: 0.051037765566913106\n","Epoch: 9028, Train Loss: 0.055539631475819265, Test Loss: 0.052345912985105277\n","Epoch: 9029, Train Loss: 0.05587700858147693, Test Loss: 0.05103033596679271\n","Epoch: 9030, Train Loss: 0.05553162316095397, Test Loss: 0.05233850896390955\n","Epoch: 9031, Train Loss: 0.05586884197864876, Test Loss: 0.05102290123641296\n","Epoch: 9032, Train Loss: 0.05552360951081764, Test Loss: 0.05233109952094686\n","Epoch: 9033, Train Loss: 0.055860669886756954, Test Loss: 0.051015461386943815\n","Epoch: 9034, Train Loss: 0.05551559053656482, Test Loss: 0.05232368466771245\n","Epoch: 9035, Train Loss: 0.05585249231702876, Test Loss: 0.051008016429242825\n","Epoch: 9036, Train Loss: 0.0555075662490353, Test Loss: 0.052316264415356056\n","Epoch: 9037, Train Loss: 0.05584430928034942, Test Loss: 0.05100056637381394\n","Epoch: 9038, Train Loss: 0.055499536658713045, Test Loss: 0.052308838774643346\n","Epoch: 9039, Train Loss: 0.05583612078722334, Test Loss: 0.050993111230773575\n","Epoch: 9040, Train Loss: 0.055491501775692266, Test Loss: 0.0523014077559227\n","Epoch: 9041, Train Loss: 0.055827926847741335, Test Loss: 0.05098565100981903\n","Epoch: 9042, Train Loss: 0.055483461609645415, Test Loss: 0.05229397136909603\n","Epoch: 9043, Train Loss: 0.05581972747155156, Test Loss: 0.05097818572020316\n","Epoch: 9044, Train Loss: 0.05547541616979831, Test Loss: 0.05228652962359447\n","Epoch: 9045, Train Loss: 0.05581152266783529, Test Loss: 0.05097071537071295\n","Epoch: 9046, Train Loss: 0.05546736546490831, Test Loss: 0.05227908252835983\n","Epoch: 9047, Train Loss: 0.05580331244528827, Test Loss: 0.05096323996965337\n","Epoch: 9048, Train Loss: 0.05545930950324839, Test Loss: 0.052271630091829925\n","Epoch: 9049, Train Loss: 0.055795096812106364, Test Loss: 0.05095575952483582\n","Epoch: 9050, Train Loss: 0.055451248292595526, Test Loss: 0.05226417232193067\n","Epoch: 9051, Train Loss: 0.05578687577597735, Test Loss: 0.050948274043572495\n","Epoch: 9052, Train Loss: 0.05544318184022527, Test Loss: 0.05225670922607187\n","Epoch: 9053, Train Loss: 0.05577864934407687, Test Loss: 0.05094078353267477\n","Epoch: 9054, Train Loss: 0.055435110152910125, Test Loss: 0.052249240811149074\n","Epoch: 9055, Train Loss: 0.0557704175230701, Test Loss: 0.050933287998456964\n","Epoch: 9056, Train Loss: 0.05542703323692324, Test Loss: 0.05224176708354949\n","Epoch: 9057, Train Loss: 0.055762180319117685, Test Loss: 0.050925787446744204\n","Epoch: 9058, Train Loss: 0.05541895109804695, Test Loss: 0.052234288049163136\n","Epoch: 9059, Train Loss: 0.05575393773788649, Test Loss: 0.050918281882885455\n","Epoch: 9060, Train Loss: 0.055410863741585305, Test Loss: 0.05222680371339876\n","Epoch: 9061, Train Loss: 0.05574568978456557, Test Loss: 0.05091077131177054\n","Epoch: 9062, Train Loss: 0.055402771172381654, Test Loss: 0.0522193140812025\n","Epoch: 9063, Train Loss: 0.055737436463884626, Test Loss: 0.050903255737850525\n","Epoch: 9064, Train Loss: 0.055394673394839125, Test Loss: 0.052211819157082444\n","Epoch: 9065, Train Loss: 0.05572917778013828, Test Loss: 0.05089573516516256\n","Epoch: 9066, Train Loss: 0.055386570412945625, Test Loss: 0.0522043189451357\n","Epoch: 9067, Train Loss: 0.05572091373721282, Test Loss: 0.05088820959735854\n","Epoch: 9068, Train Loss: 0.05537846223030269, Test Loss: 0.052196813449079234\n","Epoch: 9069, Train Loss: 0.05571264433861705, Test Loss: 0.05088067903773507\n","Epoch: 9070, Train Loss: 0.05537034885015566, Test Loss: 0.05218930267228297\n","Epoch: 9071, Train Loss: 0.05570436958751476, Test Loss: 0.05087314348926831\n","Epoch: 9072, Train Loss: 0.05536223027542882, Test Loss: 0.0521817866178061\n","Epoch: 9073, Train Loss: 0.05569608948676107, Test Loss: 0.05086560295464925\n","Epoch: 9074, Train Loss: 0.05535410650876085, Test Loss: 0.05217426528843576\n","Epoch: 9075, Train Loss: 0.055687804038940664, Test Loss: 0.05085805743632221\n","Epoch: 9076, Train Loss: 0.05534597755254342, Test Loss: 0.05216673868672647\n","Epoch: 9077, Train Loss: 0.05567951324640687, Test Loss: 0.050850506936523696\n","Epoch: 9078, Train Loss: 0.05533784340896047, Test Loss: 0.05215920681504158\n","Epoch: 9079, Train Loss: 0.055671217111322946, Test Loss: 0.050842951457323246\n","Epoch: 9080, Train Loss: 0.05532970408002911, Test Loss: 0.05215166967559518\n","Epoch: 9081, Train Loss: 0.0556629156357036, Test Loss: 0.05083539100066433\n","Epoch: 9082, Train Loss: 0.055321559567640864, Test Loss: 0.052144127270495166\n","Epoch: 9083, Train Loss: 0.05565460882145773, Test Loss: 0.050827825568405906\n","Epoch: 9084, Train Loss: 0.05531340987360323, Test Loss: 0.05213657960178509\n","Epoch: 9085, Train Loss: 0.05564629667043005, Test Loss: 0.050820255162363756\n","Epoch: 9086, Train Loss: 0.055305254999681544, Test Loss: 0.052129026671487805\n","Epoch: 9087, Train Loss: 0.05563797918444446, Test Loss: 0.0508126797843515\n","Epoch: 9088, Train Loss: 0.0552970949476398, Test Loss: 0.05212146848164553\n","Epoch: 9089, Train Loss: 0.05562965636534372, Test Loss: 0.05080509943621982\n","Epoch: 9090, Train Loss: 0.05528892971928033, Test Loss: 0.052113905034361564\n","Epoch: 9091, Train Loss: 0.055621328215030956, Test Loss: 0.05079751411989584\n","Epoch: 9092, Train Loss: 0.05528075931648324, Test Loss: 0.05210633633183811\n","Epoch: 9093, Train Loss: 0.05561299473550729, Test Loss: 0.050789923837419086\n","Epoch: 9094, Train Loss: 0.055272583741242456, Test Loss: 0.052098762376414\n","Epoch: 9095, Train Loss: 0.055604655928909355, Test Loss: 0.05078232859097825\n","Epoch: 9096, Train Loss: 0.05526440299570278, Test Loss: 0.05209118317059977\n","Epoch: 9097, Train Loss: 0.05559631179754392, Test Loss: 0.050774728382942357\n","Epoch: 9098, Train Loss: 0.055256217082191014, Test Loss: 0.052083598717110276\n","Epoch: 9099, Train Loss: 0.05558796234392061, Test Loss: 0.05076712321589272\n","Epoch: 9100, Train Loss: 0.05524802600324822, Test Loss: 0.05207600901889394\n","Epoch: 9101, Train Loss: 0.05557960757078086, Test Loss: 0.05075951309264947\n","Epoch: 9102, Train Loss: 0.055239829761656, Test Loss: 0.05206841407916054\n","Epoch: 9103, Train Loss: 0.05557124748112547, Test Loss: 0.05075189801629747\n","Epoch: 9104, Train Loss: 0.05523162836046281, Test Loss: 0.05206081390140491\n","Epoch: 9105, Train Loss: 0.05556288207823838, Test Loss: 0.050744277990207656\n","Epoch: 9106, Train Loss: 0.05522342180300518, Test Loss: 0.05205320848942795\n","Epoch: 9107, Train Loss: 0.055554511365707354, Test Loss: 0.05073665301805588\n","Epoch: 9108, Train Loss: 0.055215210092926695, Test Loss: 0.05204559784735357\n","Epoch: 9109, Train Loss: 0.05554613534744101, Test Loss: 0.050729023103838804\n","Epoch: 9110, Train Loss: 0.055206993234193646, Test Loss: 0.052037981979643606\n","Epoch: 9111, Train Loss: 0.05553775402768371, Test Loss: 0.05072138825188497\n","Epoch: 9112, Train Loss: 0.05519877123110649, Test Loss: 0.05203036089110706\n","Epoch: 9113, Train Loss: 0.05552936741102455, Test Loss: 0.0507137484668642\n","Epoch: 9114, Train Loss: 0.055190544088308854, Test Loss: 0.05202273458690795\n","Epoch: 9115, Train Loss: 0.055520975502405534, Test Loss: 0.05070610375379239\n","Epoch: 9116, Train Loss: 0.05518231181079254, Test Loss: 0.05201510307256882\n","Epoch: 9117, Train Loss: 0.05551257830712484, Test Loss: 0.05069845411803401\n","Epoch: 9118, Train Loss: 0.05517407440389988, Test Loss: 0.05200746635397086\n","Epoch: 9119, Train Loss: 0.05550417583083706, Test Loss: 0.050690799565300315\n","Epoch: 9120, Train Loss: 0.055165831873321754, Test Loss: 0.05199982443735019\n","Epoch: 9121, Train Loss: 0.05549576807954971, Test Loss: 0.0506831401016444\n","Epoch: 9122, Train Loss: 0.05515758422509275, Test Loss: 0.05199217732929185\n","Epoch: 9123, Train Loss: 0.05548735505961699, Test Loss: 0.050675475733454586\n","Epoch: 9124, Train Loss: 0.055149331465584456, Test Loss: 0.05198452503672018\n","Epoch: 9125, Train Loss: 0.05547893677773068, Test Loss: 0.05066780646744257\n","Epoch: 9126, Train Loss: 0.05514107360149336, Test Loss: 0.05197686756688604\n","Epoch: 9127, Train Loss: 0.0554705132409071, Test Loss: 0.0506601323106309\n","Epoch: 9128, Train Loss: 0.055132810639828236, Test Loss: 0.05196920492735154\n","Epoch: 9129, Train Loss: 0.05546208445647222, Test Loss: 0.05065245327033603\n","Epoch: 9130, Train Loss: 0.055124542587893205, Test Loss: 0.051961537125972325\n","Epoch: 9131, Train Loss: 0.0554536504320441, Test Loss: 0.05064476935415146\n","Epoch: 9132, Train Loss: 0.0551162694532705, Test Loss: 0.051953864170877585\n","Epoch: 9133, Train Loss: 0.05544521117551288, Test Loss: 0.05063708056992564\n","Epoch: 9134, Train Loss: 0.05510799124379851, Test Loss: 0.0519461860704474\n","Epoch: 9135, Train Loss: 0.05543676669501857, Test Loss: 0.05062938692574021\n","Epoch: 9136, Train Loss: 0.055099707967549635, Test Loss: 0.05193850283328875\n","Epoch: 9137, Train Loss: 0.05542831699892701, Test Loss: 0.05062168842988595\n","Epoch: 9138, Train Loss: 0.05509141963280625, Test Loss: 0.051930814468210296\n","Epoch: 9139, Train Loss: 0.05541986209580507, Test Loss: 0.05061398509083733\n","Epoch: 9140, Train Loss: 0.05508312624803497, Test Loss: 0.051923120984195006\n","Epoch: 9141, Train Loss: 0.05541140199439336, Test Loss: 0.05060627691722601\n","Epoch: 9142, Train Loss: 0.055074827821860114, Test Loss: 0.05191542239037266\n","Epoch: 9143, Train Loss: 0.055402936703578876, Test Loss: 0.050598563917813075\n","Epoch: 9144, Train Loss: 0.055066524363035704, Test Loss: 0.05190771869599115\n","Epoch: 9145, Train Loss: 0.05539446623236663, Test Loss: 0.050590846101461695\n","Epoch: 9146, Train Loss: 0.05505821588041802, Test Loss: 0.05190000991038733\n","Epoch: 9147, Train Loss: 0.055385990589850685, Test Loss: 0.0505831234771082\n","Epoch: 9148, Train Loss: 0.05504990238293644, Test Loss: 0.05189229604295799\n","Epoch: 9149, Train Loss: 0.055377509785185325, Test Loss: 0.05057539605373374\n","Epoch: 9150, Train Loss: 0.05504158387956519, Test Loss: 0.051884577103130575\n","Epoch: 9151, Train Loss: 0.05536902382755596, Test Loss: 0.05056766384033736\n","Epoch: 9152, Train Loss: 0.055033260379296026, Test Loss: 0.05187685310033558\n","Epoch: 9153, Train Loss: 0.0553605327261516, Test Loss: 0.05055992684590774\n","Epoch: 9154, Train Loss: 0.055024931891109984, Test Loss: 0.051869124043977394\n","Epoch: 9155, Train Loss: 0.0553520364901362, Test Loss: 0.05055218507939593\n","Epoch: 9156, Train Loss: 0.05501659842394994, Test Loss: 0.051861389943407644\n","Epoch: 9157, Train Loss: 0.05534353512862183, Test Loss: 0.05054443854969085\n","Epoch: 9158, Train Loss: 0.055008259986696034, Test Loss: 0.05185365080790002\n","Epoch: 9159, Train Loss: 0.05533502865064381, Test Loss: 0.05053668726559379\n","Epoch: 9160, Train Loss: 0.05499991658814005, Test Loss: 0.05184590664662467\n","Epoch: 9161, Train Loss: 0.05532651706513535, Test Loss: 0.050528931235795306\n","Epoch: 9162, Train Loss: 0.054991568236962315, Test Loss: 0.05183815746862571\n","Epoch: 9163, Train Loss: 0.05531800038090509, Test Loss: 0.050521170468854265\n","Epoch: 9164, Train Loss: 0.05498321494171047, Test Loss: 0.05183040328280016\n","Epoch: 9165, Train Loss: 0.055309478606616114, Test Loss: 0.05051340497317757\n","Epoch: 9166, Train Loss: 0.05497485671077936, Test Loss: 0.05182264409787852\n","Epoch: 9167, Train Loss: 0.05530095175076672, Test Loss: 0.050505634757002246\n","Epoch: 9168, Train Loss: 0.054966493552392894, Test Loss: 0.051814879922407284\n","Epoch: 9169, Train Loss: 0.055292419821673024, Test Loss: 0.05049785982838011\n","Epoch: 9170, Train Loss: 0.05495812547458871, Test Loss: 0.05180711076473379\n","Epoch: 9171, Train Loss: 0.05528388282745388, Test Loss: 0.05049008019516313\n","Epoch: 9172, Train Loss: 0.054949752485203604, Test Loss: 0.05179933663299339\n","Epoch: 9173, Train Loss: 0.05527534077601802, Test Loss: 0.05048229586499273\n","Epoch: 9174, Train Loss: 0.0549413745918627, Test Loss: 0.051791557535098676\n","Epoch: 9175, Train Loss: 0.05526679367505347, Test Loss: 0.05047450684528983\n","Epoch: 9176, Train Loss: 0.054932991801969575, Test Loss: 0.0517837734787316\n","Epoch: 9177, Train Loss: 0.055258241532019595, Test Loss: 0.0504667131432486\n","Epoch: 9178, Train Loss: 0.05492460412269992, Test Loss: 0.05177598447133664\n","Epoch: 9179, Train Loss: 0.05524968435414023, Test Loss: 0.0504589147658305\n","Epoch: 9180, Train Loss: 0.05491621156099568, Test Loss: 0.05176819052011776\n","Epoch: 9181, Train Loss: 0.055241122148400865, Test Loss: 0.050451111719763124\n","Epoch: 9182, Train Loss: 0.05490781412356404, Test Loss: 0.05176039163203774\n","Epoch: 9183, Train Loss: 0.055232554921547675, Test Loss: 0.05044330401153946\n","Epoch: 9184, Train Loss: 0.05489941181687638, Test Loss: 0.05175258781381817\n","Epoch: 9185, Train Loss: 0.055223982680087574, Test Loss: 0.05043549164741996\n","Epoch: 9186, Train Loss: 0.05489100464717082, Test Loss: 0.051744779071943935\n","Epoch: 9187, Train Loss: 0.05521540543029268, Test Loss: 0.050427674633437045\n","Epoch: 9188, Train Loss: 0.05488259262045642, Test Loss: 0.05173696541266731\n","Epoch: 9189, Train Loss: 0.05520682317820442, Test Loss: 0.0504198529754008\n","Epoch: 9190, Train Loss: 0.054874175742519245, Test Loss: 0.051729146842017106\n","Epoch: 9191, Train Loss: 0.0551982359296423, Test Loss: 0.050412026678907665\n","Epoch: 9192, Train Loss: 0.05486575401893093, Test Loss: 0.05172132336580648\n","Epoch: 9193, Train Loss: 0.05518964369021204, Test Loss: 0.05040419574935008\n","Epoch: 9194, Train Loss: 0.05485732745505854, Test Loss: 0.051713494989645586\n","Epoch: 9195, Train Loss: 0.05518104646531771, Test Loss: 0.05039636019192828\n","Epoch: 9196, Train Loss: 0.054848896056076325, Test Loss: 0.051705661718953626\n","Epoch: 9197, Train Loss: 0.05517244426017404, Test Loss: 0.05038852001166367\n","Epoch: 9198, Train Loss: 0.054840459826979326, Test Loss: 0.05169782355897329\n","Epoch: 9199, Train Loss: 0.05516383707982044, Test Loss: 0.05038067521341318\n","Epoch: 9200, Train Loss: 0.05483201877259773, Test Loss: 0.05168998051478732\n","Epoch: 9201, Train Loss: 0.05515522492913763, Test Loss: 0.05037282580188523\n","Epoch: 9202, Train Loss: 0.05482357289761304, Test Loss: 0.05168213259133452\n","Epoch: 9203, Train Loss: 0.05514660781286341, Test Loss: 0.05036497178165652\n","Epoch: 9204, Train Loss: 0.05481512220657487, Test Loss: 0.05167427979342778\n","Epoch: 9205, Train Loss: 0.05513798573561057, Test Loss: 0.05035711315718985\n","Epoch: 9206, Train Loss: 0.05480666670391895, Test Loss: 0.0516664221257731\n","Epoch: 9207, Train Loss: 0.05512935870188583, Test Loss: 0.05034924993285227\n","Epoch: 9208, Train Loss: 0.0547982063939852, Test Loss: 0.05165855959298792\n","Epoch: 9209, Train Loss: 0.05512072671610805, Test Loss: 0.05034138211293381\n","Epoch: 9210, Train Loss: 0.05478974128103699, Test Loss: 0.051650692199622115\n","Epoch: 9211, Train Loss: 0.055112089782628916, Test Loss: 0.05033350970166723\n","Epoch: 9212, Train Loss: 0.05478127136928043, Test Loss: 0.051642819950176486\n","Epoch: 9213, Train Loss: 0.0551034479057514, Test Loss: 0.050325632703246595\n","Epoch: 9214, Train Loss: 0.05477279666288354, Test Loss: 0.051634942849123316\n","Epoch: 9215, Train Loss: 0.055094801089750275, Test Loss: 0.05031775112184671\n","Epoch: 9216, Train Loss: 0.05476431716599541, Test Loss: 0.051627060900925496\n","Epoch: 9217, Train Loss: 0.05508614933889098, Test Loss: 0.05030986496164223\n","Epoch: 9218, Train Loss: 0.05475583288276571, Test Loss: 0.05161917411005654\n","Epoch: 9219, Train Loss: 0.05507749265744947, Test Loss: 0.05030197422682623\n","Epoch: 9220, Train Loss: 0.054747343817363105, Test Loss: 0.05161128248101889\n","Epoch: 9221, Train Loss: 0.05506883104973052, Test Loss: 0.050294078921627544\n","Epoch: 9222, Train Loss: 0.054738849973992885, Test Loss: 0.051603386018361466\n","Epoch: 9223, Train Loss: 0.05506016452008515, Test Loss: 0.05028617905032854\n","Epoch: 9224, Train Loss: 0.05473035135691448, Test Loss: 0.05159548472669833\n","Epoch: 9225, Train Loss: 0.05505149307292907, Test Loss: 0.050278274617281675\n","Epoch: 9226, Train Loss: 0.054721847970458586, Test Loss: 0.05158757861072403\n","Epoch: 9227, Train Loss: 0.05504281671275803, Test Loss: 0.05027036562692435\n","Epoch: 9228, Train Loss: 0.05471333981904161, Test Loss: 0.05157966767522867\n","Epoch: 9229, Train Loss: 0.05503413544416279, Test Loss: 0.050262452083793\n","Epoch: 9230, Train Loss: 0.05470482690718012, Test Loss: 0.05157175192511243\n","Epoch: 9231, Train Loss: 0.05502544927184344, Test Loss: 0.05025453399253663\n","Epoch: 9232, Train Loss: 0.054696309239504186, Test Loss: 0.05156383136539805\n","Epoch: 9233, Train Loss: 0.05501675820062205, Test Loss: 0.05024661135792859\n","Epoch: 9234, Train Loss: 0.05468778682076938, Test Loss: 0.05155590600124163\n","Epoch: 9235, Train Loss: 0.05500806223545312, Test Loss: 0.050238684184875856\n","Epoch: 9236, Train Loss: 0.054679259655866036, Test Loss: 0.051547975837943266\n","Epoch: 9237, Train Loss: 0.0549993613814345, Test Loss: 0.050230752478429265\n","Epoch: 9238, Train Loss: 0.05467072774982946, Test Loss: 0.05154004088095507\n","Epoch: 9239, Train Loss: 0.054990655643814994, Test Loss: 0.05022281624378998\n","Epoch: 9240, Train Loss: 0.054662191107846425, Test Loss: 0.0515321011358872\n","Epoch: 9241, Train Loss: 0.054981945028000766, Test Loss: 0.05021487548631496\n","Epoch: 9242, Train Loss: 0.054653649735260657, Test Loss: 0.05152415660851358\n","Epoch: 9243, Train Loss: 0.05497322953956072, Test Loss: 0.050206930211522345\n","Epoch: 9244, Train Loss: 0.054645103637578143, Test Loss: 0.05151620730477558\n","Epoch: 9245, Train Loss: 0.05496450918423029, Test Loss: 0.05019898042509321\n","Epoch: 9246, Train Loss: 0.054636552820469045, Test Loss: 0.051508253230783424\n","Epoch: 9247, Train Loss: 0.054955783967912994, Test Loss: 0.050191026132873136\n","Epoch: 9248, Train Loss: 0.05462799728976901, Test Loss: 0.05150029439281687\n","Epoch: 9249, Train Loss: 0.05494705389668091, Test Loss: 0.05018306734087171\n","Epoch: 9250, Train Loss: 0.05461943705147884, Test Loss: 0.05149233079732349\n","Epoch: 9251, Train Loss: 0.05493831897677316, Test Loss: 0.050175104055260586\n","Epoch: 9252, Train Loss: 0.05461087211176238, Test Loss: 0.051484362450916636\n","Epoch: 9253, Train Loss: 0.05492957921459381, Test Loss: 0.05016713628237028\n","Epoch: 9254, Train Loss: 0.05460230247694335, Test Loss: 0.05147638936037083\n","Epoch: 9255, Train Loss: 0.05492083461670735, Test Loss: 0.05015916402868583\n","Epoch: 9256, Train Loss: 0.05459372815350098, Test Loss: 0.051468411532616824\n","Epoch: 9257, Train Loss: 0.0549120851898338, Test Loss: 0.050151187300840616\n","Epoch: 9258, Train Loss: 0.054585149148063575, Test Loss: 0.05146042897473409\n","Epoch: 9259, Train Loss: 0.05490333094084123, Test Loss: 0.05014320610560903\n","Epoch: 9260, Train Loss: 0.054576565467401486, Test Loss: 0.0514524416939429\n","Epoch: 9261, Train Loss: 0.05489457187673812, Test Loss: 0.05013522044989844\n","Epoch: 9262, Train Loss: 0.05456797711841854, Test Loss: 0.05144444969759602\n","Epoch: 9263, Train Loss: 0.05488580800466475, Test Loss: 0.05012723034073997\n","Epoch: 9264, Train Loss: 0.05455938410814332, Test Loss: 0.051436452993167785\n","Epoch: 9265, Train Loss: 0.0548770393318827, Test Loss: 0.05011923578527843\n","Epoch: 9266, Train Loss: 0.054550786443718406, Test Loss: 0.05142845158824392\n","Epoch: 9267, Train Loss: 0.05486826586576455, Test Loss: 0.050111236790761154\n","Epoch: 9268, Train Loss: 0.054542184132389704, Test Loss: 0.051420445490509045\n","Epoch: 9269, Train Loss: 0.05485948761378174, Test Loss: 0.05010323336452645\n","Epoch: 9270, Train Loss: 0.05453357718149444, Test Loss: 0.051412434707735336\n","Epoch: 9271, Train Loss: 0.0548507045834929, Test Loss: 0.05009522551399191\n","Epoch: 9272, Train Loss: 0.054524965598449514, Test Loss: 0.05140441924776937\n","Epoch: 9273, Train Loss: 0.054841916782531176, Test Loss: 0.05008721324664109\n","Epoch: 9274, Train Loss: 0.05451634939073815, Test Loss: 0.05139639911851849\n","Epoch: 9275, Train Loss: 0.054833124218590545, Test Loss: 0.05007919657001088\n","Epoch: 9276, Train Loss: 0.05450772856589722, Test Loss: 0.051388374327938476\n","Epoch: 9277, Train Loss: 0.05482432689941352, Test Loss: 0.050071175491679165\n","Epoch: 9278, Train Loss: 0.05449910313150474, Test Loss: 0.05138034488401916\n","Epoch: 9279, Train Loss: 0.054815524832777004, Test Loss: 0.050063150019250977\n","Epoch: 9280, Train Loss: 0.05449047309516585, Test Loss: 0.05137231079477161\n","Epoch: 9281, Train Loss: 0.05480671802647941, Test Loss: 0.05005512016034553\n","Epoch: 9282, Train Loss: 0.054481838464500144, Test Loss: 0.05136427206821456\n","Epoch: 9283, Train Loss: 0.054797906488327316, Test Loss: 0.05004708592258371\n","Epoch: 9284, Train Loss: 0.05447319924712874, Test Loss: 0.051356228712361125\n","Epoch: 9285, Train Loss: 0.054789090226122154, Test Loss: 0.05003904731357568\n","Epoch: 9286, Train Loss: 0.054464555450661985, Test Loss: 0.051348180735206826\n","Epoch: 9287, Train Loss: 0.05478026924764843, Test Loss: 0.05003100434090834\n","Epoch: 9288, Train Loss: 0.05445590708268684, Test Loss: 0.051340128144716744\n","Epoch: 9289, Train Loss: 0.05477144356066087, Test Loss: 0.05002295701213409\n","Epoch: 9290, Train Loss: 0.05444725415075559, Test Loss: 0.05133207094881409\n","Epoch: 9291, Train Loss: 0.054762613172873086, Test Loss: 0.050014905334759545\n","Epoch: 9292, Train Loss: 0.054438596662374446, Test Loss: 0.051324009155369395\n","Epoch: 9293, Train Loss: 0.05475377809194684, Test Loss: 0.05000684931623546\n","Epoch: 9294, Train Loss: 0.054429934624993535, Test Loss: 0.051315942772190155\n","Epoch: 9295, Train Loss: 0.05474493832548178, Test Loss: 0.04999878896394711\n","Epoch: 9296, Train Loss: 0.054421268045997215, Test Loss: 0.05130787180701167\n","Epoch: 9297, Train Loss: 0.05473609388100627, Test Loss: 0.049990724285206005\n","Epoch: 9298, Train Loss: 0.05441259693269581, Test Loss: 0.05129979626748858\n","Epoch: 9299, Train Loss: 0.05472724476596898, Test Loss: 0.049982655287241384\n","Epoch: 9300, Train Loss: 0.0544039212923168, Test Loss: 0.05129171616118734\n","Epoch: 9301, Train Loss: 0.05471839098773144, Test Loss: 0.04997458197719394\n","Epoch: 9302, Train Loss: 0.054395241131998916, Test Loss: 0.05128363149557955\n","Epoch: 9303, Train Loss: 0.05470953255356138, Test Loss: 0.04996650436210961\n","Epoch: 9304, Train Loss: 0.05438655645878568, Test Loss: 0.05127554227803647\n","Epoch: 9305, Train Loss: 0.05470066947062729, Test Loss: 0.04995842244893511\n","Epoch: 9306, Train Loss: 0.054377867279621045, Test Loss: 0.05126744851582519\n","Epoch: 9307, Train Loss: 0.05469180174599458, Test Loss: 0.04995033624451373\n","Epoch: 9308, Train Loss: 0.054369173601345085, Test Loss: 0.051259350216104484\n","Epoch: 9309, Train Loss: 0.05468292938662144, Test Loss: 0.04994224575558283\n","Epoch: 9310, Train Loss: 0.054360475430691536, Test Loss: 0.05125124738592329\n","Epoch: 9311, Train Loss: 0.054674052399357305, Test Loss: 0.049934150988772394\n","Epoch: 9312, Train Loss: 0.054351772774286475, Test Loss: 0.05124314003221881\n","Epoch: 9313, Train Loss: 0.05466517079094103, Test Loss: 0.04992605195060359\n","Epoch: 9314, Train Loss: 0.05434306563864667, Test Loss: 0.051235028161817395\n","Epoch: 9315, Train Loss: 0.054656284568001694, Test Loss: 0.04991794864749\n","Epoch: 9316, Train Loss: 0.054334354030181095, Test Loss: 0.051226911781434306\n","Epoch: 9317, Train Loss: 0.05464739373705828, Test Loss: 0.04990984108573835\n","Epoch: 9318, Train Loss: 0.05432563795519147, Test Loss: 0.05121879089767664\n","Epoch: 9319, Train Loss: 0.05463849830452273, Test Loss: 0.04990172927155111\n","Epoch: 9320, Train Loss: 0.05431691741987491, Test Loss: 0.05121066551704545\n","Epoch: 9321, Train Loss: 0.05462959827670194, Test Loss: 0.04989361321102944\n","Epoch: 9322, Train Loss: 0.05430819243032707, Test Loss: 0.05120253564593972\n","Epoch: 9323, Train Loss: 0.05462069365980167, Test Loss: 0.049885492910177485\n","Epoch: 9324, Train Loss: 0.05429946299254643, Test Loss: 0.05119440129066171\n","Epoch: 9325, Train Loss: 0.054611784459931895, Test Loss: 0.04987736837490754\n","Epoch: 9326, Train Loss: 0.054290729112439255, Test Loss: 0.05118626245742168\n","Epoch: 9327, Train Loss: 0.05460287068311147, Test Loss: 0.04986923961104523\n","Epoch: 9328, Train Loss: 0.05428199079582516, Test Loss: 0.05117811915234402\n","Epoch: 9329, Train Loss: 0.054593952335274254, Test Loss: 0.049861106624335405\n","Epoch: 9330, Train Loss: 0.054273248048442906, Test Loss: 0.05116997138147461\n","Epoch: 9331, Train Loss: 0.05458502942227623, Test Loss: 0.049852969420450505\n","Epoch: 9332, Train Loss: 0.05426450087595875, Test Loss: 0.051161819150787755\n","Epoch: 9333, Train Loss: 0.05457610194990268, Test Loss: 0.04984482800499576\n","Epoch: 9334, Train Loss: 0.05425574928397177, Test Loss: 0.05115366246619408\n","Epoch: 9335, Train Loss: 0.05456716992387568, Test Loss: 0.04983668238351981\n","Epoch: 9336, Train Loss: 0.05424699327802452, Test Loss: 0.051145501333549746\n","Epoch: 9337, Train Loss: 0.0545582333498636, Test Loss: 0.04982853256152057\n","Epoch: 9338, Train Loss: 0.05423823286360903, Test Loss: 0.05113733575866353\n","Epoch: 9339, Train Loss: 0.054549292233487874, Test Loss: 0.04982037854445476\n","Epoch: 9340, Train Loss: 0.05422946804617619, Test Loss: 0.051129165747307304\n","Epoch: 9341, Train Loss: 0.054540346580333465, Test Loss: 0.04981222033774644\n","Epoch: 9342, Train Loss: 0.05422069883114452, Test Loss: 0.05112099130522359\n","Epoch: 9343, Train Loss: 0.054531396395956444, Test Loss: 0.049804057946796036\n","Epoch: 9344, Train Loss: 0.054211925223909156, Test Loss: 0.05111281243813588\n","Epoch: 9345, Train Loss: 0.054522441685894124, Test Loss: 0.049795891376988734\n","Epoch: 9346, Train Loss: 0.054203147229850354, Test Loss: 0.05110462915175712\n","Epoch: 9347, Train Loss: 0.05451348245567351, Test Loss: 0.04978772063370388\n","Epoch: 9348, Train Loss: 0.054194364854342834, Test Loss: 0.05109644145179877\n","Epoch: 9349, Train Loss: 0.054504518710820446, Test Loss: 0.049779545722323064\n","Epoch: 9350, Train Loss: 0.05418557810276407, Test Loss: 0.05108824934397969\n","Epoch: 9351, Train Loss: 0.0544955504568683, Test Loss: 0.049771366648239015\n","Epoch: 9352, Train Loss: 0.054176786980503065, Test Loss: 0.051080052834034814\n","Epoch: 9353, Train Loss: 0.054486577699366504, Test Loss: 0.049763183416863005\n","Epoch: 9354, Train Loss: 0.05416799149296783, Test Loss: 0.05107185192772261\n","Epoch: 9355, Train Loss: 0.054477600443888205, Test Loss: 0.04975499603363403\n","Epoch: 9356, Train Loss: 0.05415919164559469, Test Loss: 0.051063646630834555\n","Epoch: 9357, Train Loss: 0.054468618696039424, Test Loss: 0.049746804504024904\n","Epoch: 9358, Train Loss: 0.05415038744385441, Test Loss: 0.05105543694920065\n","Epoch: 9359, Train Loss: 0.054459632461464826, Test Loss: 0.049738608833550205\n","Epoch: 9360, Train Loss: 0.054141578893259934, Test Loss: 0.05104722288869802\n","Epoch: 9361, Train Loss: 0.05445064174585593, Test Loss: 0.049730409027771906\n","Epoch: 9362, Train Loss: 0.05413276599937245, Test Loss: 0.05103900445525578\n","Epoch: 9363, Train Loss: 0.054441646554956244, Test Loss: 0.04972220509230595\n","Epoch: 9364, Train Loss: 0.05412394876780765, Test Loss: 0.05103078165486145\n","Epoch: 9365, Train Loss: 0.05443264689456746, Test Loss: 0.04971399703282702\n","Epoch: 9366, Train Loss: 0.054115127204240505, Test Loss: 0.051022554493566685\n","Epoch: 9367, Train Loss: 0.054423642770555274, Test Loss: 0.04970578485507376\n","Epoch: 9368, Train Loss: 0.05410630131441088, Test Loss: 0.0510143229774902\n","Epoch: 9369, Train Loss: 0.05441463418885227, Test Loss: 0.049697568564852086\n","Epoch: 9370, Train Loss: 0.05409747110412639, Test Loss: 0.05100608711282296\n","Epoch: 9371, Train Loss: 0.05440562115546296, Test Loss: 0.04968934816803902\n","Epoch: 9372, Train Loss: 0.05408863657926656, Test Loss: 0.050997846905830466\n","Epoch: 9373, Train Loss: 0.05439660367646618, Test Loss: 0.04968112367058519\n","Epoch: 9374, Train Loss: 0.054079797745785155, Test Loss: 0.050989602362854915\n","Epoch: 9375, Train Loss: 0.05438758175801741, Test Loss: 0.04967289507851658\n","Epoch: 9376, Train Loss: 0.054070954609712035, Test Loss: 0.050981353490317065\n","Epoch: 9377, Train Loss: 0.054378555406350185, Test Loss: 0.049664662397935565\n","Epoch: 9378, Train Loss: 0.05406210717715411, Test Loss: 0.050973100294717404\n","Epoch: 9379, Train Loss: 0.05436952462777774, Test Loss: 0.04965642563502249\n","Epoch: 9380, Train Loss: 0.0540532554542969, Test Loss: 0.050964842782635696\n","Epoch: 9381, Train Loss: 0.05436048942869231, Test Loss: 0.04964818479603347\n","Epoch: 9382, Train Loss: 0.05404439944740244, Test Loss: 0.05095658096073011\n","Epoch: 9383, Train Loss: 0.05435144981556426, Test Loss: 0.04963993988730183\n","Epoch: 9384, Train Loss: 0.05403553916281055, Test Loss: 0.050948314835737404\n","Epoch: 9385, Train Loss: 0.05434240579494253, Test Loss: 0.04963169091523455\n","Epoch: 9386, Train Loss: 0.05402667460693536, Test Loss: 0.050940044414469275\n","Epoch: 9387, Train Loss: 0.05433335737345061, Test Loss: 0.049623437886311926\n","Epoch: 9388, Train Loss: 0.05401780578626493, Test Loss: 0.050931769703811684\n","Epoch: 9389, Train Loss: 0.05432430455778633, Test Loss: 0.04961518080708389\n","Epoch: 9390, Train Loss: 0.054008932707357474, Test Loss: 0.05092349071071998\n","Epoch: 9391, Train Loss: 0.054315247354716566, Test Loss: 0.049606919684166674\n","Epoch: 9392, Train Loss: 0.05400005537683814, Test Loss: 0.05091520744221658\n","Epoch: 9393, Train Loss: 0.05430618577107546, Test Loss: 0.04959865452423972\n","Epoch: 9394, Train Loss: 0.053991173801395745, Test Loss: 0.05090691990538632\n","Epoch: 9395, Train Loss: 0.0542971198137593, Test Loss: 0.04959038533404078\n","Epoch: 9396, Train Loss: 0.05398228798777796, Test Loss: 0.05089862810737154\n","Epoch: 9397, Train Loss: 0.054288049489722, Test Loss: 0.04958211212036163\n","Epoch: 9398, Train Loss: 0.05397339794278685, Test Loss: 0.05089033205536815\n","Epoch: 9399, Train Loss: 0.05427897480597105, Test Loss: 0.049573834890043834\n","Epoch: 9400, Train Loss: 0.05396450367327473, Test Loss: 0.05088203175661947\n","Epoch: 9401, Train Loss: 0.054269895769561385, Test Loss: 0.04956555364997166\n","Epoch: 9402, Train Loss: 0.05395560518613705, Test Loss: 0.05087372721841077\n","Epoch: 9403, Train Loss: 0.05426081238759008, Test Loss: 0.04955726840706842\n","Epoch: 9404, Train Loss: 0.05394670248830856, Test Loss: 0.050865418448063776\n","Epoch: 9405, Train Loss: 0.05425172466719079, Test Loss: 0.049548979168289264\n","Epoch: 9406, Train Loss: 0.053937795586756265, Test Loss: 0.05085710545293007\n","Epoch: 9407, Train Loss: 0.05424263261552723, Test Loss: 0.049540685940616565\n","Epoch: 9408, Train Loss: 0.05392888448847465, Test Loss: 0.050848788240386265\n","Epoch: 9409, Train Loss: 0.05423353623978844, Test Loss: 0.049532388731053395\n","Epoch: 9410, Train Loss: 0.05391996920047909, Test Loss: 0.050840466817826245\n","Epoch: 9411, Train Loss: 0.054224435547180976, Test Loss: 0.04952408754661731\n","Epoch: 9412, Train Loss: 0.0539110497297997, Test Loss: 0.050832141192656256\n","Epoch: 9413, Train Loss: 0.05421533054492407, Test Loss: 0.04951578239433473\n","Epoch: 9414, Train Loss: 0.053902126083475596, Test Loss: 0.05082381137228812\n","Epoch: 9415, Train Loss: 0.054206221240243026, Test Loss: 0.04950747328123492\n","Epoch: 9416, Train Loss: 0.053893198268548985, Test Loss: 0.050815477364133554\n","Epoch: 9417, Train Loss: 0.054197107640363326, Test Loss: 0.049499160214344584\n","Epoch: 9418, Train Loss: 0.05388426629205951, Test Loss: 0.050807139175598065\n","Epoch: 9419, Train Loss: 0.05418798975250481, Test Loss: 0.04949084320068124\n","Epoch: 9420, Train Loss: 0.05387533016103781, Test Loss: 0.05079879681407554\n","Epoch: 9421, Train Loss: 0.05417886758387614, Test Loss: 0.04948252224724947\n","Epoch: 9422, Train Loss: 0.05386638988250161, Test Loss: 0.05079045028694319\n","Epoch: 9423, Train Loss: 0.054169741141669774, Test Loss: 0.0494741973610343\n","Epoch: 9424, Train Loss: 0.05385744546344912, Test Loss: 0.05078209960155554\n","Epoch: 9425, Train Loss: 0.054160610433056124, Test Loss: 0.04946586854899754\n","Epoch: 9426, Train Loss: 0.05384849691085544, Test Loss: 0.05077374476524074\n","Epoch: 9427, Train Loss: 0.05415147546517979, Test Loss: 0.04945753581807278\n","Epoch: 9428, Train Loss: 0.05383954423166729, Test Loss: 0.05076538578529547\n","Epoch: 9429, Train Loss: 0.05414233624515454, Test Loss: 0.0494491991751609\n","Epoch: 9430, Train Loss: 0.05383058743279881, Test Loss: 0.05075702266898113\n","Epoch: 9431, Train Loss: 0.05413319278005945, Test Loss: 0.04944085862712776\n","Epoch: 9432, Train Loss: 0.05382162652112899, Test Loss: 0.05074865542352038\n","Epoch: 9433, Train Loss: 0.05412404507693555, Test Loss: 0.04943251418079908\n","Epoch: 9434, Train Loss: 0.053812661503496756, Test Loss: 0.05074028405609417\n","Epoch: 9435, Train Loss: 0.054114893142782755, Test Loss: 0.04942416584295937\n","Epoch: 9436, Train Loss: 0.05380369238669975, Test Loss: 0.05073190857383851\n","Epoch: 9437, Train Loss: 0.054105736984556826, Test Loss: 0.0494158136203484\n","Epoch: 9438, Train Loss: 0.05379471917749065, Test Loss: 0.05072352898384284\n","Epoch: 9439, Train Loss: 0.05409657660916753, Test Loss: 0.049407457519659276\n","Epoch: 9440, Train Loss: 0.05378574188257571, Test Loss: 0.0507151452931478\n","Epoch: 9441, Train Loss: 0.05408741202347666, Test Loss: 0.049399097547538115\n","Epoch: 9442, Train Loss: 0.05377676050861395, Test Loss: 0.05070675750874478\n","Epoch: 9443, Train Loss: 0.05407824323429737, Test Loss: 0.04939073371058163\n","Epoch: 9444, Train Loss: 0.05376777506221501, Test Loss: 0.05069836563757451\n","Epoch: 9445, Train Loss: 0.05406907024839282, Test Loss: 0.049382366015338096\n","Epoch: 9446, Train Loss: 0.053758785549939944, Test Loss: 0.05068996968652707\n","Epoch: 9447, Train Loss: 0.054059893072476314, Test Loss: 0.04937399446830577\n","Epoch: 9448, Train Loss: 0.053749791978299925, Test Loss: 0.05068156966244191\n","Epoch: 9449, Train Loss: 0.0540507117132111, Test Loss: 0.04936561907593435\n","Epoch: 9450, Train Loss: 0.053740794353757476, Test Loss: 0.05067316557210867\n","Epoch: 9451, Train Loss: 0.054041526177211426, Test Loss: 0.04935723984462543\n","Epoch: 9452, Train Loss: 0.05373179268272694, Test Loss: 0.050664757422268517\n","Epoch: 9453, Train Loss: 0.05403233647104357, Test Loss: 0.04934885678073467\n","Epoch: 9454, Train Loss: 0.05372278697157681, Test Loss: 0.05065634521961542\n","Epoch: 9455, Train Loss: 0.054023142601227404, Test Loss: 0.0493404698905715\n","Epoch: 9456, Train Loss: 0.05371377722662933, Test Loss: 0.050647928970797826\n","Epoch: 9457, Train Loss: 0.05401394457423777, Test Loss: 0.04933207918040319\n","Epoch: 9458, Train Loss: 0.05370476345416468, Test Loss: 0.05063950868242143\n","Epoch: 9459, Train Loss: 0.05400474239650746, Test Loss: 0.049323684656455764\n","Epoch: 9460, Train Loss: 0.05369574566042179, Test Loss: 0.05063108436105143\n","Epoch: 9461, Train Loss: 0.053995536074429314, Test Loss: 0.0493152863249181\n","Epoch: 9462, Train Loss: 0.053686723851602676, Test Loss: 0.05062265601321627\n","Epoch: 9463, Train Loss: 0.053986325614359953, Test Loss: 0.04930688419194324\n","Epoch: 9464, Train Loss: 0.05367769803387337, Test Loss: 0.050614223645409316\n","Epoch: 9465, Train Loss: 0.05397711102262153, Test Loss: 0.04929847826365324\n","Epoch: 9466, Train Loss: 0.05366866821336929, Test Loss: 0.05060578726409419\n","Epoch: 9467, Train Loss: 0.05396789230550683, Test Loss: 0.04929006854614194\n","Epoch: 9468, Train Loss: 0.053659634396197704, Test Loss: 0.050597346875706876\n","Epoch: 9469, Train Loss: 0.0539586694692817, Test Loss: 0.049281655045478226\n","Epoch: 9470, Train Loss: 0.05365059658844106, Test Loss: 0.05058890248666083\n","Epoch: 9471, Train Loss: 0.05394944252018962, Test Loss: 0.04927323776771034\n","Epoch: 9472, Train Loss: 0.053641554796161416, Test Loss: 0.05058045410334941\n","Epoch: 9473, Train Loss: 0.05394021146445467, Test Loss: 0.04926481671886979\n","Epoch: 9474, Train Loss: 0.05363250902540435, Test Loss: 0.050572001732151574\n","Epoch: 9475, Train Loss: 0.053930976308286735, Test Loss: 0.049256391904974904\n","Epoch: 9476, Train Loss: 0.053623459282202475, Test Loss: 0.050563545379435255\n","Epoch: 9477, Train Loss: 0.0539217370578851, Test Loss: 0.04924796333203561\n","Epoch: 9478, Train Loss: 0.053614405572580306, Test Loss: 0.05055508505156148\n","Epoch: 9479, Train Loss: 0.05391249371944251, Test Loss: 0.04923953100605646\n","Epoch: 9480, Train Loss: 0.05360534790255723, Test Loss: 0.05054662075488788\n","Epoch: 9481, Train Loss: 0.05390324629914859, Test Loss: 0.04923109493304121\n","Epoch: 9482, Train Loss: 0.05359628627815227, Test Loss: 0.05053815249577419\n","Epoch: 9483, Train Loss: 0.05389399480319534, Test Loss: 0.049222655118996705\n","Epoch: 9484, Train Loss: 0.053587220705387774, Test Loss: 0.05052968028058502\n","Epoch: 9485, Train Loss: 0.05388473923777994, Test Loss: 0.049214211569936384\n","Epoch: 9486, Train Loss: 0.053578151190293063, Test Loss: 0.05052120411569382\n","Epoch: 9487, Train Loss: 0.053875479609108594, Test Loss: 0.049205764291884176\n","Epoch: 9488, Train Loss: 0.05356907773890843, Test Loss: 0.05051272400748708\n","Epoch: 9489, Train Loss: 0.053866215923400816, Test Loss: 0.04919731329087784\n","Epoch: 9490, Train Loss: 0.0535600003572883, Test Loss: 0.05050423996236746\n","Epoch: 9491, Train Loss: 0.053856948186892434, Test Loss: 0.04918885857297273\n","Epoch: 9492, Train Loss: 0.05355091905150515, Test Loss: 0.05049575198675718\n","Epoch: 9493, Train Loss: 0.05384767640583894, Test Loss: 0.0491804001442439\n","Epoch: 9494, Train Loss: 0.0535418338276516, Test Loss: 0.050487260087100985\n","Epoch: 9495, Train Loss: 0.05383840058651867, Test Loss: 0.04917193801079008\n","Epoch: 9496, Train Loss: 0.05353274469184438, Test Loss: 0.050478764269869\n","Epoch: 9497, Train Loss: 0.05382912073523535, Test Loss: 0.04916347217873561\n","Epoch: 9498, Train Loss: 0.05352365165022624, Test Loss: 0.0504702645415593\n","Epoch: 9499, Train Loss: 0.053819836858320806, Test Loss: 0.049155002654232775\n","Epoch: 9500, Train Loss: 0.05351455470896847, Test Loss: 0.050461760908699894\n","Epoch: 9501, Train Loss: 0.05381054896213694, Test Loss: 0.049146529443464176\n","Epoch: 9502, Train Loss: 0.053505453874273017, Test Loss: 0.05045325337785118\n","Epoch: 9503, Train Loss: 0.05380125705307807, Test Loss: 0.04913805255264428\n","Epoch: 9504, Train Loss: 0.05349634915237421, Test Loss: 0.050444741955607206\n","Epoch: 9505, Train Loss: 0.053791961137572404, Test Loss: 0.04912957198802073\n","Epoch: 9506, Train Loss: 0.053487240549540016, Test Loss: 0.05043622664859645\n","Epoch: 9507, Train Loss: 0.05378266122208252, Test Loss: 0.04912108775587535\n","Epoch: 9508, Train Loss: 0.05347812807207319, Test Loss: 0.050427707463483754\n","Epoch: 9509, Train Loss: 0.053773357313107575, Test Loss: 0.04911259986252554\n","Epoch: 9510, Train Loss: 0.05346901172631236, Test Loss: 0.0504191844069705\n","Epoch: 9511, Train Loss: 0.05376404941718328, Test Loss: 0.049104108314324296\n","Epoch: 9512, Train Loss: 0.053459891518632376, Test Loss: 0.05041065748579521\n","Epoch: 9513, Train Loss: 0.053754737540882606, Test Loss: 0.04909561311766034\n","Epoch: 9514, Train Loss: 0.05345076745544412, Test Loss: 0.05040212670673308\n","Epoch: 9515, Train Loss: 0.05374542169081532, Test Loss: 0.049087114278958524\n","Epoch: 9516, Train Loss: 0.053441639543195286, Test Loss: 0.050393592076596196\n","Epoch: 9517, Train Loss: 0.05373610187362813, Test Loss: 0.04907861180467843\n","Epoch: 9518, Train Loss: 0.05343250778836865, Test Loss: 0.050385053602232034\n","Epoch: 9519, Train Loss: 0.05372677809600331, Test Loss: 0.04907010570131467\n","Epoch: 9520, Train Loss: 0.053423372197482395, Test Loss: 0.050376511290523784\n","Epoch: 9521, Train Loss: 0.05371745036465894, Test Loss: 0.04906159597539514\n","Epoch: 9522, Train Loss: 0.053414232777088566, Test Loss: 0.050367965148388455\n","Epoch: 9523, Train Loss: 0.05370811868634709, Test Loss: 0.04905308263348019\n","Epoch: 9524, Train Loss: 0.053405089533771995, Test Loss: 0.05035941518277553\n","Epoch: 9525, Train Loss: 0.05369878306785238, Test Loss: 0.04904456568216135\n","Epoch: 9526, Train Loss: 0.053395942474149166, Test Loss: 0.05035086140066571\n","Epoch: 9527, Train Loss: 0.05368944351599087, Test Loss: 0.049036045128058925\n","Epoch: 9528, Train Loss: 0.05338679160486573, Test Loss: 0.05034230380906826\n","Epoch: 9529, Train Loss: 0.05368010003760737, Test Loss: 0.04902752097782076\n","Epoch: 9530, Train Loss: 0.05337763693259518, Test Loss: 0.05033374241501946\n","Epoch: 9531, Train Loss: 0.053670752639573945, Test Loss: 0.049018993238119576\n","Epoch: 9532, Train Loss: 0.05336847846403643, Test Loss: 0.050325177225580595\n","Epoch: 9533, Train Loss: 0.053661401328787744, Test Loss: 0.04901046191565107\n","Epoch: 9534, Train Loss: 0.05335931620591157, Test Loss: 0.05031660824783498\n","Epoch: 9535, Train Loss: 0.053652046112168304, Test Loss: 0.04900192701713158\n","Epoch: 9536, Train Loss: 0.05335015016496378, Test Loss: 0.05030803548888579\n","Epoch: 9537, Train Loss: 0.053642686996655274, Test Loss: 0.048993388549295566\n","Epoch: 9538, Train Loss: 0.05334098034795459, Test Loss: 0.05029945895585351\n","Epoch: 9539, Train Loss: 0.05363332398920579, Test Loss: 0.048984846518892455\n","Epoch: 9540, Train Loss: 0.05333180676166087, Test Loss: 0.05029087865587251\n","Epoch: 9541, Train Loss: 0.05362395709679129, Test Loss: 0.04897630093268453\n","Epoch: 9542, Train Loss: 0.05332262941287259, Test Loss: 0.05028229459608901\n","Epoch: 9543, Train Loss: 0.05361458632639532, Test Loss: 0.04896775179744411\n","Epoch: 9544, Train Loss: 0.05331344830839014, Test Loss: 0.050273706783658334\n","Epoch: 9545, Train Loss: 0.053605211685010826, Test Loss: 0.04895919911995131\n","Epoch: 9546, Train Loss: 0.053304263455021775, Test Loss: 0.050265115225741966\n","Epoch: 9547, Train Loss: 0.053595833179637356, Test Loss: 0.048950642906990326\n","Epoch: 9548, Train Loss: 0.053295074859580294, Test Loss: 0.05025651992950408\n","Epoch: 9549, Train Loss: 0.05358645081727739, Test Loss: 0.048942083165347594\n","Epoch: 9550, Train Loss: 0.05328588252888075, Test Loss: 0.05024792090211\n","Epoch: 9551, Train Loss: 0.053577064604935186, Test Loss: 0.04893351990180853\n","Epoch: 9552, Train Loss: 0.053276686469737544, Test Loss: 0.050239318150722787\n","Epoch: 9553, Train Loss: 0.05356767454961281, Test Loss: 0.048924953123155986\n","Epoch: 9554, Train Loss: 0.05326748668896268, Test Loss: 0.05023071168250092\n","Epoch: 9555, Train Loss: 0.053558280658308526, Test Loss: 0.04891638283616672\n","Epoch: 9556, Train Loss: 0.05325828319336218, Test Loss: 0.05022210150459591\n","Epoch: 9557, Train Loss: 0.053548882938013784, Test Loss: 0.04890780904760979\n","Epoch: 9558, Train Loss: 0.05324907598973462, Test Loss: 0.05021348762414988\n","Epoch: 9559, Train Loss: 0.05353948139571124, Test Loss: 0.04889923176424414\n","Epoch: 9560, Train Loss: 0.05323986508486862, Test Loss: 0.05020487004829326\n","Epoch: 9561, Train Loss: 0.05353007603837238, Test Loss: 0.04889065099281621\n","Epoch: 9562, Train Loss: 0.053230650485540446, Test Loss: 0.05019624878414277\n","Epoch: 9563, Train Loss: 0.05352066687295526, Test Loss: 0.04888206674005836\n","Epoch: 9564, Train Loss: 0.0532214321985124, Test Loss: 0.05018762383879959\n","Epoch: 9565, Train Loss: 0.05351125390640314, Test Loss: 0.04887347901268741\n","Epoch: 9566, Train Loss: 0.05321221023053143, Test Loss: 0.050178995219348044\n","Epoch: 9567, Train Loss: 0.053501837145642724, Test Loss: 0.048864887817402586\n","Epoch: 9568, Train Loss: 0.053202984588326986, Test Loss: 0.050170362932854054\n","Epoch: 9569, Train Loss: 0.053492416597582866, Test Loss: 0.04885629316088496\n","Epoch: 9570, Train Loss: 0.05319375527861055, Test Loss: 0.050161726986363446\n","Epoch: 9571, Train Loss: 0.053482992269112926, Test Loss: 0.04884769504979527\n","Epoch: 9572, Train Loss: 0.053184522308073315, Test Loss: 0.05015308738690114\n","Epoch: 9573, Train Loss: 0.0534735641671018, Test Loss: 0.048839093490774005\n","Epoch: 9574, Train Loss: 0.05317528568338646, Test Loss: 0.05014444414147069\n","Epoch: 9575, Train Loss: 0.05346413229839751, Test Loss: 0.04883048849043974\n","Epoch: 9576, Train Loss: 0.05316604541119929, Test Loss: 0.05013579725705246\n","Epoch: 9577, Train Loss: 0.05345469666982554, Test Loss: 0.04882188005538914\n","Epoch: 9578, Train Loss: 0.053156801498139374, Test Loss: 0.05012714674060514\n","Epoch: 9579, Train Loss: 0.05344525728819005, Test Loss: 0.04881326819219709\n","Epoch: 9580, Train Loss: 0.05314755395081259, Test Loss: 0.050118492599063556\n","Epoch: 9581, Train Loss: 0.05343581416027202, Test Loss: 0.048804652907415945\n","Epoch: 9582, Train Loss: 0.053138302775802455, Test Loss: 0.050109834839340214\n","Epoch: 9583, Train Loss: 0.053426367292830505, Test Loss: 0.04879603420757554\n","Epoch: 9584, Train Loss: 0.05312904797967012, Test Loss: 0.05010117346832468\n","Epoch: 9585, Train Loss: 0.053416916692602213, Test Loss: 0.04878741209918406\n","Epoch: 9586, Train Loss: 0.05311978956895518, Test Loss: 0.050092508492884186\n","Epoch: 9587, Train Loss: 0.05340746236630197, Test Loss: 0.04877878658872838\n","Epoch: 9588, Train Loss: 0.053110527550176104, Test Loss: 0.050083839919864426\n","Epoch: 9589, Train Loss: 0.053398004320623665, Test Loss: 0.04877015768267425\n","Epoch: 9590, Train Loss: 0.0531012619298304, Test Loss: 0.05007516775609014\n","Epoch: 9591, Train Loss: 0.05338854256224061, Test Loss: 0.04876152538746805\n","Epoch: 9592, Train Loss: 0.053091992714396324, Test Loss: 0.050066492008366076\n","Epoch: 9593, Train Loss: 0.05337907709780665, Test Loss: 0.04875288970953635\n","Epoch: 9594, Train Loss: 0.05308271991033257, Test Loss: 0.05005781268347812\n","Epoch: 9595, Train Loss: 0.05336960793395732, Test Loss: 0.048744250655288986\n","Epoch: 9596, Train Loss: 0.05307344352408127, Test Loss: 0.050049129788194875\n","Epoch: 9597, Train Loss: 0.0533601350773112, Test Loss: 0.04873560823111862\n","Epoch: 9598, Train Loss: 0.05306416356206748, Test Loss: 0.050040443329268436\n","Epoch: 9599, Train Loss: 0.05335065853447101, Test Loss: 0.04872696244340327\n","Epoch: 9600, Train Loss: 0.05305488003070201, Test Loss: 0.05003175331343653\n","Epoch: 9601, Train Loss: 0.05334117831202537, Test Loss: 0.048718313298506996\n","Epoch: 9602, Train Loss: 0.05304559293638168, Test Loss: 0.050023059747423326\n","Epoch: 9603, Train Loss: 0.053331694416549814, Test Loss: 0.04870966080278212\n","Epoch: 9604, Train Loss: 0.053036302285492054, Test Loss: 0.05001436263794242\n","Epoch: 9605, Train Loss: 0.05332220685460983, Test Loss: 0.048701004962570724\n","Epoch: 9606, Train Loss: 0.053027008084408536, Test Loss: 0.05000566199169708\n","Epoch: 9607, Train Loss: 0.05331271563276087, Test Loss: 0.0486923457842062\n","Epoch: 9608, Train Loss: 0.05301771033949815, Test Loss: 0.04999695781538275\n","Epoch: 9609, Train Loss: 0.053303220757550986, Test Loss: 0.04868368327401471\n","Epoch: 9610, Train Loss: 0.05300840905712096, Test Loss: 0.04998825011568924\n","Epoch: 9611, Train Loss: 0.05329372223552305, Test Loss: 0.04867501743831805\n","Epoch: 9612, Train Loss: 0.052999104243632925, Test Loss: 0.04997953889930172\n","Epoch: 9613, Train Loss: 0.05328422007321552, Test Loss: 0.048666348283434385\n","Epoch: 9614, Train Loss: 0.05298979590538664, Test Loss: 0.049970824172903605\n","Epoch: 9615, Train Loss: 0.05327471427716558, Test Loss: 0.04865767581568024\n","Epoch: 9616, Train Loss: 0.05298048404873339, Test Loss: 0.04996210594317698\n","Epoch: 9617, Train Loss: 0.053265204853909454, Test Loss: 0.04864900004137271\n","Epoch: 9618, Train Loss: 0.05297116868002537, Test Loss: 0.04995338421680594\n","Epoch: 9619, Train Loss: 0.05325569180998558, Test Loss: 0.048640320966830866\n","Epoch: 9620, Train Loss: 0.05296184980561704, Test Loss: 0.04994465900047739\n","Epoch: 9621, Train Loss: 0.05324617515193566, Test Loss: 0.048631638598377665\n","Epoch: 9622, Train Loss: 0.052952527431867034, Test Loss: 0.04993593030088299\n","Epoch: 9623, Train Loss: 0.05323665488630639, Test Loss: 0.04862295294234076\n","Epoch: 9624, Train Loss: 0.05294320156513909, Test Loss: 0.049927198124720594\n","Epoch: 9625, Train Loss: 0.05322713101965094, Test Loss: 0.04861426400505523\n","Epoch: 9626, Train Loss: 0.052933872211804726, Test Loss: 0.04991846247869573\n","Epoch: 9627, Train Loss: 0.0532176035585306, Test Loss: 0.04860557179286396\n","Epoch: 9628, Train Loss: 0.052924539378243526, Test Loss: 0.04990972336952375\n","Epoch: 9629, Train Loss: 0.053208072509516556, Test Loss: 0.04859687631211986\n","Epoch: 9630, Train Loss: 0.05291520307084558, Test Loss: 0.04990098080393016\n","Epoch: 9631, Train Loss: 0.05319853787919061, Test Loss: 0.048588177569186417\n","Epoch: 9632, Train Loss: 0.0529058632960118, Test Loss: 0.049892234788652394\n","Epoch: 9633, Train Loss: 0.05318899967414668, Test Loss: 0.04857947557043886\n","Epoch: 9634, Train Loss: 0.052896520060155355, Test Loss: 0.04988348533044047\n","Epoch: 9635, Train Loss: 0.05317945790099155, Test Loss: 0.04857077032226538\n","Epoch: 9636, Train Loss: 0.05288717336970265, Test Loss: 0.049874732436058646\n","Epoch: 9637, Train Loss: 0.05316991256634651, Test Loss: 0.04856206183106852\n","Epoch: 9638, Train Loss: 0.052877823231094835, Test Loss: 0.0498659761122859\n","Epoch: 9639, Train Loss: 0.05316036367684785, Test Loss: 0.04855335010326519\n","Epoch: 9640, Train Loss: 0.052868469650787855, Test Loss: 0.04985721636591645\n","Epoch: 9641, Train Loss: 0.05315081123914731, Test Loss: 0.04854463514528719\n","Epoch: 9642, Train Loss: 0.05285911263525291, Test Loss: 0.049848453203759885\n","Epoch: 9643, Train Loss: 0.053141255259912326, Test Loss: 0.048535916963581525\n","Epoch: 9644, Train Loss: 0.05284975219097677, Test Loss: 0.049839686632642745\n","Epoch: 9645, Train Loss: 0.05313169574582749, Test Loss: 0.04852719556461203\n","Epoch: 9646, Train Loss: 0.052840388324463444, Test Loss: 0.049830916659407566\n","Epoch: 9647, Train Loss: 0.05312213270359368, Test Loss: 0.048518470954858076\n","Epoch: 9648, Train Loss: 0.052831021042232806, Test Loss: 0.049822143290914105\n","Epoch: 9649, Train Loss: 0.0531125661399292, Test Loss: 0.048509743140815506\n","Epoch: 9650, Train Loss: 0.05282165035082171, Test Loss: 0.04981336653403809\n","Epoch: 9651, Train Loss: 0.05310299606156867, Test Loss: 0.04850101212899571\n","Epoch: 9652, Train Loss: 0.052812276256782836, Test Loss: 0.049804586395672104\n","Epoch: 9653, Train Loss: 0.053093422475263725, Test Loss: 0.04849227792592654\n","Epoch: 9654, Train Loss: 0.0528028987666857, Test Loss: 0.049795802882724526\n","Epoch: 9655, Train Loss: 0.053083845387782214, Test Loss: 0.04848354053815059\n","Epoch: 9656, Train Loss: 0.05279351788711502, Test Loss: 0.04978701600211957\n","Epoch: 9657, Train Loss: 0.05307426480590793, Test Loss: 0.04847479997222603\n","Epoch: 9658, Train Loss: 0.052784133624671326, Test Loss: 0.04977822576079646\n","Epoch: 9659, Train Loss: 0.05306468073644008, Test Loss: 0.04846605623472525\n","Epoch: 9660, Train Loss: 0.05277474598596984, Test Loss: 0.0497694321657094\n","Epoch: 9661, Train Loss: 0.053055093186193066, Test Loss: 0.04845730933223442\n","Epoch: 9662, Train Loss: 0.0527653549776399, Test Loss: 0.04976063522382576\n","Epoch: 9663, Train Loss: 0.053045502161994725, Test Loss: 0.04844855927135273\n","Epoch: 9664, Train Loss: 0.0527559606063242, Test Loss: 0.049751834942125935\n","Epoch: 9665, Train Loss: 0.05303590767068633, Test Loss: 0.04843980605869089\n","Epoch: 9666, Train Loss: 0.05274656287867727, Test Loss: 0.04974303132760223\n","Epoch: 9667, Train Loss: 0.0530263097191213, Test Loss: 0.04843104970087186\n","Epoch: 9668, Train Loss: 0.05273716180136624, Test Loss: 0.049734224387258136\n","Epoch: 9669, Train Loss: 0.05301670831416453, Test Loss: 0.04842229020452758\n","Epoch: 9670, Train Loss: 0.05272775738106765, Test Loss: 0.04972541412810663\n","Epoch: 9671, Train Loss: 0.05300710346269081, Test Loss: 0.0484135275762999\n","Epoch: 9672, Train Loss: 0.05271834962446809, Test Loss: 0.0497166005571695\n","Epoch: 9673, Train Loss: 0.052997495171583975, Test Loss: 0.048404761822838045\n","Epoch: 9674, Train Loss: 0.05270893853826199, Test Loss: 0.04970778368147606\n","Epoch: 9675, Train Loss: 0.052987883447735785, Test Loss: 0.0483959929507977\n","Epoch: 9676, Train Loss: 0.052699524129150466, Test Loss: 0.049698963508061585\n","Epoch: 9677, Train Loss: 0.052978268298044306, Test Loss: 0.04838722096684092\n","Epoch: 9678, Train Loss: 0.05269010640384136, Test Loss: 0.04969014004396676\n","Epoch: 9679, Train Loss: 0.05296864972941337, Test Loss: 0.048378445877633326\n","Epoch: 9680, Train Loss: 0.0526806853690463, Test Loss: 0.04968131329623592\n","Epoch: 9681, Train Loss: 0.05295902774875086, Test Loss: 0.048369667689844215\n","Epoch: 9682, Train Loss: 0.05267126103148098, Test Loss: 0.04967248327191594\n","Epoch: 9683, Train Loss: 0.05294940236296753, Test Loss: 0.048360886410144154\n","Epoch: 9684, Train Loss: 0.05266183339786253, Test Loss: 0.04966364997805479\n","Epoch: 9685, Train Loss: 0.05293977357897559, Test Loss: 0.04835210204520515\n","Epoch: 9686, Train Loss: 0.05265240247490984, Test Loss: 0.04965481342170087\n","Epoch: 9687, Train Loss: 0.052930141403688215, Test Loss: 0.048343314601698044\n","Epoch: 9688, Train Loss: 0.052642968269340874, Test Loss: 0.049645973609900744\n","Epoch: 9689, Train Loss: 0.05292050584401698, Test Loss: 0.04833452408629157\n","Epoch: 9690, Train Loss: 0.05263353078787181, Test Loss: 0.04963713054969909\n","Epoch: 9691, Train Loss: 0.05291086690687208, Test Loss: 0.048325730505653294\n","Epoch: 9692, Train Loss: 0.05262409003721782, Test Loss: 0.04962828424813724\n","Epoch: 9693, Train Loss: 0.05290122459916075, Test Loss: 0.048316933866445595\n","Epoch: 9694, Train Loss: 0.052614646024089225, Test Loss: 0.049619434712251935\n","Epoch: 9695, Train Loss: 0.052891578927786086, Test Loss: 0.048308134175326214\n","Epoch: 9696, Train Loss: 0.052605198755191866, Test Loss: 0.049610581949074056\n","Epoch: 9697, Train Loss: 0.05288192989964571, Test Loss: 0.048299331438947636\n","Epoch: 9698, Train Loss: 0.052595748237226604, Test Loss: 0.049601725965628715\n","Epoch: 9699, Train Loss: 0.052872277521631976, Test Loss: 0.0482905256639553\n","Epoch: 9700, Train Loss: 0.05258629447688744, Test Loss: 0.04959286676893346\n","Epoch: 9701, Train Loss: 0.05286262180063003, Test Loss: 0.04828171685698788\n","Epoch: 9702, Train Loss: 0.05257683748086193, Test Loss: 0.04958400436599794\n","Epoch: 9703, Train Loss: 0.05285296274351766, Test Loss: 0.04827290502467514\n","Epoch: 9704, Train Loss: 0.05256737725582887, Test Loss: 0.04957513876382295\n","Epoch: 9705, Train Loss: 0.05284330035716417, Test Loss: 0.04826409017363855\n","Epoch: 9706, Train Loss: 0.05255791380845898, Test Loss: 0.04956626996939967\n","Epoch: 9707, Train Loss: 0.052833634648429766, Test Loss: 0.04825527231049004\n","Epoch: 9708, Train Loss: 0.052548447145413776, Test Loss: 0.049557397989710546\n","Epoch: 9709, Train Loss: 0.052823965624166296, Test Loss: 0.0482464514418321\n","Epoch: 9710, Train Loss: 0.05253897727334536, Test Loss: 0.0495485228317262\n","Epoch: 9711, Train Loss: 0.05281429329121433, Test Loss: 0.04823762757425666\n","Epoch: 9712, Train Loss: 0.05252950419889563, Test Loss: 0.049539644502408005\n","Epoch: 9713, Train Loss: 0.05280461765640556, Test Loss: 0.04822880071434588\n","Epoch: 9714, Train Loss: 0.05252002792869692, Test Loss: 0.049530763008706236\n","Epoch: 9715, Train Loss: 0.05279493872656113, Test Loss: 0.048219970868671226\n","Epoch: 9716, Train Loss: 0.052510548469371074, Test Loss: 0.04952187835756017\n","Epoch: 9717, Train Loss: 0.052785256508491564, Test Loss: 0.04821113804379356\n","Epoch: 9718, Train Loss: 0.05250106582752955, Test Loss: 0.04951299055589842\n","Epoch: 9719, Train Loss: 0.05277557100899723, Test Loss: 0.04820230224626305\n","Epoch: 9720, Train Loss: 0.05249158000977333, Test Loss: 0.04950409961063837\n","Epoch: 9721, Train Loss: 0.05276588223486772, Test Loss: 0.0481934634826194\n","Epoch: 9722, Train Loss: 0.05248209102269318, Test Loss: 0.049495205528687004\n","Epoch: 9723, Train Loss: 0.05275619019288263, Test Loss: 0.0481846217593922\n","Epoch: 9724, Train Loss: 0.05247259887286997, Test Loss: 0.0494863083169408\n","Epoch: 9725, Train Loss: 0.05274649488981153, Test Loss: 0.04817577708310081\n","Epoch: 9726, Train Loss: 0.05246310356687457, Test Loss: 0.0494774079822859\n","Epoch: 9727, Train Loss: 0.05273679633241418, Test Loss: 0.04816692946025476\n","Epoch: 9728, Train Loss: 0.05245360511126828, Test Loss: 0.0494685045315988\n","Epoch: 9729, Train Loss: 0.052727094527440996, Test Loss: 0.04815807889735469\n","Epoch: 9730, Train Loss: 0.052444103512603715, Test Loss: 0.04945959797174665\n","Epoch: 9731, Train Loss: 0.05271738948163359, Test Loss: 0.048149225400892\n","Epoch: 9732, Train Loss: 0.05243459877742447, Test Loss: 0.04945068830958802\n","Epoch: 9733, Train Loss: 0.05270768120172542, Test Loss: 0.04814036897735007\n","Epoch: 9734, Train Loss: 0.05242509091226648, Test Loss: 0.04944177555197271\n","Epoch: 9735, Train Loss: 0.05269796969444157, Test Loss: 0.048131509633204464\n","Epoch: 9736, Train Loss: 0.05241557992365803, Test Loss: 0.04943285970574351\n","Epoch: 9737, Train Loss: 0.05268825496650058, Test Loss: 0.0481226473749232\n","Epoch: 9738, Train Loss: 0.05240606581812008, Test Loss: 0.04942394077773553\n","Epoch: 9739, Train Loss: 0.0526785370246137, Test Loss: 0.04811378220896884\n","Epoch: 9740, Train Loss: 0.05239654860216858, Test Loss: 0.04941501877477895\n","Epoch: 9741, Train Loss: 0.05266881587548768, Test Loss: 0.04810491414179785\n","Epoch: 9742, Train Loss: 0.05238702828231341, Test Loss: 0.04940609370369739\n","Epoch: 9743, Train Loss: 0.05265909152582313, Test Loss: 0.048096043179861464\n","Epoch: 9744, Train Loss: 0.05237750486505968, Test Loss: 0.04939716557131015\n","Epoch: 9745, Train Loss: 0.05264936398231679, Test Loss: 0.04808716932960715\n","Epoch: 9746, Train Loss: 0.05236797835690898, Test Loss: 0.04938823438443297\n","Epoch: 9747, Train Loss: 0.05263963325166215, Test Loss: 0.04807829259747883\n","Epoch: 9748, Train Loss: 0.05235844876435955, Test Loss: 0.04937930014987811\n","Epoch: 9749, Train Loss: 0.052629899340549686, Test Loss: 0.04806941298991776\n","Epoch: 9750, Train Loss: 0.05234891609390738, Test Loss: 0.049370362874455814\n","Epoch: 9751, Train Loss: 0.052620162255668214, Test Loss: 0.04806053051336359\n","Epoch: 9752, Train Loss: 0.052339380352047084, Test Loss: 0.04936142256497486\n","Epoch: 9753, Train Loss: 0.0526104220037055, Test Loss: 0.048051645174255\n","Epoch: 9754, Train Loss: 0.052329841545272614, Test Loss: 0.04935247922824356\n","Epoch: 9755, Train Loss: 0.05260067859134918, Test Loss: 0.04804275697902984\n","Epoch: 9756, Train Loss: 0.05232029968007743, Test Loss: 0.04934353287106965\n","Epoch: 9757, Train Loss: 0.05259093202528676, Test Loss: 0.04803386593412684\n","Epoch: 9758, Train Loss: 0.05231075476295612, Test Loss: 0.04933458350026254\n","Epoch: 9759, Train Loss: 0.05258118231220773, Test Loss: 0.04802497204598595\n","Epoch: 9760, Train Loss: 0.052301206800404954, Test Loss: 0.04932563112263262\n","Epoch: 9761, Train Loss: 0.05257142945880298, Test Loss: 0.04801607532104871\n","Epoch: 9762, Train Loss: 0.05229165579892192, Test Loss: 0.04931667574499304\n","Epoch: 9763, Train Loss: 0.05256167347176645, Test Loss: 0.04800717576575948\n","Epoch: 9764, Train Loss: 0.05228210176500831, Test Loss: 0.049307717374159794\n","Epoch: 9765, Train Loss: 0.05255191435779536, Test Loss: 0.04799827338656581\n","Epoch: 9766, Train Loss: 0.05227254470516882, Test Loss: 0.04929875601695222\n","Epoch: 9767, Train Loss: 0.0525421521235906, Test Loss: 0.04798936818991867\n","Epoch: 9768, Train Loss: 0.05226298462591204, Test Loss: 0.04928979168019387\n","Epoch: 9769, Train Loss: 0.05253238677585762, Test Loss: 0.0479804601822735\n","Epoch: 9770, Train Loss: 0.05225342153375125, Test Loss: 0.04928082437071284\n","Epoch: 9771, Train Loss: 0.05252261832130681, Test Loss: 0.04797154937009002\n","Epoch: 9772, Train Loss: 0.05224385543520434, Test Loss: 0.049271854095342606\n","Epoch: 9773, Train Loss: 0.0525128467666542, Test Loss: 0.04796263575983407\n","Epoch: 9774, Train Loss: 0.052234286336795714, Test Loss: 0.0492628808609219\n","Epoch: 9775, Train Loss: 0.0525030721186215, Test Loss: 0.04795371935797597\n","Epoch: 9776, Train Loss: 0.05222471424505464, Test Loss: 0.0492539046742951\n","Epoch: 9777, Train Loss: 0.05249329438393639, Test Loss: 0.047944800170992195\n","Epoch: 9778, Train Loss: 0.05221513916651683, Test Loss: 0.04924492554231288\n","Epoch: 9779, Train Loss: 0.052483513569333215, Test Loss: 0.04793587820536518\n","Epoch: 9780, Train Loss: 0.05220556110772428, Test Loss: 0.049235943471832105\n","Epoch: 9781, Train Loss: 0.052473729681552825, Test Loss: 0.04792695346758377\n","Epoch: 9782, Train Loss: 0.05219598007522574, Test Loss: 0.04922695846971615\n","Epoch: 9783, Train Loss: 0.052463942727342935, Test Loss: 0.047918025964142956\n","Epoch: 9784, Train Loss: 0.05218639607557655, Test Loss: 0.049217970542834825\n","Epoch: 9785, Train Loss: 0.05245415271345801, Test Loss: 0.0479090957015439\n","Epoch: 9786, Train Loss: 0.05217680911533842, Test Loss: 0.049208979698064384\n","Epoch: 9787, Train Loss: 0.052444359646659465, Test Loss: 0.04790016268629447\n","Epoch: 9788, Train Loss: 0.05216721920108017, Test Loss: 0.049199985942287784\n","Epoch: 9789, Train Loss: 0.05243456353371548, Test Loss: 0.04789122692490867\n","Epoch: 9790, Train Loss: 0.05215762633937722, Test Loss: 0.049190989282394494\n","Epoch: 9791, Train Loss: 0.05242476438140139, Test Loss: 0.04788228842390672\n","Epoch: 9792, Train Loss: 0.05214803053681134, Test Loss: 0.04918198972527996\n","Epoch: 9793, Train Loss: 0.05241496219649862, Test Loss: 0.04787334718981499\n","Epoch: 9794, Train Loss: 0.05213843179997087, Test Loss: 0.04917298727784588\n","Epoch: 9795, Train Loss: 0.0524051569857954, Test Loss: 0.04786440322916556\n","Epoch: 9796, Train Loss: 0.05212883013545036, Test Loss: 0.04916398194699974\n","Epoch: 9797, Train Loss: 0.052395348756085966, Test Loss: 0.047855456548495746\n","Epoch: 9798, Train Loss: 0.05211922554984964, Test Loss: 0.04915497373965433\n","Epoch: 9799, Train Loss: 0.05238553751417029, Test Loss: 0.047846507154348794\n","Epoch: 9800, Train Loss: 0.05210961804977502, Test Loss: 0.04914596266272794\n","Epoch: 9801, Train Loss: 0.052375723266854184, Test Loss: 0.047837555053272106\n","Epoch: 9802, Train Loss: 0.05210000764183722, Test Loss: 0.04913694872314348\n","Epoch: 9803, Train Loss: 0.05236590602094841, Test Loss: 0.04782860025181776\n","Epoch: 9804, Train Loss: 0.05209039433265194, Test Loss: 0.04912793192782798\n","Epoch: 9805, Train Loss: 0.05235608578326836, Test Loss: 0.04781964275654206\n","Epoch: 9806, Train Loss: 0.052080778128839454, Test Loss: 0.04911891228371247\n","Epoch: 9807, Train Loss: 0.05234626256063361, Test Loss: 0.04781068257400426\n","Epoch: 9808, Train Loss: 0.052071159037023376, Test Loss: 0.04910988979773122\n","Epoch: 9809, Train Loss: 0.05233643635986746, Test Loss: 0.04780171971076739\n","Epoch: 9810, Train Loss: 0.05206153706383128, Test Loss: 0.049100864476821764\n","Epoch: 9811, Train Loss: 0.05232660718779688, Test Loss: 0.047792754173396804\n","Epoch: 9812, Train Loss: 0.05205191221589349, Test Loss: 0.04909183632792342\n","Epoch: 9813, Train Loss: 0.05231677505125093, Test Loss: 0.04778378596845982\n","Epoch: 9814, Train Loss: 0.052042284499842495, Test Loss: 0.04908280535797767\n","Epoch: 9815, Train Loss: 0.05230693995706129, Test Loss: 0.047774815102525056\n","Epoch: 9816, Train Loss: 0.05203265392231256, Test Loss: 0.049073771573926984\n","Epoch: 9817, Train Loss: 0.052297101912060995, Test Loss: 0.04776584158216243\n","Epoch: 9818, Train Loss: 0.05202302048993931, Test Loss: 0.049064734982714325\n","Epoch: 9819, Train Loss: 0.05228726092308401, Test Loss: 0.0477568654139418\n","Epoch: 9820, Train Loss: 0.05201338420935884, Test Loss: 0.04905569559128322\n","Epoch: 9821, Train Loss: 0.05227741699696518, Test Loss: 0.04774788660443321\n","Epoch: 9822, Train Loss: 0.0520037450872076, Test Loss: 0.049046653406576025\n","Epoch: 9823, Train Loss: 0.05226757014053866, Test Loss: 0.04773890516020567\n","Epoch: 9824, Train Loss: 0.051994103130121316, Test Loss: 0.04903760843553453\n","Epoch: 9825, Train Loss: 0.05225772036063847, Test Loss: 0.0477299210878275\n","Epoch: 9826, Train Loss: 0.051984458344735376, Test Loss: 0.049028560685098664\n","Epoch: 9827, Train Loss: 0.05224786766409707, Test Loss: 0.04772093439386481\n","Epoch: 9828, Train Loss: 0.05197481073768311, Test Loss: 0.04901951016220665\n","Epoch: 9829, Train Loss: 0.05223801205774579, Test Loss: 0.04771194508488127\n","Epoch: 9830, Train Loss: 0.05196516031559586, Test Loss: 0.04901045687379329\n","Epoch: 9831, Train Loss: 0.05222815354841281, Test Loss: 0.04770295316743826\n","Epoch: 9832, Train Loss: 0.05195550708510281, Test Loss: 0.04900140082679114\n","Epoch: 9833, Train Loss: 0.052218292142924574, Test Loss: 0.047693958648093664\n","Epoch: 9834, Train Loss: 0.05194585105283008, Test Loss: 0.04899234202812881\n","Epoch: 9835, Train Loss: 0.052208427848104, Test Loss: 0.0476849615334021\n","Epoch: 9836, Train Loss: 0.0519361922254006, Test Loss: 0.04898328048473073\n","Epoch: 9837, Train Loss: 0.052198560670770264, Test Loss: 0.047675961829913745\n","Epoch: 9838, Train Loss: 0.051926530609433266, Test Loss: 0.04897421620351744\n","Epoch: 9839, Train Loss: 0.052188690617739016, Test Loss: 0.04766695954417431\n","Epoch: 9840, Train Loss: 0.05191686621154284, Test Loss: 0.04896514919140388\n","Epoch: 9841, Train Loss: 0.052178817695820855, Test Loss: 0.047657954682725136\n","Epoch: 9842, Train Loss: 0.05190719903833984, Test Loss: 0.04895607945530108\n","Epoch: 9843, Train Loss: 0.05216894191182278, Test Loss: 0.04764894725210234\n","Epoch: 9844, Train Loss: 0.05189752909642986, Test Loss: 0.04894700700211357\n","Epoch: 9845, Train Loss: 0.05215906327254581, Test Loss: 0.04763993725883651\n","Epoch: 9846, Train Loss: 0.05188785639241333, Test Loss: 0.04893793183874107\n","Epoch: 9847, Train Loss: 0.05214918178478659, Test Loss: 0.047630924709453064\n","Epoch: 9848, Train Loss: 0.051878180932885674, Test Loss: 0.0489288539720769\n","Epoch: 9849, Train Loss: 0.05213929745533578, Test Loss: 0.04762190961047184\n","Epoch: 9850, Train Loss: 0.05186850272443705, Test Loss: 0.04891977340900885\n","Epoch: 9851, Train Loss: 0.05212941029097895, Test Loss: 0.0476128919684063\n","Epoch: 9852, Train Loss: 0.051858821773651545, Test Loss: 0.048910690156418704\n","Epoch: 9853, Train Loss: 0.052119520298496126, Test Loss: 0.04760387178976436\n","Epoch: 9854, Train Loss: 0.05184913808710787, Test Loss: 0.04890160422118133\n","Epoch: 9855, Train Loss: 0.05210962748466091, Test Loss: 0.047594849081047885\n","Epoch: 9856, Train Loss: 0.051839451671379, Test Loss: 0.04889251561016606\n","Epoch: 9857, Train Loss: 0.052099731856241664, Test Loss: 0.047585823848752526\n","Epoch: 9858, Train Loss: 0.051829762533032044, Test Loss: 0.048883424330236014\n","Epoch: 9859, Train Loss: 0.052089833420001085, Test Loss: 0.04757679609936805\n","Epoch: 9860, Train Loss: 0.051820070678628344, Test Loss: 0.048874330388247414\n","Epoch: 9861, Train Loss: 0.05207993218269543, Test Loss: 0.04756776583937817\n","Epoch: 9862, Train Loss: 0.05181037611472358, Test Loss: 0.048865233791051066\n","Epoch: 9863, Train Loss: 0.052070028151075955, Test Loss: 0.047558733075261074\n","Epoch: 9864, Train Loss: 0.05180067884786818, Test Loss: 0.048856134545491654\n","Epoch: 9865, Train Loss: 0.05206012133188817, Test Loss: 0.04754969781348874\n","Epoch: 9866, Train Loss: 0.051790978884606556, Test Loss: 0.04884703265840754\n","Epoch: 9867, Train Loss: 0.05205021173187177, Test Loss: 0.047540660060527706\n","Epoch: 9868, Train Loss: 0.05178127623147798, Test Loss: 0.048837928136631945\n","Epoch: 9869, Train Loss: 0.05204029935776174, Test Loss: 0.047531619822839224\n","Epoch: 9870, Train Loss: 0.05177157089501679, Test Loss: 0.04882882098699225\n","Epoch: 9871, Train Loss: 0.05203038421628767, Test Loss: 0.047522577106879474\n","Epoch: 9872, Train Loss: 0.051761862881752434, Test Loss: 0.04881971121631079\n","Epoch: 9873, Train Loss: 0.05202046631417454, Test Loss: 0.04751353191909905\n","Epoch: 9874, Train Loss: 0.051752152198209105, Test Loss: 0.04881059883140484\n","Epoch: 9875, Train Loss: 0.05201054565814261, Test Loss: 0.047504484265944585\n","Epoch: 9876, Train Loss: 0.05174243885090733, Test Loss: 0.048801483839086945\n","Epoch: 9877, Train Loss: 0.05200062225490798, Test Loss: 0.047495434153857846\n","Epoch: 9878, Train Loss: 0.05173272284636306, Test Loss: 0.04879236624616518\n","Epoch: 9879, Train Loss: 0.05199069611118256, Test Loss: 0.04748638158927676\n","Epoch: 9880, Train Loss: 0.051723004191088634, Test Loss: 0.04878324605944352\n","Epoch: 9881, Train Loss: 0.051980767233674716, Test Loss: 0.047477326578634675\n","Epoch: 9882, Train Loss: 0.05171328289159221, Test Loss: 0.04877412328572196\n","Epoch: 9883, Train Loss: 0.0519708356290892, Test Loss: 0.04746826912836254\n","Epoch: 9884, Train Loss: 0.051703558954379725, Test Loss: 0.048764997931797234\n","Epoch: 9885, Train Loss: 0.051960901304127914, Test Loss: 0.04745920924488735\n","Epoch: 9886, Train Loss: 0.05169383238595353, Test Loss: 0.04875587000446313\n","Epoch: 9887, Train Loss: 0.05195096426549027, Test Loss: 0.047450146934633536\n","Epoch: 9888, Train Loss: 0.051684103192813725, Test Loss: 0.048746739510510374\n","Epoch: 9889, Train Loss: 0.05194102451987301, Test Loss: 0.047441082204022905\n","Epoch: 9890, Train Loss: 0.051674371381458, Test Loss: 0.04873760645672721\n","Epoch: 9891, Train Loss: 0.051931082073970784, Test Loss: 0.047432015059475176\n","Epoch: 9892, Train Loss: 0.0516646369583824, Test Loss: 0.04872847084989966\n","Epoch: 9893, Train Loss: 0.051921136934476524, Test Loss: 0.04742294550740806\n","Epoch: 9894, Train Loss: 0.0516548999300812, Test Loss: 0.048719332696812834\n","Epoch: 9895, Train Loss: 0.0519111891080825, Test Loss: 0.04741387355423811\n","Epoch: 9896, Train Loss: 0.05164516030304773, Test Loss: 0.04871019200424907\n","Epoch: 9897, Train Loss: 0.05190123860147878, Test Loss: 0.047404799206379876\n","Epoch: 9898, Train Loss: 0.0516354180837738, Test Loss: 0.04870104877899056\n","Epoch: 9899, Train Loss: 0.05189128542135562, Test Loss: 0.04739572247024777\n","Epoch: 9900, Train Loss: 0.05162567327875132, Test Loss: 0.048691903027818235\n","Epoch: 9901, Train Loss: 0.05188132957440241, Test Loss: 0.047386643352255446\n","Epoch: 9902, Train Loss: 0.051615925894471684, Test Loss: 0.04868275475751309\n","Epoch: 9903, Train Loss: 0.05187137106730907, Test Loss: 0.04737756185881603\n","Epoch: 9904, Train Loss: 0.051606175937426293, Test Loss: 0.04867360397485493\n","Epoch: 9905, Train Loss: 0.05186140990676465, Test Loss: 0.047368477996342376\n","Epoch: 9906, Train Loss: 0.05159642341410632, Test Loss: 0.0486644506866242\n","Epoch: 9907, Train Loss: 0.0518514460994593, Test Loss: 0.04735939177124782\n","Epoch: 9908, Train Loss: 0.051586668331003935, Test Loss: 0.04865529489960116\n","Epoch: 9909, Train Loss: 0.05184147965208337, Test Loss: 0.04735030318994575\n","Epoch: 9910, Train Loss: 0.05157691069461159, Test Loss: 0.048646136620566785\n","Epoch: 9911, Train Loss: 0.0518315105713283, Test Loss: 0.04734121225885031\n","Epoch: 9912, Train Loss: 0.051567150511422896, Test Loss: 0.04863697585630276\n","Epoch: 9913, Train Loss: 0.05182153886388671, Test Loss: 0.04733211898437648\n","Epoch: 9914, Train Loss: 0.0515573877879325, Test Loss: 0.04862781261359127\n","Epoch: 9915, Train Loss: 0.05181156453645197, Test Loss: 0.047323023372939525\n","Epoch: 9916, Train Loss: 0.051547622530635835, Test Loss: 0.04861864689921505\n","Epoch: 9917, Train Loss: 0.051801587595718644, Test Loss: 0.047313925430956055\n","Epoch: 9918, Train Loss: 0.05153785474602985, Test Loss: 0.04860947871995851\n","Epoch: 9919, Train Loss: 0.051791608048383085, Test Loss: 0.04730482516484353\n","Epoch: 9920, Train Loss: 0.05152808444061273, Test Loss: 0.048600308082606736\n","Epoch: 9921, Train Loss: 0.05178162590114289, Test Loss: 0.0472957225810208\n","Epoch: 9922, Train Loss: 0.051518311620884404, Test Loss: 0.048591134993945816\n","Epoch: 9923, Train Loss: 0.05177164116069703, Test Loss: 0.047286617685907625\n","Epoch: 9924, Train Loss: 0.05150853629334603, Test Loss: 0.048581959460763345\n","Epoch: 9925, Train Loss: 0.05176165383374644, Test Loss: 0.04727751048592522\n","Epoch: 9926, Train Loss: 0.05149875846450047, Test Loss: 0.04857278148984823\n","Epoch: 9927, Train Loss: 0.05175166392699359, Test Loss: 0.04726840098749544\n","Epoch: 9928, Train Loss: 0.05148897814085161, Test Loss: 0.048563601087989405\n","Epoch: 9929, Train Loss: 0.0517416714471415, Test Loss: 0.04725928919704159\n","Epoch: 9930, Train Loss: 0.05147919532890519, Test Loss: 0.048554418261978\n","Epoch: 9931, Train Loss: 0.051731676400895835, Test Loss: 0.047250175120988024\n","Epoch: 9932, Train Loss: 0.05146941003516824, Test Loss: 0.04854523301860582\n","Epoch: 9933, Train Loss: 0.05172167879496327, Test Loss: 0.047241058765760104\n","Epoch: 9934, Train Loss: 0.05145962226614906, Test Loss: 0.04853604536466527\n","Epoch: 9935, Train Loss: 0.05171167863605142, Test Loss: 0.04723194013778358\n","Epoch: 9936, Train Loss: 0.05144983202835676, Test Loss: 0.04852685530694964\n","Epoch: 9937, Train Loss: 0.0517016759308693, Test Loss: 0.04722281924348574\n","Epoch: 9938, Train Loss: 0.0514400393283023, Test Loss: 0.04851766285225292\n","Epoch: 9939, Train Loss: 0.05169167068612696, Test Loss: 0.04721369608929323\n","Epoch: 9940, Train Loss: 0.051430244172496344, Test Loss: 0.048508468007369306\n","Epoch: 9941, Train Loss: 0.05168166290853512, Test Loss: 0.04720457068163446\n","Epoch: 9942, Train Loss: 0.05142044656745149, Test Loss: 0.048499270779093585\n","Epoch: 9943, Train Loss: 0.05167165260480544, Test Loss: 0.04719544302693717\n","Epoch: 9944, Train Loss: 0.05141064651968021, Test Loss: 0.048490071174220405\n","Epoch: 9945, Train Loss: 0.051661639781649865, Test Loss: 0.04718631313162928\n","Epoch: 9946, Train Loss: 0.05140084403569529, Test Loss: 0.04848086919954429\n","Epoch: 9947, Train Loss: 0.051651624445780656, Test Loss: 0.047177181002138475\n","Epoch: 9948, Train Loss: 0.051391039122009714, Test Loss: 0.04847166486185898\n","Epoch: 9949, Train Loss: 0.051641606603909734, Test Loss: 0.04716804664489202\n","Epoch: 9950, Train Loss: 0.05138123178513627, Test Loss: 0.04846245816795838\n","Epoch: 9951, Train Loss: 0.051631586262749496, Test Loss: 0.04715891006631721\n","Epoch: 9952, Train Loss: 0.05137142203158811, Test Loss: 0.04845324912463505\n","Epoch: 9953, Train Loss: 0.05162156342901145, Test Loss: 0.047149771272839015\n","Epoch: 9954, Train Loss: 0.051361609867876454, Test Loss: 0.04844403773868042\n","Epoch: 9955, Train Loss: 0.051611538109406496, Test Loss: 0.047140630270882396\n","Epoch: 9956, Train Loss: 0.05135179530051275, Test Loss: 0.04843482401688437\n","Epoch: 9957, Train Loss: 0.05160151031064426, Test Loss: 0.04713148706687036\n","Epoch: 9958, Train Loss: 0.05134197833600681, Test Loss: 0.04842560796603545\n","Epoch: 9959, Train Loss: 0.05159148003943356, Test Loss: 0.04712234166722471\n","Epoch: 9960, Train Loss: 0.051332158980867466, Test Loss: 0.048416389592920606\n","Epoch: 9961, Train Loss: 0.05158144730248198, Test Loss: 0.04711319407836509\n","Epoch: 9962, Train Loss: 0.05132233724160186, Test Loss: 0.04840716890432386\n","Epoch: 9963, Train Loss: 0.05157141210649465, Test Loss: 0.047104044306708795\n","Epoch: 9964, Train Loss: 0.05131251312471501, Test Loss: 0.048397945907027166\n","Epoch: 9965, Train Loss: 0.05156137445817501, Test Loss: 0.04709489235867101\n","Epoch: 9966, Train Loss: 0.05130268663671007, Test Loss: 0.04838872060781023\n","Epoch: 9967, Train Loss: 0.051551334364224745, Test Loss: 0.04708573824066409\n","Epoch: 9968, Train Loss: 0.05129285778408776, Test Loss: 0.04837949301344909\n","Epoch: 9969, Train Loss: 0.05154129183134222, Test Loss: 0.0470765819590976\n","Epoch: 9970, Train Loss: 0.05128302657334622, Test Loss: 0.04837026313071686\n","Epoch: 9971, Train Loss: 0.05153124686622343, Test Loss: 0.04706742352037755\n","Epoch: 9972, Train Loss: 0.05127319301098042, Test Loss: 0.0483610309663837\n","Epoch: 9973, Train Loss: 0.05152119947556172, Test Loss: 0.04705826293090765\n","Epoch: 9974, Train Loss: 0.051263357103483245, Test Loss: 0.04835179652721576\n","Epoch: 9975, Train Loss: 0.05151114966604696, Test Loss: 0.04704910019708686\n","Epoch: 9976, Train Loss: 0.05125351885734315, Test Loss: 0.04834255981997543\n","Epoch: 9977, Train Loss: 0.05150109744436568, Test Loss: 0.047039935325311436\n","Epoch: 9978, Train Loss: 0.05124367827904627, Test Loss: 0.04833332085142112\n","Epoch: 9979, Train Loss: 0.05149104281720078, Test Loss: 0.047030768321973115\n","Epoch: 9980, Train Loss: 0.05123383537507429, Test Loss: 0.048324079628307454\n","Epoch: 9981, Train Loss: 0.05148098579123198, Test Loss: 0.04702159919345991\n","Epoch: 9982, Train Loss: 0.05122399015190573, Test Loss: 0.04831483615738429\n","Epoch: 9983, Train Loss: 0.05147092637313453, Test Loss: 0.047012427946155295\n","Epoch: 9984, Train Loss: 0.051214142616014695, Test Loss: 0.048305590445397\n","Epoch: 9985, Train Loss: 0.05146086456957968, Test Loss: 0.0470032545864387\n","Epoch: 9986, Train Loss: 0.051204292773871576, Test Loss: 0.04829634249908668\n","Epoch: 9987, Train Loss: 0.05145080038723489, Test Loss: 0.046994079120685074\n","Epoch: 9988, Train Loss: 0.05119444063194258, Test Loss: 0.04828709232518962\n","Epoch: 9989, Train Loss: 0.051440733832763215, Test Loss: 0.046984901555264635\n","Epoch: 9990, Train Loss: 0.051184586196689495, Test Loss: 0.04827783993043716\n","Epoch: 9991, Train Loss: 0.0514306649128232, Test Loss: 0.046975721896543095\n","Epoch: 9992, Train Loss: 0.05117472947456986, Test Loss: 0.048268585321555785\n","Epoch: 9993, Train Loss: 0.05142059363406902, Test Loss: 0.04696654015088146\n","Epoch: 9994, Train Loss: 0.05116487047203675, Test Loss: 0.04825932850526753\n","Epoch: 9995, Train Loss: 0.05141052000315081, Test Loss: 0.04695735632463592\n","Epoch: 9996, Train Loss: 0.05115500919553885, Test Loss: 0.04825006948828843\n","Epoch: 9997, Train Loss: 0.05140044402671313, Test Loss: 0.04694817042415782\n","Epoch: 9998, Train Loss: 0.05114514565152016, Test Loss: 0.048240808277330675\n","Epoch: 9999, Train Loss: 0.051390365711397185, Test Loss: 0.046938982455793865\n"]}]},{"cell_type":"code","source":["test_predictions = ann.forward(X_test)\n","plt.scatter(X_test,test_predictions,label='Predictions')\n","plt.scatter(X_test,y_test,label = 'Labels')\n","plt.legend()"],"metadata":{"id":"Mv1xvbP5CO-D","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"ok","timestamp":1658399526737,"user_tz":-330,"elapsed":628,"user":{"displayName":"Indumathi palanikumar","userId":"12151466194348290233"}},"outputId":"d9c9f3b7-2cfb-4b15-acdf-bd11e25297d5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7ff1637a9a90>"]},"metadata":{},"execution_count":54},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print('Average Train loss: %s, Average Test Loss: %s' % (np.mean(train_losses),np.mean(test_losses)))"],"metadata":{"id":"Xwsrksbj7FaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Dataset 2\n","train_df = pd.read_csv('Train_Dataset2.csv',header=None)\n","X_train, y_train = train_df[0].to_numpy().reshape(1,-1),train_df[1].to_numpy().reshape(1,-1)\n","test_df = pd.read_csv('Test_Dataset2.csv',header=None)\n","X_test, y_test = test_df[0].to_numpy().reshape(1,-1),test_df[1].to_numpy().reshape(1,-1)"],"metadata":{"id":"mZES5cPG7FcF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.scatter(X_train,y_train,label='Training Data')\n","plt.legend()"],"metadata":{"id":"_I0yWC6-CpN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train for dataset 2"],"metadata":{"id":"sLVjfgao7Ffx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_predictions = #Forward propagation\n","plt.scatter(X_test,test_predictions,label='Predictions')\n","plt.scatter(X_test,y_test,label = 'Labels')\n","plt.legend()"],"metadata":{"id":"7hyx6DdACiBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Average Train loss: %s, Average Test Loss: %s' % (np.mean(train_losses),np.mean(test_losses)))"],"metadata":{"id":"JFms8ppFCsEY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <center> Introduction to Pytorch"],"metadata":{"id":"-GO6k1YLEWdK"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor"],"metadata":{"id":"1XvQM7qRCz4O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#All operations in pytorch work on tensors\n","X_train_tensor = torch.from_numpy(X_train).T.float()\n","y_train_tensor = torch.from_numpy(y_train).T.float()\n","X_test_tensor = torch.from_numpy(X_test).T.float()\n","y_test_tensor = torch.from_numpy(y_test).T.float()"],"metadata":{"id":"E2hWcwrZEegn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LinearModel(nn.Module):\n","    def __init__(self, hidden_units, output_units):\n","        super(LinearModel, self).__init__()\n","        self.linear_stack = nn.Sequential(\n","            nn.Linear(1, hidden_units), #Hidden layer\n","            nn.Sigmoid(), #Activation\n","            nn.Linear(hidden_units, output_units) #Output layer\n","        )\n","\n","    def forward(self, x):\n","        logits = self.linear_stack(x)\n","        return logits"],"metadata":{"id":"PXaT7INoLLQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Define model\n","model = LinearModel(10,1)"],"metadata":{"id":"NgdNgD64NaoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Do forward propagation\n","predictions = model(X_train_tensor)\n","predictions.shape"],"metadata":{"id":"ca8Bqpn4Nckn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658400749454,"user_tz":-330,"elapsed":353,"user":{"displayName":"Indumathi palanikumar","userId":"12151466194348290233"}},"outputId":"0c3cb6c8-9161-4183-b747-283473b64387"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1000, 1])"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["#Define loss function\n","loss_fn = nn.MSELoss(reduction='mean')\n","print('Loss after forward propagation %s' % float(loss_fn(predictions,y_train_tensor)))"],"metadata":{"id":"0PkiaSMpNdvw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658400873218,"user_tz":-330,"elapsed":370,"user":{"displayName":"Indumathi palanikumar","userId":"12151466194348290233"}},"outputId":"bd5b5865-e96c-4a67-821c-a26189ea2b88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss after forward propagation 1.0619128942489624\n"]}]},{"cell_type":"code","source":["#Dataloader for train set\n","train_dataset = TensorDataset(X_train_tensor,y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size = 10, shuffle=True)"],"metadata":{"id":"f8Mjy89bQc0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Dataloader for test set\n"],"metadata":{"id":"CFSB3u38Qc2K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Define gradient descent algorithm\n","optimizer = torch.optim.Adam(model.parameters(),lr = 3e-4)"],"metadata":{"id":"vzUjDRtvRhb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train()\n","    for batch, (X, y) in enumerate(dataloader):\n","        X_batch, y_batch = X, y\n","\n","        # Compute prediction error\n","        pred = model(X_batch)\n","        loss = loss_fn(pred,y_batch)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 10 == 0:\n","            loss, current = loss.item(), batch * len(X_batch)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\") "],"metadata":{"id":"lFY5doxrNist"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the model\n","epochs = 2\n","for i in range(epochs):\n","  print(f\"Epoch {i+1}\\n-------------------------------\")\n","  train(train_loader,model,loss_fn,optimizer)"],"metadata":{"id":"ufCYh0XhS7LL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658401555114,"user_tz":-330,"elapsed":357,"user":{"displayName":"Indumathi palanikumar","userId":"12151466194348290233"}},"outputId":"5784ce85-a2c6-48c5-cb1b-6f9b345b9bbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","loss: 1.171530  [    0/ 1000]\n","loss: 1.153157  [  100/ 1000]\n","loss: 1.049253  [  200/ 1000]\n","loss: 0.961007  [  300/ 1000]\n","loss: 1.185112  [  400/ 1000]\n","loss: 1.144912  [  500/ 1000]\n","loss: 0.680995  [  600/ 1000]\n","loss: 0.817590  [  700/ 1000]\n","loss: 1.071355  [  800/ 1000]\n","loss: 0.925736  [  900/ 1000]\n","Epoch 2\n","-------------------------------\n","loss: 1.291936  [    0/ 1000]\n","loss: 0.669414  [  100/ 1000]\n","loss: 0.468970  [  200/ 1000]\n","loss: 0.477141  [  300/ 1000]\n","loss: 0.937304  [  400/ 1000]\n","loss: 0.798501  [  500/ 1000]\n","loss: 0.657761  [  600/ 1000]\n","loss: 0.915305  [  700/ 1000]\n","loss: 1.081740  [  800/ 1000]\n","loss: 0.583763  [  900/ 1000]\n"]}]},{"cell_type":"code","source":["test_predictions = model(X_test_tensor).T.detach().numpy()\n","plt.scatter(X_test,test_predictions,label='Predictions')\n","plt.scatter(X_test,y_test,label = 'Labels')\n","plt.legend()"],"metadata":{"id":"olhZvcqaXEXl","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"ok","timestamp":1658401572070,"user_tz":-330,"elapsed":1231,"user":{"displayName":"Indumathi palanikumar","userId":"12151466194348290233"}},"outputId":"fa43b2b3-0403-4c80-8008-577e83ec0c55"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7ff0f3d81390>"]},"metadata":{},"execution_count":69},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print('Average test loss is %s' % float(loss_fn(model(X_test_tensor),y_test_tensor)))"],"metadata":{"id":"mQ1Gp9xiXrUa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658401590291,"user_tz":-330,"elapsed":439,"user":{"displayName":"Indumathi palanikumar","userId":"12151466194348290233"}},"outputId":"258a4894-71a6-4a91-c9fb-5c671cd03aa6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average test loss is 0.6816112399101257\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"IE2VhIHXcAdD"},"execution_count":null,"outputs":[]}]}